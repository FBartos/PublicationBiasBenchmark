---
title:        "README"
bibliography: inst/REFERENCES.bib
csl:          inst/apa.csl
output:       github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# PublicationBiasBenchmark

An R package for benchmarking publication bias correction methods through simulation studies.

## Installation

```{r, eval = FALSE}
# Install from GitHub
devtools::install_github("FBartos/PublicationBiasBenchmark")
```

## Usage
```{r}
library(PublicationBiasBenchmark)
```

### Simulating From Existing Data Generating Models

```r

# Obtain a data.frame with pre-defined conditions
dgm_conditions("Stanley2017")

# simulate the data from the second condition
df <- simulate_dgm("Stanley2017", 2)

# fit a method
run_method("RMA", df)

```


### Use Pre-Simulated Datasets

```r 

# download the pre-simulated datasets
# the default settings downloads the datasets to the `resources` directory, use
# PublicationBiasBenchmark.options(simulation_directory = "/path/")
# to change the settings
download_dgm_datasets("no_bias")

# retrieve first repetition of first condition from the downloaded datasets
retrieve_dgm_dataset("no_bias", condition_id = 1, repetition_id = 1)

```

### Use Pre-Computed Results

```r 

# download the pre-computed results
download_dgm_results("no_bias")

# retrieve results the first repetition of first condition of RMA from the downloaded results
retrieve_dgm_results("no_bias", method = "RMA", condition_id = 1, repetition_id = 1)

# retrieve all results across all conditions and repetitions
retrieve_dgm_results("no_bias")

```

### Use Pre-Computed Measures

```r 

# download the pre-computed measures
download_dgm_measures("no_bias")

# retrieve measures of bias the first condition of RMA from the downloaded results
retrieve_dgm_measures("no_bias", measure = "bias", method = "RMA", condition_id = 1)

# retrieve all measures across all conditions and measures
retrieve_dgm_measures("no_bias")

```

### Visualize Precomputed Results

```{r fig.height = 12}
# retrieve all measures across all conditions and measures
df <- retrieve_dgm_measures("no_bias")

# retrieve conditions
conditions <- dgm_conditions("no_bias")

# add labels
df$label <- with(df, paste0(method, " (", method_setting, ")"))

# distinguish between H0 and H1
df$H0 <- df$condition_id %in% conditions$condition_id[conditions$mean_effect == 0]

par(mfrow = c(3, 2))

par(mar = c(4, 10, 1, 1))
boxplot(convergence ~ label, horizontal = T, las = 1, ylab = "", ylim = c(0.5, 1), data = df, xlab = "Convergence")
boxplot(rmse ~ label, horizontal = T, las = 1, ylab = "", ylim = c(0, 0.25), data = df, xlab = "RMSE")
boxplot(bias ~ label, horizontal = T, las = 1, ylab = "", ylim = c(-0.25, 0.25), data = df, xlab = "Bias")
abline(v = 0, lty = 3)
boxplot(coverage ~ label, horizontal = T, las = 1, ylab = "", ylim = c(0.5, 1), data = df, xlab = "CI Coverage")
abline(v = 0.95, lty = 3)
boxplot(power ~ label, horizontal = T, las = 1, ylab = "", ylim = c(0, 0.5), data = df[df$H0,], xlab = "Type I Error")
abline(v = 0.05, lty = 3)
boxplot(power ~ label, horizontal = T, las = 1, ylab = "", ylim = c(0.5, 1), data = df[!df$H0,], xlab = "Power")
```


### Simulating From Existing DGM With Custom Settings

```r

# define sim setting
sim_settings <- list(
  n_studies     = 100,
  mean_effect   = 0.3,
  heterogeneity = 0.1
)

# check whether it is feasible
# (defined outside of the function - not to decrease performance during simulation)
validate_dgm_setting("no_bias", sim_settings)

# simulate the data
df <- simulate_dgm("no_bias", sim_settings)

# fit a method
run_method("RMA", df)

```

### Key Functions

#### Data Generating Mechanisms
- `simulate_dgm()`: Generates simulated data according to specified data generating model and settings.
- `dgm_conditions()`: Lists prespecified conditions of the data generating mechanism.
- `validate_dgm_setting()`: Validates (custom) setting of the data generating mechanism.
- `download_dgm_datasets()`: Downloads pre-simulated datasets from the OSF repository.
- `retrieve_dgm_dataset()`: Retrieves the pre-simulated dataset of a given condition and repetition from downloaded from the pre-downloaded OSF repository.

#### Method Estimation And Results
- `run_method()`: Estimates method on a supplied data according to the specified settings.
- `method_settings()`: Lists prespecified settings of the method.
- `download_dgm_results()`: Downloads pre-computed results from the OSF repository.
- `retrieve_dgm_results()`: Retrieves the pre-computed results of a given method, condition, and repetition from the pre-downloaded OSF repository.

#### Performance measures And Results
- `bias()`, `bias_mcse()`, etc.: Functions to compute performance measures and their Monte Carlo standard errors.
- `download_dgm_measures()`: Downloads pre-computed performance measures from the OSF repository.
- `retrieve_dgm_measures()`: Retrieves the pre-computed performance measures of a given method, condition, and repetition from the pre-downloaded OSF repository.

### Available DGM Models

See `methods("dgm")` for the full list:

- `"no_bias"`: Generates data without publication bias (a test simulation)
- `"Stanley2017"`: @stanley2017finding
- `"Alinaghi2018"`: @alinaghi2018meta
- `"Bom2019"`: @bom2019kinked
- `"Carter2019"`: @carter2019correcting

### Available Methods

See `methods("method")` for the full list:

- `"RMA"`: Random effects meta-analysis
- `"WLS"`: Weighted Least Squares
- `"trimfill"`: Trim-and-Fill [@duval2000trim]
- `"WAAPWLS"`: Weighted Least Squares - Weighted Average of Adequately Power Studies [@stanley2017finding]
- `"WILS"`: Weighted and Iterated Least Squares [@stanley2024harnessing]
- `"PET"`: Precision-Effect Test (PET) publication bias adjustment [@stanley2014meta]
- `"PEESE"`: Precision-Effect Estimate with Standard Errors (PEESE) publication bias adjustment [@stanley2014meta]
- `"PETPEESE"`: Precision-Effect Test and Precision-Effect Estimate with Standard Errors (PET-PEESE) publication bias adjustment [@stanley2014meta]
- `"EK"`: Endogenous Kink [@bom2019kinked]
- `"SM"`: Selection Models (3PSM, 4PSM) [@vevea1995general] 
- `"pcurve"`: P-curve [@simonsohn2014pcurve]
- `"puniform"`: P-uniform and P-uniform* [@vanassen2015meta, @vanaert2025puniform]
- `"AK"`: Andrews & Kasy selection models (AK1, AK2) [@andrews2019identification]
- `"RoBMA"`: Robust Bayesian Meta-Analysis [@bartos2023robust]


### Available Performance Measures

See `?measures` for the full list of performance measures and their Monte Carlo standard errors/

### DGM OSF Repositories
All DGM are linked within the OSF repository (<https://osf.io/exf3m/>) and contain the following elements:

- `data` : folder containing by-condition simulated datasets for all repetitions
- `results` : folder containing by-method results for all conditions * repetitions
- `measures` : folder containing by-measure performance for all methods * conditions 
- `metadata` : folder containing the following information:
  - `dgm-conditions.csv` : file mapping of all conditions and the corresponding settings
  - `dgm-generation.R` : file with code for exact reproduction of the pre-simulated datasets
  - `dgm-sessionInfo.txt`: file with reproducibility details for the pre-simulated datasets
  - `dgm-session.log`: file with reproducibility details for the pre-simulated datasets (based on sessioninfo package)
  - `results.R` : file with code for exact reproduction of the by method results (might be method / method groups specific)
  - `results-sessionInfo.txt`: file with reproducibility details for the precomputed results (might be method / method groups specific)
  - `pm-computation.R` : file with code for computation of performance measures 
  
### References
