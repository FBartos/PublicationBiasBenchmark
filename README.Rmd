---
title:        "README"
bibliography: inst/REFERENCES.bib
csl:          inst/apa.csl
output:       github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# PublicationBiasBenchmark

An R package for benchmarking publication bias correction methods through simulation studies.

## Installation

```{r, eval = FALSE}
# Install from GitHub
devtools::install_github("FBartos/PublicationBiasBenchmark")
```

## Usage

### Basic Example

```{r example, eval = FALSE}
library(PublicationBiasBenchmark)

# define sim setting
sim_settings <- list(
  n_studies     = 100,
  mean_effect   = 0.3,
  heterogeneity = 0.1
)

# check whether it is feasible
# (defined outside of the function - not to decrease performance during simulation)
validate_dgm_setting("no_bias", sim_settings)

# simulate the data
df <- simulate_dgm("no_bias", sim_settings)

# fit a method
method("PET", df)
```

### Key Functions

#### Data Generating Mechanisms
- `simulate_dgm()`: Generates simulated data according to specified data generating model and settings.
- `dgm_settings()`: Lists prespecified settings of the data generating mechanism.
- `validate_dgm_setting()`: Validates setting of the data generating mechanism.

### Available DGM Models

- `"no_bias"`: Generates data without publication bias (a test simulation)
- `"Stanley2017"`: @stanley2017finding
- `"Alinaghi2018"`: @alinaghi2018meta
- `"Bom2019"`: @bom2019kinked
- `"Carter2019"`: @carter2019correcting

### Available Methods

- `"PET"`: Publication bias correction using PET (Precision-Effect Test)
- ...

## Documentation

For detailed documentation of all functions and parameters, use:

```{r, eval = FALSE}
?simulate_dgm
?method
?validate_dgm_setting
```


### References
