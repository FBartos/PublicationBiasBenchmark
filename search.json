[{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_DGMs.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Adding New Data-Generating Mechanisms","text":"DGM package consists three key components: Main DGM function: Implements data-generating mechanism Validation function: Validates input parameters settings Conditions function: Defines pre-specified conditions three functions must implemented single file named dgm-{DGM_NAME}.R R/ directory. Implementation three functions allows users generate data DGM via simulate_dgm() function.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_DGMs.html","id":"file-structure-and-naming","dir":"Articles","previous_headings":"","what":"File Structure and Naming","title":"Adding New Data-Generating Mechanisms","text":"DGM called “no_bias”, need create file named R/dgm-no_bias.R containing three functions: dgm.no_bias(): main data-generating mechanism implementation validate_dgm_setting.no_bias(): Parameter validation dgm_conditions.no_bias(): Pre-defined conditions naming pattern crucial package’s S3 method dispatch system work correctly.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_DGMs.html","id":"main-dgm-function-dgm-dgm_name","dir":"Articles","previous_headings":"","what":"1. Main DGM Function: dgm.{DGM_NAME}()","title":"Adding New Data-Generating Mechanisms","text":"core function implements data-generating mechanism. no_bias implementation example:","code":"#' @title Normal Unbiased Data-Generating Mechanism #' #' @description #' An example data-generating mechanism to simulate effect sizes without #' publication bias. #' #' @param dgm_name DGM name (automatically passed) #' @param settings List containing \\describe{ #'   \\item{mean_effect}{Mean effect} #'   \\item{heterogeneity}{Effect heterogeneity} #'   \\item{n_studies}{Number of effect size estimates} #' } #' #' #' @return Data frame with \\describe{ #'   \\item{yi}{effect size} #'   \\item{sei}{standard error} #' } #' #' @references #' \\insertAllCited{} #' #' @seealso [dgm()], [validate_dgm_setting()] #' @export dgm.no_bias <- function(dgm_name, settings) {    # Extract settings   n_studies     <- settings[[\"n_studies\"]]   mean_effect   <- settings[[\"mean_effect\"]]   heterogeneity <- settings[[\"heterogeneity\"]]    # Simulate sample sizes based on empirical distribution   N_shape <- 2   N_scale <- 58   N_low   <- 25   N_high  <- 500    N_seq <- seq(N_low, N_high, 1)   N_den <- stats::dnbinom(N_seq, size = N_shape, prob = 1/(N_scale+1)) /       (stats::pnbinom(N_high, size = N_shape, prob = 1/(N_scale+1)) -         stats::pnbinom(N_low - 1, size = N_shape, prob = 1/(N_scale+1)))    N <- sample(N_seq, n_studies, TRUE, N_den)    # Compute standard errors based on sample sizes (Cohen's d formula)   standard_errors <- sqrt(4/N)    # Simulate true effect sizes with heterogeneity   effect_sizes <- stats::rnorm(n_studies, mean_effect,                                sqrt(heterogeneity^2 + standard_errors^2))    # Return standardized data frame   data <- data.frame(     yi  = effect_sizes,     sei = standard_errors,     ni  = N   )    return(data) }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_DGMs.html","id":"key-requirements-for-the-main-function","dir":"Articles","previous_headings":"1. Main DGM Function: dgm.{DGM_NAME}()","what":"Key Requirements for the Main Function:","title":"Adding New Data-Generating Mechanisms","text":"Input Parameters: dgm_name: Automatically passed framework settings: Named list containing DGM parameters condition_id value Output: Must return data frame required columns: yi: Effect sizes sei: Standard errors ni: Sample sizes es_type: Type effect size (e.g., “SMD”, “logOR”, “none”) Optional additional columns (commonly used): study_id: Unique identifier study/cluster (presence multilevel/clustered data)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_DGMs.html","id":"validation-function-validate_dgm_setting-dgm_name","dir":"Articles","previous_headings":"","what":"2. Validation Function: validate_dgm_setting.{DGM_NAME}()","title":"Adding New Data-Generating Mechanisms","text":"function validates required parameters provided valid values:","code":"#' @export validate_dgm_setting.no_bias <- function(dgm_name, settings) {    # Check that all required settings are specified   required_params <- c(\"n_studies\", \"mean_effect\", \"heterogeneity\")   missing_params <- setdiff(required_params, names(settings))   if (length(missing_params) > 0)     stop(\"Missing required settings: \", paste(missing_params, collapse = \", \"))    # Extract settings for validation   n_studies     <- settings[[\"n_studies\"]]   mean_effect   <- settings[[\"mean_effect\"]]   heterogeneity <- settings[[\"heterogeneity\"]]    # Validate each parameter   if (length(n_studies) != 1 || !is.numeric(n_studies) || is.na(n_studies) ||        !is.wholenumber(n_studies) || n_studies < 1)     stop(\"'n_studies' must be an integer larger than 0\")      if (length(mean_effect) != 1 || !is.numeric(mean_effect) || is.na(mean_effect))     stop(\"'mean_effect' must be numeric\")      if (length(heterogeneity) != 1 || !is.numeric(heterogeneity) ||        is.na(heterogeneity) || heterogeneity < 0)     stop(\"'heterogeneity' must be non-negative\")    return(invisible(TRUE)) }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_DGMs.html","id":"key-points-for-validation","dir":"Articles","previous_headings":"2. Validation Function: validate_dgm_setting.{DGM_NAME}()","what":"Key Points for Validation:","title":"Adding New Data-Generating Mechanisms","text":"Check missing required parameters Validate parameter types (numeric, integer, character, etc.) Check parameter ranges constraints Provide clear, informative error messages Return invisible(TRUE) successful validation Use stop() validation failures","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_DGMs.html","id":"conditions-function-dgm_conditions-dgm_name","dir":"Articles","previous_headings":"","what":"3. Conditions Function: dgm_conditions.{DGM_NAME}()","title":"Adding New Data-Generating Mechanisms","text":"function defines pre-specified conditions benchmarking studies: Always add condition_id column unique identifiers. column used generating data pre-defined conditions. defined, settings changed retrospectively ensure reproducibility continuity benchmark.","code":"#' @export dgm_conditions.no_bias <- function(dgm_name) {    # Generate a grid of pre-specified settings   settings <- data.frame(expand.grid(     mean_effect    = c(0, 0.3),     heterogeneity  = c(0, 0.15),     n_studies      = c(10, 100)   ))    # Attach unique condition identifiers   settings$condition_id <- 1:nrow(settings)    return(settings) }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_DGMs.html","id":"using-your-new-dgm","dir":"Articles","previous_headings":"","what":"Using Your New DGM","title":"Adding New Data-Generating Mechanisms","text":"implemented, DGM can used unified interface:","code":"# Use with custom settings data <- simulate_dgm(\"no_bias\", list(   mean_effect = 0.2,   heterogeneity = 0.1,   n_studies = 50 ))  # Use with pre-defined conditions data <- simulate_dgm(\"no_bias\", condition_id = 1)  # View available conditions conditions <- dgm_conditions(\"no_bias\") print(conditions)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_Methods.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Adding New Methods","text":"method package consists three key components: Main method function: Implements statistical method Settings function: Defines available method configurations Extra columns function: Specifies additional result columns three functions must implemented single file named method-{METHOD_NAME}.R R/ directory. Implementation three functions allows users apply method via run_method() function.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_Methods.html","id":"file-structure-and-naming","dir":"Articles","previous_headings":"","what":"File Structure and Naming","title":"Adding New Methods","text":"method called “PET”, need create file named R/method-PET.R containing three functions: method.PET(): main implementation method_settings.PET(): Available settings/configurations method_extra_columns.PET(): Additional result columns naming pattern crucial package’s S3 method dispatch system work correctly.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_Methods.html","id":"main-method-function-method-method_name","dir":"Articles","previous_headings":"","what":"1. Main Method Function: method.{METHOD_NAME}()","title":"Adding New Methods","text":"core function implements statistical method. PET implementation example:","code":"#' @title PET (Precision-Effect Test) Method #' #' @description #' Implements the Precision-Effect Test for publication bias correction. #' PET regresses effect sizes against standard errors to test for and correct #' publication bias. The intercept represents the bias-corrected effect size #' estimate. #' #' @param method_name Method name (automatically passed) #' @param data Data frame with yi (effect sizes) and sei (standard errors) #' @param settings List of method settings #' #' @return Data frame with PET results #' #' @export method.PET <- function(method_name, data, settings = NULL) {      # Extract data   effect_sizes    <- data$yi   standard_errors <- data$sei      # Input validation and error handling   if (length(effect_sizes) < 3)     stop(\"At least 3 estimates required for PET analysis\", call. = FALSE)      if (stats::var(standard_errors) <= 0)     stop(\"No variance in standard errors\", call. = FALSE)      # Implement the statistical method   pet_model <- stats::lm(effect_sizes ~ standard_errors,                          weights = 1/standard_errors^2)      # Extract and process results   coefficients    <- stats::coef(pet_model)   se_coefficients <- summary(pet_model)$coefficients[, \"Std. Error\"]   p_values        <- summary(pet_model)$coefficients[, \"Pr(>|t|)\"]      # Main estimates   estimate    <- coefficients[1]  # Intercept = bias-corrected effect   estimate_se <- se_coefficients[1]   estimate_p  <- p_values[1]      # Additional method-specific results   bias_coefficient <- coefficients[2]   bias_p_value     <- p_values[2]      # Calculate confidence intervals   estimate_lci <- estimate - 1.96 * estimate_se   estimate_uci <- estimate + 1.96 * estimate_se      # Return standardized results   return(data.frame(     method         = method_name,     estimate       = estimate,     standard_error = estimate_se,     ci_lower       = estimate_lci,     ci_upper       = estimate_uci,     p_value        = estimate_p,     BF             = NA,     convergence    = TRUE,     note           = NA,     # Method-specific columns     bias_coefficient = bias_coefficient,     bias_p_value     = bias_p_value   )) }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_Methods.html","id":"key-requirements-for-the-main-function","dir":"Articles","previous_headings":"1. Main Method Function: method.{METHOD_NAME}()","what":"Key Requirements for the Main Function:","title":"Adding New Methods","text":"Input Parameters: method_name: Automatically passed framework data: Data frame yi (effect sizes), sei (standard errors), ni (sample sizes). settings: Optional list method-specific settings Output: Must return data frame required columns: method: Method name estimate: Meta-analytic effect size estimate standard_error: Standard error estimate ci_lower: Lower confidence interval bound (95%) ci_upper: Upper confidence interval bound (95%) p_value: P-value estimate BF: Bayes factor estimate convergence: Logical indicating successful convergence note: Character string notes method provide certain values (e.g., Bayes factor), use NA. Error Handling: Include input validation meaningful error messages Use stop() call. = FALSE user-friendly errors framework handles errors automatically - function can throw errors. package catch errors attach empty output convergence = FALSE error message note.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_Methods.html","id":"settings-function-method_settings-method_name","dir":"Articles","previous_headings":"","what":"2. Settings Function: method_settings.{METHOD_NAME}()","title":"Adding New Methods","text":"function defines available configurations method: selected settings passed main function settings parameter. , main function can use settings adjust behavior. defined, settings changed retrospectively ensure reproducibility continuity benchmark. example method defines several settings can examine random effects meta-analysis (RMA) method:","code":"#' @export method_settings.PET <- function(method_name) {      settings <- list(     \"default\" = list() # PET has no configurable settings   )      return(settings) } # Example with multiple settings (from RMA method) method_settings.RMA <- function(method_name) {      settings <- list(     \"default\" = list(       method = \"REML\",        test.uni = \"knha\",        test.mv = \"t\",        control = list(stepadj = 0.5, maxiter = 500)     )   )      return(settings) }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_Methods.html","id":"extra-columns-function-method_extra_columns-method_name","dir":"Articles","previous_headings":"","what":"3. Extra Columns Function: method_extra_columns.{METHOD_NAME}()","title":"Adding New Methods","text":"function specifies additional columns method returns beyond required ones: column names must match exactly additional columns main function returns. columns included final output data frame alongside required columns guarantee results can merged case method fails error. Use character(0) c() method extra columns","code":"#' @export method_extra_columns.PET <- function(method_name) {   c(\"bias_coefficient\", \"bias_p_value\") }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Adding_New_Methods.html","id":"using-your-new-method","dir":"Articles","previous_headings":"","what":"Using Your New Method","title":"Adding New Methods","text":"implemented, method can used unified interface:","code":"# Create example data data <- data.frame(   yi  = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Run your method result <- run_method(\"PET\", data) print(result)  # Use specific settings (if available) result <- run_method(\"PET\", data, \"default\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Computing Method Measures","text":"computing storing method results DGMs, need : Compute standard performance measures (bias, RMSE, coverage, power, etc.) Compute measures method replacement (methods convergence issues) Store measures appropriate directory structure process creates performance summaries allow systematic comparison methods across conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Computing Method Measures","text":"computing measures, ensure : Method results computed stored DGMs (see Computing Method Results) Results files correct directory structure PublicationBiasBenchmark package loaded","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"performance-measures","dir":"Articles","previous_headings":"","what":"Performance Measures","title":"Computing Method Measures","text":"package computes various performance measures defined measures() documentation. measure computed separately method-setting-condition combination.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"method-replacement-strategy","dir":"Articles","previous_headings":"","what":"Method Replacement Strategy","title":"Computing Method Measures","text":"methods may fail converge certain datasets. method replacement strategy handles cases : Computing measures successfully converged cases failed cases, substituting results fallback method Ensuring fair comparison across conditions example, RMA fails converge, might replaced simpler FMA (fixed-effects) results specific cases.","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"step-1-set-up-your-environment","dir":"Articles","previous_headings":"Computing Measures: Step-by-Step Guide","what":"Step 1: Set Up Your Environment","title":"Computing Method Measures","text":"","code":"library(PublicationBiasBenchmark)  # Verify the directory containing results data_folder <- PublicationBiasBenchmark.get_option(\"simulation_directory\") print(paste(\"Working directory:\", data_folder))"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"step-2-define-dgms-and-methods","dir":"Articles","previous_headings":"Computing Measures: Step-by-Step Guide","what":"Step 2: Define DGMs and Methods","title":"Computing Method Measures","text":"Specify DGMs process methods compute measures :","code":"# List of DGMs to evaluate dgm_names <- c(   \"Stanley2017\",   \"Alinaghi2018\",   \"Bom2019\",   \"Carter2019\" )  # Define your new method methods_settings <- data.frame(   method          = c(\"myNewMethod\"),   method_setting  = c(\"default\"),   power_test_type = c(\"p_value\") )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"step-3-compute-performance-measures","dir":"Articles","previous_headings":"Computing Measures: Step-by-Step Guide","what":"Step 3: Compute Performance Measures","title":"Computing Method Measures","text":"Process DGM compute standard replacement performance measures:","code":"for (dgm_name in dgm_names) {      # Download precomputed results for existing methods (for replacements)   download_dgm_results(dgm_name)      ### Simple performance metrics ----   # Compute primary measures (not dependent on CI or power)   compute_measures(     dgm_name        = dgm_name,     method          = methods_settings$method,     method_setting  = methods_settings$method_setting,     power_test_type = methods_settings$power_test_type,     measures        = c(\"bias\", \"relative_bias\", \"mse\", \"rmse\",                         \"empirical_variance\", \"empirical_se\", \"convergence\"),     verbose         = TRUE,     estimate_col    = \"estimate\",     true_effect_col = \"mean_effect\",     ci_lower_col    = \"ci_lower\",     ci_upper_col    = \"ci_upper\",     p_value_col     = \"p_value\",     bf_col          = \"BF\",     convergence_col = \"convergence\",     n_repetitions   = 1000,     overwrite       = FALSE   )      # If your method does not return CI or hypothesis test, skip these measures   compute_measures(     dgm_name        = dgm_name,     method          = methods_settings$method,     method_setting  = methods_settings$method_setting,     power_test_type = methods_settings$power_test_type,     measures        = c(\"power\", \"coverage\", \"mean_ci_width\", \"interval_score\",                         \"negative_likelihood_ratio\", \"positive_likelihood_ratio\"),     verbose         = TRUE,     estimate_col    = \"estimate\",     true_effect_col = \"mean_effect\",     ci_lower_col    = \"ci_lower\",     ci_upper_col    = \"ci_upper\",     p_value_col     = \"p_value\",     bf_col          = \"BF\",     convergence_col = \"convergence\",     n_repetitions   = 1000,     overwrite       = FALSE   )         ### Replacement performance metrics ----   # Specify method replacement strategy   # The most common one: random-effects meta-analysis -> fixed-effect meta-analysis   RMA_replacement <- list(     method          = c(\"RMA\", \"FMA\"),      method_setting  = c(\"default\", \"default\"),      power_test_type = c(\"p_value\", \"p_value\")   )      method_replacements <- list(     \"myNewMethod-default\" = RMA_replacement   )      compute_measures(     dgm_name            = dgm_name,     method              = methods_settings$method,     method_setting      = methods_settings$method_setting,     power_test_type     = methods_settings$power_test_type,     method_replacements = method_replacements,     measures            = c(\"bias\", \"relative_bias\", \"mse\", \"rmse\",                             \"empirical_variance\", \"empirical_se\", \"convergence\"),     verbose         = TRUE,     estimate_col    = \"estimate\",     true_effect_col = \"mean_effect\",     ci_lower_col    = \"ci_lower\",     ci_upper_col    = \"ci_upper\",     p_value_col     = \"p_value\",     bf_col          = \"BF\",     convergence_col = \"convergence\",     n_repetitions   = 1000,     overwrite       = FALSE   )      # If your method does not return CI or hypothesis test, skip these measures   compute_measures(     dgm_name            = dgm_name,     method              = methods_settings$method,     method_setting      = methods_settings$method_setting,     power_test_type     = methods_settings$power_test_type,     method_replacements = method_replacements,     measures            = c(\"power\", \"coverage\", \"mean_ci_width\", \"interval_score\",                             \"negative_likelihood_ratio\", \"positive_likelihood_ratio\"),     verbose         = TRUE,     estimate_col    = \"estimate\",     true_effect_col = \"mean_effect\",     ci_lower_col    = \"ci_lower\",     ci_upper_col    = \"ci_upper\",     p_value_col     = \"p_value\",     bf_col          = \"BF\",     convergence_col = \"convergence\",     n_repetitions   = 1000,     overwrite       = FALSE   )    }"},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"core-parameters","dir":"Articles","previous_headings":"Understanding the Parameters","what":"Core Parameters","title":"Computing Method Measures","text":"dgm_name: Name data-generating mechanism method: Vector method names method_setting: Vector method settings (must match length method) power_test_type: significance determined (\"p_value\" \"bayes_factor\")","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"measure-selection","dir":"Articles","previous_headings":"Understanding the Parameters","what":"Measure Selection","title":"Computing Method Measures","text":"measures: Vector measure names compute (see measures() available options)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"column-mapping","dir":"Articles","previous_headings":"Understanding the Parameters","what":"Column Mapping","title":"Computing Method Measures","text":"estimate_col: Column name effect size estimates (default: \"estimate\") true_effect_col: Column name true effects conditions (default: \"mean_effect\") ci_lower_col: Column name CI lower bounds (default: \"ci_lower\") ci_upper_col: Column name CI upper bounds (default: \"ci_upper\") p_value_col: Column name p-values (default: \"p_value\") bf_col: Column name Bayes factors (default: \"BF\") convergence_col: Column name convergence indicator (default: \"convergence\")","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"control-parameters","dir":"Articles","previous_headings":"Understanding the Parameters","what":"Control Parameters","title":"Computing Method Measures","text":"n_repetitions: Expected number repetitions per condition (default: 1000) verbose: Whether print progress messages (default: TRUE) overwrite: Whether overwrite existing measure files (default: FALSE)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Measures.html","id":"contributing-to-the-package","dir":"Articles","previous_headings":"","what":"Contributing to the Package","title":"Computing Method Measures","text":"package maintainers compute update precomputed measures contribute new method. want contribute new method: Implement method following Adding New Methods guidelines Compute results DGMs (see Computing Method Results) Submit pull request method implementation results Package maintainers compute measures integrate benchmark","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Results.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Computing Method Results","text":"implementing three required functions new method (method.{METHOD_NAME}(), method_settings.{METHOD_NAME}(), method_extra_columns.{METHOD_NAME}()), need : Download presimulated datasets DGM Apply method condition-repetition combinations Store results appropriate directory structure Document session information reproducibility process ensures method can compared existing methods using precomputed performance measures.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Results.html","id":"prerequisites","dir":"Articles","previous_headings":"","what":"Prerequisites","title":"Computing Method Results","text":"computing results, ensure : method fully implemented three required functions sufficient computational resources (full benchmark may take hours days depending method complexity) adequate disk space storing results (typically several hundred MB per DGM)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Results.html","id":"directory-structure","dir":"Articles","previous_headings":"","what":"Directory Structure","title":"Computing Method Results","text":"benchmark uses following directory structure storing results: default, structure created directory specified PublicationBiasBenchmark.get_option(\"simulation_directory\").","code":"resources/ ├── {DGM_NAME}/ │   ├── data/ │   │   └── {condition_id}.csv          # Presimulated datasets │   ├── results/ │   │   └── {METHOD}-{SETTING}.csv      # Method results │   └── metadata/ │       ├── dgm-conditions.csv          # Condition parameters │       ├── {METHOD}-{SETTING}-sessionInfo.txt │       └── {METHOD}-{SETTING}-session.log"},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Results.html","id":"step-1-set-up-your-environment","dir":"Articles","previous_headings":"Computing Results: Step-by-Step Guide","what":"Step 1: Set Up Your Environment","title":"Computing Method Results","text":"First, ensure ’re working correct directory package loaded:","code":"library(PublicationBiasBenchmark)  # Optional: Set custom directory for results storage # PublicationBiasBenchmark.options(simulation_directory = \"/path/to/storage\")  # Verify the directory data_folder <- PublicationBiasBenchmark.get_option(\"simulation_directory\") print(paste(\"Working directory:\", data_folder))"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Results.html","id":"step-2-define-dgms-and-method-information","dir":"Articles","previous_headings":"Computing Results: Step-by-Step Guide","what":"Step 2: Define DGMs and Method Information","title":"Computing Method Results","text":"Specify DGMs want compute results method details:","code":"# List of DGMs to evaluate dgm_names <- c(   \"Stanley2017\",   \"Alinaghi2018\",   \"Bom2019\",   \"Carter2019\" )  # Your method information method_name    <- \"myNewMethod\" method_setting <- \"default\"  # Or other setting name if you have multiple"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Results.html","id":"step-3-download-presimulated-datasets","dir":"Articles","previous_headings":"Computing Results: Step-by-Step Guide","what":"Step 3: Download Presimulated Datasets","title":"Computing Method Results","text":"Download presimulated datasets DGMs: step needs done . datasets cached locally subsequent use.","code":"# Download datasets for all DGMs for (dgm_name in dgm_names) {   message(\"Downloading datasets for: \", dgm_name)   download_dgm_datasets(dgm_name) }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Results.html","id":"step-4-compute-results-for-each-dgm","dir":"Articles","previous_headings":"Computing Results: Step-by-Step Guide","what":"Step 4: Compute Results for Each DGM","title":"Computing Method Results","text":"Now, compute results method across conditions repetitions. code bellow template can adapt needed. likely adapt code run parallel/using job scheduler cluster.","code":"# Set seed for reproducibility set.seed(1)  # Process each DGM for (dgm_name in dgm_names) {      message(\"\\n\", \"=\"*60)   message(\"Processing DGM: \", dgm_name)   message(\"=\"*60)      # Get condition information   conditions <- dgm_conditions(dgm_name)   message(\"Number of conditions: \", nrow(conditions))      # Container to store all results for this DGM   all_results <- list()      # Process each condition   for (condition_id in conditions$condition_id) {          message(\"  Condition \", condition_id, \" / \", nrow(conditions))          # Retrieve all repetitions for this condition     condition_datasets <- retrieve_dgm_dataset(       dgm_name = dgm_name,       condition_id = condition_id,       repetition_id = NULL  # NULL retrieves all repetitions     )          # Get unique repetition IDs     repetition_ids <- unique(condition_datasets$repetition_id)     message(\"    Repetitions: \", length(repetition_ids))          # Compute results for each repetition     condition_results <- list()     for (repetition_id in repetition_ids) {              # Extract data for this specific repetition       repetition_data <- condition_datasets[         condition_datasets$repetition_id == repetition_id,        ]              # Apply your method (error handling is done internally)       result <- run_method(         method_name = method_name,         data        = repetition_data,         settings    = method_setting       )              # Attach metadata       result$condition_id  <- condition_id       result$repetition_id <- repetition_id              condition_results[[repetition_id]] <- result     }          # Combine results for this condition     all_results[[condition_id]] <- do.call(rbind, condition_results)   }      # Combine all results for this DGM   dgm_results <- do.call(rbind, all_results)      # Save results   results_dir <- file.path(data_folder, dgm_name, \"results\")   if (!dir.exists(results_dir)) {     dir.create(results_dir, recursive = TRUE)   }      results_file <- file.path(     results_dir,      paste0(method_name, \"-\", method_setting, \".csv\")   )      write.csv(dgm_results, file = results_file, row.names = FALSE)   message(\"Results saved to: \", results_file)      # Save session information   metadata_dir <- file.path(data_folder, dgm_name, \"metadata\")   if (!dir.exists(metadata_dir)) {     dir.create(metadata_dir, recursive = TRUE)   }      # sessionInfo() output   sessioninfo_file <- file.path(     metadata_dir,     paste0(method_name, \"-\", method_setting, \"-sessionInfo.txt\")   )   writeLines(     capture.output(sessionInfo()),      sessioninfo_file   )      # Detailed session info (if sessioninfo package is available)   if (requireNamespace(\"sessioninfo\", quietly = TRUE)) {     session_log_file <- file.path(       metadata_dir,       paste0(method_name, \"-\", method_setting, \"-session.log\")     )     sessioninfo::session_info(to_file = session_log_file)   }      message(\"Session info saved to: \", metadata_dir) }  message(\"All computations completed!\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Computing_Method_Results.html","id":"contributing-to-the-package","dir":"Articles","previous_headings":"","what":"Contributing to the Package","title":"Computing Method Results","text":"package maintainers compute update precomputed measures contribute new method. want contribute new method: Implement method following Adding New Methods guidelines Compute results DGMs (see Computing Method Results) Submit pull request method implementation results Package maintainers compute measures integrate benchmark","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"complete-results","dir":"Articles","previous_headings":"","what":"Complete Results","title":"Results: Overall","text":"results based Stanley2017 (2017), Alinaghi2018 (2018), Bom2019 (2019), Carter2019 (2019) data-generating mechanisms total 1665 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"average-performance","dir":"Articles","previous_headings":"Complete Results","what":"Average Performance","title":"Results: Overall","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using average empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution empirical standard error values corresponding outcome scale. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using average Interval Score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution empirical standard error values corresponding outcome scale. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution CI width values corresponding outcome scale. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"by-condition-performance-conditional-on-method-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Overall","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using interval score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average 95% CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution 95% CI width values corresponding outcome scale.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"by-condition-performance-replacement-in-case-of-non-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Overall","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using interval score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average 95% CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution 95% CI width values corresponding outcome scale.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"subset-publication-bias-present","dir":"Articles","previous_headings":"","what":"Subset: Publication Bias Present","title":"Results: Overall","text":"results based Stanley2017 (2017), Alinaghi2018 (2018), Bom2019 (2019), Carter2019 (2019) data-generating mechanisms total 1143 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"average-performance-1","dir":"Articles","previous_headings":"Subset: Publication Bias Present","what":"Average Performance","title":"Results: Overall","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using average empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution empirical standard error values corresponding outcome scale. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using average Interval Score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution empirical standard error values corresponding outcome scale. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution CI width values corresponding outcome scale. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"by-condition-performance-conditional-on-method-convergence-1","dir":"Articles","previous_headings":"Subset: Publication Bias Present","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Overall","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using interval score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average 95% CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution 95% CI width values corresponding outcome scale.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-1","dir":"Articles","previous_headings":"Subset: Publication Bias Present","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Overall","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using interval score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average 95% CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution 95% CI width values corresponding outcome scale.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"subset-publication-bias-absent","dir":"Articles","previous_headings":"","what":"Subset: Publication Bias Absent","title":"Results: Overall","text":"results based Stanley2017 (2017), Alinaghi2018 (2018), Bom2019 (2019), Carter2019 (2019) data-generating mechanisms total 522 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"average-performance-2","dir":"Articles","previous_headings":"Subset: Publication Bias Absent","what":"Average Performance","title":"Results: Overall","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using average empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution empirical standard error values corresponding outcome scale. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using average Interval Score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution empirical standard error values corresponding outcome scale. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution CI width values corresponding outcome scale. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"by-condition-performance-conditional-on-method-convergence-2","dir":"Articles","previous_headings":"Subset: Publication Bias Absent","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Overall","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using interval score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average 95% CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution 95% CI width values corresponding outcome scale.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-2","dir":"Articles","previous_headings":"Subset: Publication Bias Absent","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Overall","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using interval score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average 95% CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution 95% CI width values corresponding outcome scale.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Results: Overall","text":"report compiled Thu Oct 16 08:42:47 2025 (UTC) using following computational environment","code":"sessionInfo() ## R version 4.5.1 (2025-06-13) ## Platform: x86_64-pc-linux-gnu ## Running under: Ubuntu 24.04.3 LTS ##  ## Matrix products: default ## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 ##  ## locale: ##  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        ##  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    ##  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           ## [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    ##  ## time zone: UTC ## tzcode source: system (glibc) ##  ## attached base packages: ## [1] stats     graphics  grDevices utils     datasets  methods   base      ##  ## other attached packages: ## [1] scales_1.4.0                   ggdist_3.3.3                   ## [3] ggplot2_4.0.0                  PublicationBiasBenchmark_0.1.0 ## [5] devtools_2.4.6                 usethis_3.2.1                  ##  ## loaded via a namespace (and not attached): ##  [1] gtable_0.3.6         xfun_0.53            bslib_0.9.0          ##  [4] htmlwidgets_1.6.4    remotes_2.5.0        lattice_0.22-7       ##  [7] vctrs_0.6.5          tools_4.5.1          Rdpack_2.6.4         ## [10] generics_0.1.4       curl_7.0.0           sandwich_3.1-1       ## [13] tibble_3.3.0         pkgconfig_2.0.3      RColorBrewer_1.1-3   ## [16] S7_0.2.0             desc_1.4.3           distributional_0.5.0 ## [19] lifecycle_1.0.4      compiler_4.5.1       farver_2.1.2         ## [22] stringr_1.5.2        textshaping_1.0.4    htmltools_0.5.8.1    ## [25] sass_0.4.10          clubSandwich_0.6.1   yaml_2.3.10          ## [28] pillar_1.11.1        pkgdown_2.1.3        jquerylib_0.1.4      ## [31] ellipsis_0.3.2       cachem_1.1.0         sessioninfo_1.2.3    ## [34] digest_0.6.37        stringi_1.8.7        purrr_1.1.0          ## [37] labeling_0.4.3       fastmap_1.2.0        grid_4.5.1           ## [40] cli_3.6.5            magrittr_2.0.4       triebeard_0.4.1      ## [43] crul_1.6.0           pkgbuild_1.4.8       osfr_0.2.9           ## [46] withr_3.0.2          rmarkdown_2.30       httr_1.4.7           ## [49] ragg_1.5.0           zoo_1.8-14           kableExtra_1.4.0     ## [52] memoise_2.0.1        evaluate_1.0.5       knitr_1.50           ## [55] rbibutils_2.3        viridisLite_0.4.2    rlang_1.1.6          ## [58] urltools_1.7.3.1     Rcpp_1.1.0           glue_1.8.0           ## [61] httpcode_0.3.0       xml2_1.4.0           pkgload_1.4.1        ## [64] svglite_2.2.1        rstudioapi_0.17.1    jsonlite_2.0.0       ## [67] R6_2.6.1             systemfonts_1.3.1    fs_1.6.6"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"complete-results","dir":"Articles","previous_headings":"","what":"Complete Results","title":"Results: Alinaghi (2018)","text":"results based Alinaghi2018 (2018) data-generating mechanism total 81 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"average-performance","dir":"Articles","previous_headings":"Complete Results","what":"Average Performance","title":"Results: Alinaghi (2018)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"by-condition-performance-conditional-on-method-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Alinaghi (2018)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"by-condition-performance-replacement-in-case-of-non-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Alinaghi (2018)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"subset-fixed-effects","dir":"Articles","previous_headings":"","what":"Subset: Fixed Effects","title":"Results: Alinaghi (2018)","text":"results based Alinaghi2018 (2018) data-generating mechanism total 27 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"average-performance-1","dir":"Articles","previous_headings":"Subset: Fixed Effects","what":"Average Performance","title":"Results: Alinaghi (2018)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"by-condition-performance-conditional-on-method-convergence-1","dir":"Articles","previous_headings":"Subset: Fixed Effects","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Alinaghi (2018)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-1","dir":"Articles","previous_headings":"Subset: Fixed Effects","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Alinaghi (2018)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"subset-random-effects","dir":"Articles","previous_headings":"","what":"Subset: Random Effects","title":"Results: Alinaghi (2018)","text":"results based Alinaghi2018 (2018) data-generating mechanism total 27 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"average-performance-2","dir":"Articles","previous_headings":"Subset: Random Effects","what":"Average Performance","title":"Results: Alinaghi (2018)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"by-condition-performance-conditional-on-method-convergence-2","dir":"Articles","previous_headings":"Subset: Random Effects","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Alinaghi (2018)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-2","dir":"Articles","previous_headings":"Subset: Random Effects","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Alinaghi (2018)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"subset-panel-random-effects","dir":"Articles","previous_headings":"","what":"Subset: Panel Random Effects","title":"Results: Alinaghi (2018)","text":"results based Alinaghi2018 (2018) data-generating mechanism total 27 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"average-performance-3","dir":"Articles","previous_headings":"Subset: Panel Random Effects","what":"Average Performance","title":"Results: Alinaghi (2018)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"by-condition-performance-conditional-on-method-convergence-3","dir":"Articles","previous_headings":"Subset: Panel Random Effects","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Alinaghi (2018)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-3","dir":"Articles","previous_headings":"Subset: Panel Random Effects","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Alinaghi (2018)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Alinaghi2018.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Results: Alinaghi (2018)","text":"report compiled Thu Oct 16 08:33:55 2025 (UTC) using following computational environment","code":"sessionInfo() ## R version 4.5.1 (2025-06-13) ## Platform: x86_64-pc-linux-gnu ## Running under: Ubuntu 24.04.3 LTS ##  ## Matrix products: default ## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 ##  ## locale: ##  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        ##  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    ##  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           ## [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    ##  ## time zone: UTC ## tzcode source: system (glibc) ##  ## attached base packages: ## [1] stats     graphics  grDevices utils     datasets  methods   base      ##  ## other attached packages: ## [1] scales_1.4.0                   ggdist_3.3.3                   ## [3] ggplot2_4.0.0                  PublicationBiasBenchmark_0.1.0 ## [5] devtools_2.4.6                 usethis_3.2.1                  ##  ## loaded via a namespace (and not attached): ##  [1] gtable_0.3.6         xfun_0.53            bslib_0.9.0          ##  [4] htmlwidgets_1.6.4    remotes_2.5.0        lattice_0.22-7       ##  [7] vctrs_0.6.5          tools_4.5.1          Rdpack_2.6.4         ## [10] generics_0.1.4       curl_7.0.0           sandwich_3.1-1       ## [13] tibble_3.3.0         pkgconfig_2.0.3      RColorBrewer_1.1-3   ## [16] S7_0.2.0             desc_1.4.3           distributional_0.5.0 ## [19] lifecycle_1.0.4      compiler_4.5.1       farver_2.1.2         ## [22] stringr_1.5.2        textshaping_1.0.4    htmltools_0.5.8.1    ## [25] sass_0.4.10          clubSandwich_0.6.1   yaml_2.3.10          ## [28] pillar_1.11.1        pkgdown_2.1.3        jquerylib_0.1.4      ## [31] ellipsis_0.3.2       cachem_1.1.0         sessioninfo_1.2.3    ## [34] digest_0.6.37        stringi_1.8.7        purrr_1.1.0          ## [37] labeling_0.4.3       fastmap_1.2.0        grid_4.5.1           ## [40] cli_3.6.5            magrittr_2.0.4       triebeard_0.4.1      ## [43] crul_1.6.0           pkgbuild_1.4.8       osfr_0.2.9           ## [46] withr_3.0.2          rmarkdown_2.30       httr_1.4.7           ## [49] ragg_1.5.0           zoo_1.8-14           kableExtra_1.4.0     ## [52] memoise_2.0.1        evaluate_1.0.5       knitr_1.50           ## [55] rbibutils_2.3        viridisLite_0.4.2    rlang_1.1.6          ## [58] urltools_1.7.3.1     Rcpp_1.1.0           glue_1.8.0           ## [61] httpcode_0.3.0       xml2_1.4.0           pkgload_1.4.1        ## [64] svglite_2.2.1        rstudioapi_0.17.1    jsonlite_2.0.0       ## [67] R6_2.6.1             systemfonts_1.3.1    fs_1.6.6"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Bom2019.html","id":"complete-results","dir":"Articles","previous_headings":"","what":"Complete Results","title":"Results: Bom (2019)","text":"results based Bom2019 (2019) data-generating mechanism total 504 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Bom2019.html","id":"average-performance","dir":"Articles","previous_headings":"Complete Results","what":"Average Performance","title":"Results: Bom (2019)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Bom2019.html","id":"by-condition-performance-conditional-on-method-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Bom (2019)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Bom2019.html","id":"by-condition-performance-replacement-in-case-of-non-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Bom (2019)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Bom2019.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Results: Bom (2019)","text":"report compiled Thu Oct 16 08:35:33 2025 (UTC) using following computational environment","code":"sessionInfo() ## R version 4.5.1 (2025-06-13) ## Platform: x86_64-pc-linux-gnu ## Running under: Ubuntu 24.04.3 LTS ##  ## Matrix products: default ## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 ##  ## locale: ##  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        ##  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    ##  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           ## [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    ##  ## time zone: UTC ## tzcode source: system (glibc) ##  ## attached base packages: ## [1] stats     graphics  grDevices utils     datasets  methods   base      ##  ## other attached packages: ## [1] scales_1.4.0                   ggdist_3.3.3                   ## [3] ggplot2_4.0.0                  PublicationBiasBenchmark_0.1.0 ## [5] devtools_2.4.6                 usethis_3.2.1                  ##  ## loaded via a namespace (and not attached): ##  [1] gtable_0.3.6         xfun_0.53            bslib_0.9.0          ##  [4] htmlwidgets_1.6.4    remotes_2.5.0        lattice_0.22-7       ##  [7] vctrs_0.6.5          tools_4.5.1          Rdpack_2.6.4         ## [10] generics_0.1.4       curl_7.0.0           sandwich_3.1-1       ## [13] tibble_3.3.0         pkgconfig_2.0.3      RColorBrewer_1.1-3   ## [16] S7_0.2.0             desc_1.4.3           distributional_0.5.0 ## [19] lifecycle_1.0.4      compiler_4.5.1       farver_2.1.2         ## [22] stringr_1.5.2        textshaping_1.0.4    htmltools_0.5.8.1    ## [25] sass_0.4.10          clubSandwich_0.6.1   yaml_2.3.10          ## [28] pillar_1.11.1        pkgdown_2.1.3        jquerylib_0.1.4      ## [31] ellipsis_0.3.2       cachem_1.1.0         sessioninfo_1.2.3    ## [34] digest_0.6.37        stringi_1.8.7        purrr_1.1.0          ## [37] labeling_0.4.3       fastmap_1.2.0        grid_4.5.1           ## [40] cli_3.6.5            magrittr_2.0.4       triebeard_0.4.1      ## [43] crul_1.6.0           pkgbuild_1.4.8       osfr_0.2.9           ## [46] withr_3.0.2          rmarkdown_2.30       httr_1.4.7           ## [49] ragg_1.5.0           zoo_1.8-14           kableExtra_1.4.0     ## [52] memoise_2.0.1        evaluate_1.0.5       knitr_1.50           ## [55] rbibutils_2.3        viridisLite_0.4.2    rlang_1.1.6          ## [58] urltools_1.7.3.1     Rcpp_1.1.0           glue_1.8.0           ## [61] httpcode_0.3.0       xml2_1.4.0           pkgload_1.4.1        ## [64] svglite_2.2.1        rstudioapi_0.17.1    jsonlite_2.0.0       ## [67] R6_2.6.1             systemfonts_1.3.1    fs_1.6.6"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"complete-results","dir":"Articles","previous_headings":"","what":"Complete Results","title":"Results: Carter (2019)","text":"results based Carter2019 (2019) data-generating mechanism total 756 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"average-performance","dir":"Articles","previous_headings":"Complete Results","what":"Average Performance","title":"Results: Carter (2019)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"by-condition-performance-conditional-on-method-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Carter (2019)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"by-condition-performance-replacement-in-case-of-non-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Carter (2019)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"subset-no-questionable-research-practices","dir":"Articles","previous_headings":"","what":"Subset: No Questionable Research Practices","title":"Results: Carter (2019)","text":"results based Carter2019 (2019) data-generating mechanism total 252 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"average-performance-1","dir":"Articles","previous_headings":"Subset: No Questionable Research Practices","what":"Average Performance","title":"Results: Carter (2019)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"by-condition-performance-conditional-on-method-convergence-1","dir":"Articles","previous_headings":"Subset: No Questionable Research Practices","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Carter (2019)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-1","dir":"Articles","previous_headings":"Subset: No Questionable Research Practices","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Carter (2019)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"subset-medium-questionable-research-practices","dir":"Articles","previous_headings":"","what":"Subset: Medium Questionable Research Practices","title":"Results: Carter (2019)","text":"results based Carter2019 (2019) data-generating mechanism total 252 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"average-performance-2","dir":"Articles","previous_headings":"Subset: Medium Questionable Research Practices","what":"Average Performance","title":"Results: Carter (2019)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"by-condition-performance-conditional-on-method-convergence-2","dir":"Articles","previous_headings":"Subset: Medium Questionable Research Practices","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Carter (2019)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-2","dir":"Articles","previous_headings":"Subset: Medium Questionable Research Practices","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Carter (2019)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"subset-high-questionable-research-practices","dir":"Articles","previous_headings":"","what":"Subset: High Questionable Research Practices","title":"Results: Carter (2019)","text":"results based Carter2019 (2019) data-generating mechanism total 252 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"average-performance-3","dir":"Articles","previous_headings":"Subset: High Questionable Research Practices","what":"Average Performance","title":"Results: Carter (2019)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"by-condition-performance-conditional-on-method-convergence-3","dir":"Articles","previous_headings":"Subset: High Questionable Research Practices","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Carter (2019)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-3","dir":"Articles","previous_headings":"Subset: High Questionable Research Practices","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Carter (2019)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Carter2019.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Results: Carter (2019)","text":"report compiled Thu Oct 16 08:38:36 2025 (UTC) using following computational environment","code":"sessionInfo() ## R version 4.5.1 (2025-06-13) ## Platform: x86_64-pc-linux-gnu ## Running under: Ubuntu 24.04.3 LTS ##  ## Matrix products: default ## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 ##  ## locale: ##  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        ##  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    ##  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           ## [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    ##  ## time zone: UTC ## tzcode source: system (glibc) ##  ## attached base packages: ## [1] stats     graphics  grDevices utils     datasets  methods   base      ##  ## other attached packages: ## [1] scales_1.4.0                   ggdist_3.3.3                   ## [3] ggplot2_4.0.0                  PublicationBiasBenchmark_0.1.0 ## [5] devtools_2.4.6                 usethis_3.2.1                  ##  ## loaded via a namespace (and not attached): ##  [1] gtable_0.3.6         xfun_0.53            bslib_0.9.0          ##  [4] htmlwidgets_1.6.4    remotes_2.5.0        lattice_0.22-7       ##  [7] vctrs_0.6.5          tools_4.5.1          Rdpack_2.6.4         ## [10] generics_0.1.4       curl_7.0.0           sandwich_3.1-1       ## [13] tibble_3.3.0         pkgconfig_2.0.3      RColorBrewer_1.1-3   ## [16] S7_0.2.0             desc_1.4.3           distributional_0.5.0 ## [19] lifecycle_1.0.4      compiler_4.5.1       farver_2.1.2         ## [22] stringr_1.5.2        textshaping_1.0.4    htmltools_0.5.8.1    ## [25] sass_0.4.10          clubSandwich_0.6.1   yaml_2.3.10          ## [28] pillar_1.11.1        pkgdown_2.1.3        jquerylib_0.1.4      ## [31] ellipsis_0.3.2       cachem_1.1.0         sessioninfo_1.2.3    ## [34] digest_0.6.37        stringi_1.8.7        purrr_1.1.0          ## [37] labeling_0.4.3       fastmap_1.2.0        grid_4.5.1           ## [40] cli_3.6.5            magrittr_2.0.4       triebeard_0.4.1      ## [43] crul_1.6.0           pkgbuild_1.4.8       osfr_0.2.9           ## [46] withr_3.0.2          rmarkdown_2.30       httr_1.4.7           ## [49] ragg_1.5.0           zoo_1.8-14           kableExtra_1.4.0     ## [52] memoise_2.0.1        evaluate_1.0.5       knitr_1.50           ## [55] rbibutils_2.3        viridisLite_0.4.2    rlang_1.1.6          ## [58] urltools_1.7.3.1     Rcpp_1.1.0           glue_1.8.0           ## [61] httpcode_0.3.0       xml2_1.4.0           pkgload_1.4.1        ## [64] svglite_2.2.1        rstudioapi_0.17.1    jsonlite_2.0.0       ## [67] R6_2.6.1             systemfonts_1.3.1    fs_1.6.6"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Method_Replacement.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Method Replacement Strategy","text":"handle cases methods fail converge, method replacement strategy implemented. approach emulates data analyst might practice: sophisticated method fails converge, fall back simpler, robust method. replacement applied sequentially: first replacement method also fails, next method sequence tried, . strategy allows method performance evaluated realistic scenario non-convergence handled pragmatically. However, noted results replacement longer reflect “pure” method performance, may combine estimates multiple different methods.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Method_Replacement.html","id":"replacement-specification","dir":"Articles","previous_headings":"","what":"Replacement Specification","title":"Method Replacement Strategy","text":"table shows replacement sequence method. method fails converge, methods “Replacement Sequence” column applied order convergence achieved replacement options exhausted. em-dash — indicates replacement (method assumed always converge). choice replacement methods based : Similarity: Replacement methods conceptually similar original method possible. Robustness: Simpler methods generally robust convergence issues. Practicality: sequence reflects applied researchers might reasonably faced convergence failures.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"complete-results","dir":"Articles","previous_headings":"","what":"Complete Results","title":"Results: Stanley (2017)","text":"results based Stanley2017 (2017) data-generating mechanism total 324 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"average-performance","dir":"Articles","previous_headings":"Complete Results","what":"Average Performance","title":"Results: Stanley (2017)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using average empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution empirical standard error values corresponding outcome scale. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using average Interval Score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution empirical standard error values corresponding outcome scale. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution CI width values corresponding outcome scale. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"by-condition-performance-conditional-on-method-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Stanley (2017)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using interval score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average 95% CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution 95% CI width values corresponding outcome scale.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"by-condition-performance-replacement-in-case-of-non-convergence","dir":"Articles","previous_headings":"Complete Results","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Stanley (2017)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Methods compared using condition-wise ranks. Direct comparison using average RMSE possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution RMSE values corresponding outcome scale.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Methods compared using condition-wise ranks. Direct comparison using average bias possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Methods compared using condition-wise ranks. Direct comparison using empirical standard error possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Methods compared using condition-wise ranks. Direct comparison using interval score possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution bias values corresponding outcome scale.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. Methods compared using condition-wise ranks. Direct comparison using average 95% CI width possible data-generating mechanisms differ outcome scale. See DGM-specific results (subresults) see distribution 95% CI width values corresponding outcome scale.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"subset-standardized-mean-difference-effect-sizes","dir":"Articles","previous_headings":"","what":"Subset: Standardized Mean Difference Effect Sizes","title":"Results: Stanley (2017)","text":"results based Stanley2017 (2017) data-generating mechanism total 270 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"average-performance-1","dir":"Articles","previous_headings":"Subset: Standardized Mean Difference Effect Sizes","what":"Average Performance","title":"Results: Stanley (2017)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"by-condition-performance-conditional-on-method-convergence-1","dir":"Articles","previous_headings":"Subset: Standardized Mean Difference Effect Sizes","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Stanley (2017)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-1","dir":"Articles","previous_headings":"Subset: Standardized Mean Difference Effect Sizes","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Stanley (2017)","text":"results incorporate method replacement handle non-convergence. method fails converge, results replaced results simpler method (e.g., random-effects meta-analysis without publication bias adjustment). emulates data analyst may practice case method converge. However, note results correspond “pure” method performance might combine multiple different methods. See Method Replacement Strategy details method replacement specification. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"subset-log-odd-ratio-effect-sizes","dir":"Articles","previous_headings":"","what":"Subset: Log Odd Ratio Effect Sizes","title":"Results: Stanley (2017)","text":"results based Stanley2017 (2017) data-generating mechanism total 54 conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"average-performance-2","dir":"Articles","previous_headings":"Subset: Log Odd Ratio Effect Sizes","what":"Average Performance","title":"Results: Stanley (2017)","text":"Method performance measures aggregated across simulated conditions provide overall impression method performance. However, keep mind method high overall ranking necessarily “best” method particular application. select suitable method application, consider also non-aggregated performance measures conditions relevant application. RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. 95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%. 95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method. positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method. negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method. type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%. power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"by-condition-performance-conditional-on-method-convergence-2","dir":"Articles","previous_headings":"Subset: Log Odd Ratio Effect Sizes","what":"By-Condition Performance (Conditional on Method Convergence)","title":"Results: Stanley (2017)","text":"results conditional method convergence. Note methods might differ convergence rate therefore compared data sets. Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"by-condition-performance-replacement-in-case-of-non-convergence-2","dir":"Articles","previous_headings":"Subset: Log Odd Ratio Effect Sizes","what":"By-Condition Performance (Replacement in Case of Non-Convergence)","title":"Results: Stanley (2017)","text":"Convergence RMSE Bias Empirical SE Interval Score 95% CI Coverage 95% CI Width Log Positive Likelihood Ratio Log Negative Likelihood Ratio Type Error Rate Power   RMSE (Root Mean Square Error) overall summary measure estimation performance combines bias empirical SE. RMSE square root average squared difference meta-analytic estimate true effect across simulation runs. lower RMSE indicates better method. Values larger 0.5 visualized 0.5.  Bias average difference meta-analytic estimate true effect across simulation runs. Ideally, value close 0. Values lower -0.5 larger 0.5 visualized -0.5 0.5 respectively.  empirical SE standard deviation meta-analytic estimate across simulation runs. lower empirical SE indicates less variability better method performance. Values larger 0.5 visualized 0.5.  interval score measures accuracy confidence interval combining width coverage. penalizes intervals wide fail include true value. lower interval score indicates better method. Values larger 100 visualized 100.  95% CI coverage proportion simulation runs 95% confidence interval contained true effect. Ideally, value close nominal level 95%.  95% CI width average length 95% confidence interval true effect. lower average 95% CI length indicates better method.  positive likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much significant test result changes odds alternative hypothesis versus null hypothesis. useful method positive likelihood ratio greater 1 (log positive likelihood ratio greater 0). higher (log) positive likelihood ratio indicates better method.  negative likelihood ratio overall summary measure hypothesis testing performance combines power type error rate. indicates much non-significant test result changes odds alternative hypothesis versus null hypothesis. useful method negative likelihood ratio less 1 (log negative likelihood ratio less 0). lower (log) negative likelihood ratio indicates better method.  type error rate proportion simulation runs null hypothesis effect incorrectly rejected true. Ideally, value close nominal level 5%.  power proportion simulation runs null hypothesis effect correctly rejected alternative hypothesis true. higher power indicates better method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Results_Stanley2017.html","id":"session-info","dir":"Articles","previous_headings":"","what":"Session Info","title":"Results: Stanley (2017)","text":"report compiled Thu Oct 16 08:40:56 2025 (UTC) using following computational environment","code":"sessionInfo() ## R version 4.5.1 (2025-06-13) ## Platform: x86_64-pc-linux-gnu ## Running under: Ubuntu 24.04.3 LTS ##  ## Matrix products: default ## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3  ## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0 ##  ## locale: ##  [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8        ##  [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8    ##  [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C           ## [10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C    ##  ## time zone: UTC ## tzcode source: system (glibc) ##  ## attached base packages: ## [1] stats     graphics  grDevices utils     datasets  methods   base      ##  ## other attached packages: ## [1] scales_1.4.0                   ggdist_3.3.3                   ## [3] ggplot2_4.0.0                  PublicationBiasBenchmark_0.1.0 ## [5] devtools_2.4.6                 usethis_3.2.1                  ##  ## loaded via a namespace (and not attached): ##  [1] gtable_0.3.6         xfun_0.53            bslib_0.9.0          ##  [4] htmlwidgets_1.6.4    remotes_2.5.0        lattice_0.22-7       ##  [7] vctrs_0.6.5          tools_4.5.1          Rdpack_2.6.4         ## [10] generics_0.1.4       curl_7.0.0           sandwich_3.1-1       ## [13] tibble_3.3.0         pkgconfig_2.0.3      RColorBrewer_1.1-3   ## [16] S7_0.2.0             desc_1.4.3           distributional_0.5.0 ## [19] lifecycle_1.0.4      compiler_4.5.1       farver_2.1.2         ## [22] stringr_1.5.2        textshaping_1.0.4    htmltools_0.5.8.1    ## [25] sass_0.4.10          clubSandwich_0.6.1   yaml_2.3.10          ## [28] pillar_1.11.1        pkgdown_2.1.3        jquerylib_0.1.4      ## [31] ellipsis_0.3.2       cachem_1.1.0         sessioninfo_1.2.3    ## [34] digest_0.6.37        stringi_1.8.7        purrr_1.1.0          ## [37] labeling_0.4.3       fastmap_1.2.0        grid_4.5.1           ## [40] cli_3.6.5            magrittr_2.0.4       triebeard_0.4.1      ## [43] crul_1.6.0           pkgbuild_1.4.8       osfr_0.2.9           ## [46] withr_3.0.2          rmarkdown_2.30       httr_1.4.7           ## [49] ragg_1.5.0           zoo_1.8-14           kableExtra_1.4.0     ## [52] memoise_2.0.1        evaluate_1.0.5       knitr_1.50           ## [55] rbibutils_2.3        viridisLite_0.4.2    rlang_1.1.6          ## [58] urltools_1.7.3.1     Rcpp_1.1.0           glue_1.8.0           ## [61] httpcode_0.3.0       xml2_1.4.0           pkgload_1.4.1        ## [64] svglite_2.2.1        rstudioapi_0.17.1    jsonlite_2.0.0       ## [67] R6_2.6.1             systemfonts_1.3.1    fs_1.6.6"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Measures.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Using Precomputed Measures","text":"package provides precomputed performance measures multiple publication bias correction methods evaluated different simulation conditions. measures include: Bias: Average difference estimates true effect sizes Relative Bias: Bias expressed proportion true effect size MSE (Mean Square Error): Average squared difference estimates true values RMSE (Root Mean Square Error): Square root MSE, measuring overall estimation accuracy Empirical Variance: Variability estimates across repetitions Empirical SE: Standard deviation estimates across repetitions Coverage: Proportion confidence intervals containing true effect Mean CI Width: Average width confidence intervals Interval Score: Proper scoring rule probabilistic interval forecasts Power: Proportion significant results (conditions null hypothesis true corresponds Type error rate) Positive Likelihood Ratio: Indication much significant test result changes odds H1 versus H0 Negative Likelihood Ratio: Indication much non-significant test result changes odds H1 versus H0 Convergence: Proportion successful method convergence precomputed results organized data-generating mechanism (DGM), DGM representing different patterns publication bias meta-analytic conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Measures.html","id":"available-data-generating-mechanisms","dir":"Articles","previous_headings":"","what":"Available Data-Generating Mechanisms","title":"Using Precomputed Measures","text":"package includes precomputed measures several DGMs. See Adding New DGMs vignette details individual DGMs simulation designs. can view specific conditions DGM using dgm_conditions() function:","code":"# View conditions for the Stanley2017 DGM conditions <- dgm_conditions(\"Stanley2017\") head(conditions)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Measures.html","id":"downloading-precomputed-measures","dir":"Articles","previous_headings":"","what":"Downloading Precomputed Measures","title":"Using Precomputed Measures","text":"accessing precomputed measures, need download package repository. download_dgm_measures() function downloads measures specified DGM: measures downloaded local cache directory automatically available subsequent analysis. need download , unless want update newer version.","code":"# Download precomputed measures for the Stanley2017 DGM download_dgm_measures(\"Stanley2017\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Measures.html","id":"retrieving-precomputed-measures","dir":"Articles","previous_headings":"","what":"Retrieving Precomputed Measures","title":"Using Precomputed Measures","text":"downloaded, can retrieve precomputed measures using retrieve_dgm_measures() function. function offers flexible filtering options extract exactly data need.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Measures.html","id":"retrieving-specific-measures","dir":"Articles","previous_headings":"Retrieving Precomputed Measures","what":"Retrieving Specific Measures","title":"Using Precomputed Measures","text":"can retrieve measures specific method condition: measure argument can measure function names listed measures() documentation.","code":"# Retrieve bias measures for RMA method in condition 1 retrieve_dgm_measures(   dgm = \"Stanley2017\",   measure = \"bias\",   method = \"RMA\",   condition_id = 1 )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Measures.html","id":"retrieving-all-measures","dir":"Articles","previous_headings":"Retrieving Precomputed Measures","what":"Retrieving All Measures","title":"Using Precomputed Measures","text":"retrieve measures across conditions methods, simply omit filtering arguments: returns comprehensive data frame columns: dgm: Data-generating mechanism name condition_id: Simulation condition identifier method: Publication bias correction method name method_setting: Specific method configuration bias, bias_mcse, rmse, rmse_mcse, …: Performance measures Monte Carlo standard errors","code":"# Retrieve all measures across all conditions and methods df <- retrieve_dgm_measures(\"Stanley2017\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Measures.html","id":"filtering-by-method-or-setting","dir":"Articles","previous_headings":"Retrieving Precomputed Measures","what":"Filtering by Method or Setting","title":"Using Precomputed Measures","text":"can also filter method name method setting:","code":"# Retrieve all measures for PET-PEESE method pet_peese_results <- retrieve_dgm_measures(   dgm = \"Stanley2017\",   method = \"PETPEESE\" )  # Retrieve measures for a specific method setting rma_reml_results <- retrieve_dgm_measures(   dgm = \"Stanley2017\",   method = \"RMA\",   method_setting = \"default\" )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Measures.html","id":"visualizing-precomputed-results","dir":"Articles","previous_headings":"","what":"Visualizing Precomputed Results","title":"Using Precomputed Measures","text":"retrieved measures, can create comprehensive visualizations compare method performance. ’s example creates multi-panel plot comparing methods across conditions:","code":"# Retrieve all measures across all conditions and methods df <- retrieve_dgm_measures(\"Stanley2017\")  # Retrieve conditions to identify null vs. alternative hypotheses conditions <- dgm_conditions(\"Stanley2017\")  # Create readable method labels df$label <- with(df, paste0(method, \" (\", method_setting, \")\"))  # Identify conditions under null hypothesis (H₀: mean effect = 0) df$H0 <- df$condition_id %in% conditions$condition_id[conditions$mean_effect == 0]  # Create multi-panel visualization par(mfrow = c(3, 2)) par(mar = c(4, 10, 1, 1))  # Panel 1: Convergence rates boxplot(convergence * 100 ~ label,          horizontal = TRUE,          las = 1,          ylab = \"\",          ylim = c(20, 100),          data = df,          xlab = \"Convergence (%)\")  # Panel 2: RMSE boxplot(rmse ~ label,          horizontal = TRUE,          las = 1,          ylab = \"\",          ylim = c(0, 0.6),          data = df,          xlab = \"RMSE\")  # Panel 3: Bias boxplot(bias ~ label,          horizontal = TRUE,          las = 1,          ylab = \"\",          ylim = c(-0.25, 0.25),          data = df,          xlab = \"Bias\") abline(v = 0, lty = 3)  # Reference line at zero  # Panel 4: Coverage boxplot(coverage * 100 ~ label,          horizontal = TRUE,          las = 1,          ylab = \"\",          ylim = c(30, 100),          data = df,          xlab = \"95% CI Coverage (%)\") abline(v = 95, lty = 3)  # Reference line at nominal level  # Panel 5: Type I Error Rate (H₀ conditions only) boxplot(power * 100 ~ label,          horizontal = TRUE,          las = 1,          ylab = \"\",          ylim = c(0, 40),          data = df[df$H0, ],          xlab = \"Type I Error Rate (%)\") abline(v = 5, lty = 3)  # Reference line at α = 0.05  # Panel 6: Power (H₁ conditions only) boxplot(power * 100 ~ label,          horizontal = TRUE,          las = 1,          ylab = \"\",          ylim = c(10, 100),          data = df[!df$H0, ],          xlab = \"Power (%)\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Results.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Using Precomputed Results","text":"package provides access raw simulation results publication bias correction methods evaluated across different data-generating mechanisms (DGMs). result represents single application method simulated meta-analytic dataset (.e., iteration given DGM). Raw results contain detailed output individual simulation repetition, including: estimate (numeric): meta-analytic effect size estimate method application standard_error (numeric): Standard error estimate ci_lower (numeric), ci_upper (numeric): Lower upper bounds 95% confidence interval p_value (numeric): P-value testing null hypothesis effect (applicable) BF (numeric): Bayes factor alternative hypothesis assuming presence effect (applicable) convergence (logical): Whether method successfully converged note (character): Additional notes describing convergence issues warnings Method-Specific Outputs: Additional columns specific method (e.g., bias_coefficient, tau, …) Unlike precomputed measures summarize performance across repetitions, raw results allow : Compute custom performance metrics included standard measures Examine distribution estimates across simulations Investigate specific cases methods fail perform poorly Create custom visualizations method behavior Conduct sensitivity analyses different criteria","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Results.html","id":"available-data-generating-mechanisms","dir":"Articles","previous_headings":"","what":"Available Data-Generating Mechanisms","title":"Using Precomputed Results","text":"package includes precomputed results several DGMs. See Adding New DGMs vignette details individual DGMs simulation designs. can view specific conditions DGM using dgm_conditions() function: condition represents unique combination simulation parameters (e.g., true effect size, heterogeneity, number studies, publication bias severity).","code":"# View conditions for the Stanley2017 DGM conditions <- dgm_conditions(\"Stanley2017\") head(conditions)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Results.html","id":"downloading-precomputed-results","dir":"Articles","previous_headings":"","what":"Downloading Precomputed Results","title":"Using Precomputed Results","text":"accessing precomputed results, need download package repository. download_dgm() function downloads raw results specified DGM: Note: Raw results files significantly larger summarized measures files. DGM may require several hundred megabytes storage space. results downloaded local cache directory automatically available subsequent analysis. need download , unless want update newer version. download function display progress information total size files downloaded.","code":"# Download precomputed results for the Stanley2017 DGM download_dgm_results(\"Stanley2017\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Results.html","id":"retrieving-precomputed-results","dir":"Articles","previous_headings":"","what":"Retrieving Precomputed Results","title":"Using Precomputed Results","text":"downloaded, can retrieve precomputed results using retrieve_dgm_results() function. function offers flexible filtering options extract exactly data need without loading entire dataset memory.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Results.html","id":"retrieving-a-specific-repetition","dir":"Articles","previous_headings":"Retrieving Precomputed Results","what":"Retrieving a Specific Repetition","title":"Using Precomputed Results","text":"can retrieve results specific method, condition, repetition: returns data frame single row containing output applying RMA method first simulated dataset condition 1.","code":"# Retrieve results for the first repetition of condition 1 for RMA method retrieve_dgm_results(   dgm = \"Stanley2017\",   method = \"RMA\",   condition_id = 1,   repetition_id = 1 )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Results.html","id":"retrieving-all-repetitions-for-a-condition","dir":"Articles","previous_headings":"Retrieving Precomputed Results","what":"Retrieving All Repetitions for a Condition","title":"Using Precomputed Results","text":"retrieve repetitions specific condition method:","code":"# Retrieve all repetitions for condition 1 of RMA method condition_1_results <- retrieve_dgm_results(   dgm = \"Stanley2017\",   method = \"RMA\",   condition_id = 1 )  # Examine the distribution of estimates hist(condition_1_results$estimate,       main = \"Distribution of RMA Estimates\",      xlab = \"Effect Size Estimate\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Results.html","id":"retrieving-all-results","dir":"Articles","previous_headings":"Retrieving Precomputed Results","what":"Retrieving All Results","title":"Using Precomputed Results","text":"retrieve results across conditions, methods, repetitions, simply omit filtering arguments:","code":"# Retrieve all results df <- retrieve_dgm_results(\"Stanley2017\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Precomputed_Results.html","id":"filtering-by-method-or-setting","dir":"Articles","previous_headings":"Retrieving Precomputed Results","what":"Filtering by Method or Setting","title":"Using Precomputed Results","text":"can filter results specific methods method settings:","code":"# Retrieve all results for PET-PEESE method pet_peese_results <- retrieve_dgm_results(   dgm = \"Stanley2017\",   method = \"PETPEESE\" )  # Retrieve results for a specific method setting rma_reml_results <- retrieve_dgm_results(   dgm = \"Stanley2017\",   method = \"RMA\",   method_setting = \"default\" )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Using Presimulated Datasets","text":"package provides access presimulated meta-analytic datasets used benchmark. dataset represents collection studies (effect sizes standard errors) generated according specific simulation conditions. exact datasets benchmark methods applied.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"what-are-presimulated-datasets","dir":"Articles","previous_headings":"Overview","what":"What Are Presimulated Datasets?","title":"Using Presimulated Datasets","text":"Presimulated datasets contain raw study-level data meta-analyses, including: yi (numeric): effect size estimate study sei (numeric): Standard error yi ni (integer): Total sample size estimate (e.g., sum groups applicable) es_type (character): Effect size type, used disambiguate scale yi. Currently used values \"SMD\" (standardized mean difference / Cohen’s d), \"logOR\" (log odds ratio), \"none\" (unspecified generic continuous coefficient) study_id (integer/character, optional): Identifier primary study/cluster DGM yields multiple estimates per study (e.g., Alinaghi2018). absent, row treated independent study condition_id (integer): Identifier condition repetition_id (integer): Identifier simulation repetition dataset represents one simulated meta-analysis specific conditions (e.g., true effect size, heterogeneity, number studies, publication bias pattern).","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"why-use-presimulated-datasets","dir":"Articles","previous_headings":"Overview","what":"Why Use Presimulated Datasets?","title":"Using Presimulated Datasets","text":"Accessing presimulated datasets allows : Test new methods: Apply publication bias correction methods data used benchmark Verify results: Re-run existing methods verify benchmark results Conduct custom analyses: Investigate characteristics simulated studies (e.g., funnel plot asymmetry, p-value distributions) Compare approaches: Test alternative implementations parameter settings Examine specific cases: Analyze datasets particular conditions repetitions detail","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"available-data-generating-mechanisms","dir":"Articles","previous_headings":"","what":"Available Data-Generating Mechanisms","title":"Using Presimulated Datasets","text":"package includes presimulated datasets several DGMs. See Adding New DGMs vignette details individual DGMs simulation designs. can view specific conditions DGM using dgm_conditions() function: condition represents unique combination simulation parameters determines meta-analytic datasets generated.","code":"# View conditions for the Stanley2017 DGM conditions <- dgm_conditions(\"Stanley2017\") head(conditions)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"downloading-presimulated-datasets","dir":"Articles","previous_headings":"","what":"Downloading Presimulated Datasets","title":"Using Presimulated Datasets","text":"accessing presimulated datasets, need download package repository. download_dgm_datasets() function downloads datasets specified DGM: Note: Dataset files can quite large contain individual study data across many simulation repetitions. DGM may require several gigabytes storage space.","code":"# Download presimulated datasets for the Stanley2017 DGM download_dgm_datasets(\"Stanley2017\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"specifying-download-location","dir":"Articles","previous_headings":"Downloading Presimulated Datasets","what":"Specifying Download Location","title":"Using Presimulated Datasets","text":"default, datasets downloaded package’s default simulation directory. can change location using package options: datasets downloaded local cache directory automatically available subsequent analysis. need download , unless want update newer version.","code":"# Set custom download directory PublicationBiasBenchmark.options(simulation_directory = \"/path/to/your/directory\")  # Then download datasets download_dgm_datasets(\"Stanley2017\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"retrieving-presimulated-datasets","dir":"Articles","previous_headings":"","what":"Retrieving Presimulated Datasets","title":"Using Presimulated Datasets","text":"downloaded, can retrieve presimulated datasets using retrieve_dgm_dataset() function. function allows extract specific simulation repetitions conditions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"retrieving-a-single-repetition","dir":"Articles","previous_headings":"Retrieving Presimulated Datasets","what":"Retrieving a Single Repetition","title":"Using Presimulated Datasets","text":"can retrieve specific simulated meta-analytic dataset specifying condition repetition: returns data frame containing study-level data (effect sizes yi, standard errors sei, …) specific simulated meta-analysis.","code":"# Retrieve first repetition of condition 1 dataset <- retrieve_dgm_dataset(   dgm = \"Stanley2017\",   condition_id = 1,   repetition_id = 1 )  # Examine the dataset structure head(dataset) str(dataset)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/articles/Using_Presimulated_Datasets.html","id":"retrieving-all-repetitions-for-a-condition","dir":"Articles","previous_headings":"Retrieving Presimulated Datasets","what":"Retrieving All Repetitions for a Condition","title":"Using Presimulated Datasets","text":"retrieve simulation repetitions specific condition, omit repetition_id argument: useful want apply method multiple repetitions without repeatedly calling retrieve function.","code":"# Retrieve all repetitions for condition 1 all_reps <- retrieve_dgm_dataset(   dgm = \"Stanley2017\",   condition_id = 1 )  # Check how many repetitions are available length(unique(all_reps$repetition_id))  # Extract data for a specific repetition rep_5 <- all_reps[all_reps$repetition_id == 5, ]"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"František Bartoš. Author, maintainer. Samuel Pawel. Author. Björn S. Siepe. Author.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Bartoš F, Pawel S, Siepe B (2025). PublicationBiasBenchmark: Benchmark Methods Publication Bias Correction. R package version 0.1.0, https://github.com/FBartos/PublicationBiasBenchmark.","code":"@Manual{,   title = {PublicationBiasBenchmark: Benchmark Methods for Publication Bias Correction},   author = {František Bartoš and Samuel Pawel and Björn S. Siepe},   year = {2025},   note = {R package version 0.1.0},   url = {https://github.com/FBartos/PublicationBiasBenchmark}, }"},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"publicationbiasbenchmark","dir":"","previous_headings":"","what":"PublicationBiasBenchmark","title":"Benchmark Methods for Publication Bias Correction","text":"PublicationBiasBenchmark R package benchmarking publication bias correction methods simulation studies. provides: - Predefined data-generating mechanisms literature - Functions running meta-analytic methods simulated data - Pre-simulated datasets pre-computed results reproducible benchmarks - Tools visualizing comparing method performance datasets results hosted OSF: https://doi.org/10.17605/OSF.IO/EXF3M use package research, please cite: Bartoš, F., Pawel, S., Siepe, B. S. (2025). Rethinking Simulation Studies: Living Synthetic Benchmarks Cumulative Methodological Research. Working paper. https://github.com/FBartos/PublicationBiasBenchmark BibTeX entry given Overviews benchmark results available articles package website: Overall Results Stanley (2017) Alinaghi (2018) Bom (2019) Carter (2019) Contributor guidelines extending package data-generating mechanisms, methods, results available : add new data-generating mechanism add new method add new method compute method results compute method measures Illustrations use precomputed datasets, results, measures available : use presimulated datasets use precomputed results use precomputed measures rest file overviews main features package.","code":"@misc{Bartos2025,   year = {2025},   author = {Franti{\\v{s}}ek Barto{\\v{s}} and Samuel Pawel and Bj{\\\"o}rn S. Siepe},   title = {Rethinking Simulation Studies: {L}iving Synthetic Benchmarks for Cumulative Methodological Research},   url = {https://github.com/FBartos/PublicationBiasBenchmark},   note = {Working paper} }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Benchmark Methods for Publication Bias Correction","text":"","code":"# Install from GitHub remotes::install_github(\"FBartos/PublicationBiasBenchmark\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"usage","dir":"","previous_headings":"","what":"Usage","title":"Benchmark Methods for Publication Bias Correction","text":"","code":"#> Registered S3 method overwritten by 'clubSandwich': #>   method    from     #>   bread.mlm sandwich #> Data, results, and measures will be saved to 'C:/R-Packages/PublicationBiasBenchmark/resources'. #> To change the default location, use `PublicationBiasBenchmark.options(simulation_directory = `/path/`)` #>  #> Attaching package: 'PublicationBiasBenchmark' #> The following object is masked from 'package:stats': #>  #>     power"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"simulating-from-existing-data-generating-mechanisms","dir":"","previous_headings":"Usage","what":"Simulating From Existing Data-Generating Mechanisms","title":"Benchmark Methods for Publication Bias Correction","text":"","code":"# Obtain a data.frame with pre-defined conditions dgm_conditions(\"Stanley2017\")  # simulate the data from the second condition df <- simulate_dgm(\"Stanley2017\", 2)  # fit a method run_method(\"RMA\", df)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"using-pre-simulated-datasets","dir":"","previous_headings":"Usage","what":"Using Pre-Simulated Datasets","title":"Benchmark Methods for Publication Bias Correction","text":"","code":"# download the pre-simulated datasets # the default settings downloads the datasets to the `resources` directory, use # PublicationBiasBenchmark.options(simulation_directory = \"/path/\") # to change the settings download_dgm_datasets(\"no_bias\")  # retrieve first repetition of first condition from the downloaded datasets retrieve_dgm_dataset(\"no_bias\", condition_id = 1, repetition_id = 1)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"using-pre-computed-results","dir":"","previous_headings":"Usage","what":"Using Pre-Computed Results","title":"Benchmark Methods for Publication Bias Correction","text":"","code":"# download the pre-computed results download_dgm_results(\"no_bias\")  # retrieve results the first repetition of first condition of RMA from the downloaded results retrieve_dgm_results(\"no_bias\", method = \"RMA\", condition_id = 1, repetition_id = 1)  # retrieve all results across all conditions and repetitions retrieve_dgm_results(\"no_bias\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"using-pre-computed-measures","dir":"","previous_headings":"Usage","what":"Using Pre-Computed Measures","title":"Benchmark Methods for Publication Bias Correction","text":"","code":"# download the pre-computed measures download_dgm_measures(\"no_bias\")  # retrieve measures of bias the first condition of RMA from the downloaded results retrieve_dgm_measures(\"no_bias\", measure = \"bias\", method = \"RMA\", condition_id = 1)  # retrieve all measures across all conditions and measures retrieve_dgm_measures(\"no_bias\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"simulating-from-an-existing-dgm-with-custom-settings","dir":"","previous_headings":"Usage","what":"Simulating From an Existing DGM With Custom Settings","title":"Benchmark Methods for Publication Bias Correction","text":"","code":"# define sim setting sim_settings <- list(   n_studies     = 100,   mean_effect   = 0.3,   heterogeneity = 0.1 )  # check whether it is feasible # (defined outside of the function - not to decrease performance during simulation) validate_dgm_setting(\"no_bias\", sim_settings)  # simulate the data df <- simulate_dgm(\"no_bias\", sim_settings)  # fit a method run_method(\"RMA\", df)"},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"data-generating-mechanisms","dir":"","previous_headings":"Usage > Key Functions","what":"Data-Generating Mechanisms","title":"Benchmark Methods for Publication Bias Correction","text":"simulate_dgm(): Generates simulated data according specified data-generating mechanism settings. dgm_conditions(): Lists prespecified conditions data-generating mechanism. validate_dgm_setting(): Validates (custom) setting data-generating mechanism. download_dgm_datasets(): Downloads pre-simulated datasets OSF repository. retrieve_dgm_dataset(): Retrieves pre-simulated dataset given condition repetition downloaded pre-downloaded OSF repository.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"method-estimation-and-results","dir":"","previous_headings":"Usage > Key Functions","what":"Method Estimation And Results","title":"Benchmark Methods for Publication Bias Correction","text":"run_method(): Estimates method supplied data according specified settings. method_settings(): Lists prespecified settings method. download_dgm_results(): Downloads pre-computed results OSF repository. retrieve_dgm_results(): Retrieves pre-computed results given method, condition, repetition pre-downloaded OSF repository.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"performance-measures-and-results","dir":"","previous_headings":"Usage > Key Functions","what":"Performance measures And Results","title":"Benchmark Methods for Publication Bias Correction","text":"bias(), bias_mcse(), etc.: Functions compute performance measures Monte Carlo standard errors. download_dgm_measures(): Downloads pre-computed performance measures OSF repository. retrieve_dgm_measures(): Retrieves pre-computed performance measures given method, condition, repetition pre-downloaded OSF repository.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"available-data-generating-mechanisms","dir":"","previous_headings":"Usage","what":"Available Data-Generating Mechanisms","title":"Benchmark Methods for Publication Bias Correction","text":"See methods(\"dgm\") full list: \"no_bias\": Generates data without publication bias (test simulation) \"Stanley2017\": Tom D. Stanley et al. (2017) \"Alinaghi2018\": Alinaghi & Reed (2018) \"Bom2019\": Bom & Rachinger (2019) \"Carter2019\": Carter et al. (2019)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"available-methods","dir":"","previous_headings":"Usage","what":"Available Methods","title":"Benchmark Methods for Publication Bias Correction","text":"See methods(\"method\") full list: \"mean\": Mean effects size \"FMA\": Fixed effects meta-analysis \"RMA\": Random effects meta-analysis \"WLS\": Weighted Least Squares \"trimfill\": Trim--Fill (Duval & Tweedie, 2000) \"WAAPWLS\": Weighted Least Squares - Weighted Average Adequately Power Studies (Tom D. Stanley et al., 2017) \"WILS\": Weighted Iterated Least Squares (T. D. Stanley & Doucouliagos, 2024) \"PET\": Precision-Effect Test (PET) publication bias adjustment (Tom D. Stanley & Doucouliagos, 2014) \"PEESE\": Precision-Effect Estimate Standard Errors (PEESE) publication bias adjustment (Tom D. Stanley & Doucouliagos, 2014) \"PETPEESE\": Precision-Effect Test Precision-Effect Estimate Standard Errors (PET-PEESE) publication bias adjustment (Tom D. Stanley & Doucouliagos, 2014) \"EK\": Endogenous Kink (Bom & Rachinger, 2019) \"SM\": Selection Models (3PSM, 4PSM) (Vevea & Hedges, 1995) \"pcurve\": P-curve (Simonsohn et al., 2014) \"puniform\": P-uniform P-uniform* Aert & Assen (2025)  \"RoBMA\": Robust Bayesian Meta-Analysis (Bartoš et al., 2023)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"available-performance-measures","dir":"","previous_headings":"Usage","what":"Available Performance Measures","title":"Benchmark Methods for Publication Bias Correction","text":"See ?measures full list performance measures Monte Carlo standard errors/","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/index.html","id":"dgm-osf-repositories","dir":"","previous_headings":"Usage","what":"DGM OSF Repositories","title":"Benchmark Methods for Publication Bias Correction","text":"DGMs linked OSF repository (https://osf.io/exf3m/) contain following elements: data : folder containing -condition simulated datasets repetitions results : folder containing -method results conditions * repetitions measures : folder containing -measure performance methods * conditions dgm-conditions.csv : file mapping conditions corresponding settings dgm-generation.R : file code exact reproduction pre-simulated datasets dgm-sessionInfo.txt: file reproducibility details pre-simulated datasets dgm-session.log: file reproducibility details pre-simulated datasets (based sessioninfo package) results.R : file code exact reproduction method results (might method / method groups specific) results-sessionInfo.txt: file reproducibility details precomputed results (might method / method groups specific) pm-computation.R : file code computation performance measures","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/PublicationBiasBenchmark-package.html","id":null,"dir":"Reference","previous_headings":"","what":"PublicationBiasBenchmark: Benchmark Methods for Publication Bias Correction — PublicationBiasBenchmark-package","title":"PublicationBiasBenchmark: Benchmark Methods for Publication Bias Correction — PublicationBiasBenchmark-package","text":"Implements unified interface benchmarking meta-analytic publication bias correction methods simulation studies. provides 1) predefined data-generating mechanisms literature, 2) functions running meta-analytic methods simulated data, 3) pre-simulated datasets pre-computed results reproducible benchmarks, 4) tools visualizing comparing method performance.","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/PublicationBiasBenchmark-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"PublicationBiasBenchmark: Benchmark Methods for Publication Bias Correction — PublicationBiasBenchmark-package","text":"Maintainer: František Bartoš f.bartos96@gmail.com (ORCID) Authors: Samuel Pawel (ORCID) Björn S. Siepe (ORCID)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/PublicationBiasBenchmark_options.html","id":null,"dir":"Reference","previous_headings":"","what":"Options for the PublicationBiasBenchmark package — PublicationBiasBenchmark_options","title":"Options for the PublicationBiasBenchmark package — PublicationBiasBenchmark_options","text":"placeholder object functions PublicationBiasBenchmark package.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/PublicationBiasBenchmark_options.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Options for the PublicationBiasBenchmark package — PublicationBiasBenchmark_options","text":"","code":"PublicationBiasBenchmark.options(...)  PublicationBiasBenchmark.get_option(name)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/PublicationBiasBenchmark_options.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Options for the PublicationBiasBenchmark package — PublicationBiasBenchmark_options","text":"... named option(s) change - list available options, see details . name name option get current value - list available options, see details .","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/PublicationBiasBenchmark_options.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Options for the PublicationBiasBenchmark package — PublicationBiasBenchmark_options","text":"current value available PublicationBiasBenchmark options (applying changes specified) returned invisibly named list.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/PublicationBiasBenchmark_options.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Options for the PublicationBiasBenchmark package — PublicationBiasBenchmark_options","text":"\"simulation_directory\" Location benchmark data/results/measures stored \"prompt_for_download\" Whether file download ask explicit approval","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_G_squared.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate sample variance of generic statistic — S_G_squared","title":"Calculate sample variance of generic statistic — S_G_squared","text":"Calculate sample variance generic statistic","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_G_squared.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate sample variance of generic statistic — S_G_squared","text":"","code":"S_G_squared(G)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_G_squared.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate sample variance of generic statistic — S_G_squared","text":"G Vector generic statistics","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_G_squared.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate sample variance of generic statistic — S_G_squared","text":"Sample variance S_G^2","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_theta_minus_theta_squared.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate sample variance of squared errors — S_theta_minus_theta_squared","title":"Calculate sample variance of squared errors — S_theta_minus_theta_squared","text":"Calculate sample variance squared errors","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_theta_minus_theta_squared.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate sample variance of squared errors — S_theta_minus_theta_squared","text":"","code":"S_theta_minus_theta_squared(theta_hat, theta)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_theta_minus_theta_squared.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate sample variance of squared errors — S_theta_minus_theta_squared","text":"theta_hat Vector estimates theta True parameter value","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_theta_minus_theta_squared.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate sample variance of squared errors — S_theta_minus_theta_squared","text":"Sample variance S_(theta_hat - theta)^2","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_theta_squared.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate sample variance of estimates — S_theta_squared","title":"Calculate sample variance of estimates — S_theta_squared","text":"Calculate sample variance estimates","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_theta_squared.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate sample variance of estimates — S_theta_squared","text":"","code":"S_theta_squared(theta_hat)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_theta_squared.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate sample variance of estimates — S_theta_squared","text":"theta_hat Vector estimates","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_theta_squared.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate sample variance of estimates — S_theta_squared","text":"Sample variance S_theta^2","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_w_squared.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate sample variance of CI widths — S_w_squared","title":"Calculate sample variance of CI widths — S_w_squared","text":"Calculate sample variance CI widths","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_w_squared.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate sample variance of CI widths — S_w_squared","text":"","code":"S_w_squared(ci_upper, ci_lower)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_w_squared.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate sample variance of CI widths — S_w_squared","text":"ci_upper Vector upper CI bounds ci_lower Vector lower CI bounds","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/S_w_squared.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate sample variance of CI widths — S_w_squared","text":"Sample variance S_w^2","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compare_measures.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare method with Multiple Measures for a DGM — compare_measures","title":"Compare method with Multiple Measures for a DGM — compare_measures","text":"high-level wrapper function computes multiple pairwise comparison measures Data-Generating Mechanism (DGM) saves results CSV files. provides clean extensible interface comparing method performance.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compare_measures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare method with Multiple Measures for a DGM — compare_measures","text":"","code":"compare_measures(   dgm_name,   method,   method_setting,   measures = NULL,   verbose = TRUE,   estimate_col = \"estimate\",   true_effect_col = \"mean_effect\",   convergence_col = \"convergence\",   method_replacements = NULL,   n_repetitions = 1000,   overwrite = FALSE,   conditions = NULL,   path = NULL )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compare_measures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare method with Multiple Measures for a DGM — compare_measures","text":"dgm_name Character string specifying name DGM dataset download. method Character vector method names method_setting Character vector method settings, must length method measures Character vector measures compute. NULL, computes standard measures. verbose Print detailed progress calculation. estimate_col Character string specifying column name containing parameter estimates. Default \"estimate\" true_effect_col Character string specifying column name conditions data frame containing true effect sizes. Default \"mean_effect\" convergence_col Character string specifying column name containing convergence indicators. Default \"convergence\" method_replacements Named list replacement method specifications. element named \"method-method_setting\" combination (e.g., \"RMA-default\") contain named list : method: Character vector replacement method names method_setting: Character vector replacement method settings (length methods) power_test_type: Optional character vector power test types replacement method (length methods). specified, uses main power_test_type parameter multiple elements specified within vectors, replacements applied consecutively case previous replacements also failed converge. Defaults NULL, .e., omitting repetitions without converged results method--method basis. n_repetitions Number repetitions condition. Neccessary method replacement. Defaults 1000. overwrite Logical indicating whether overwrite existing files. Defaults FALSE, means missing files downloaded. conditions Data frame conditions dgm_conditions() path Character string specifying directory path datasets/results/measures saved. Defaults location specified via PublicationBiasBenchmark.get_option(\"simulation_directory\"). objects stored dgm_name/datasets, dgm_name/results, dgm_name/measures subfolders.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compare_measures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare method with Multiple Measures for a DGM — compare_measures","text":"Invisible list computed comparison data frames","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compare_single_measure.html","id":null,"dir":"Reference","previous_headings":"","what":"Compare method with a Single Measure for a DGM — compare_single_measure","title":"Compare method with a Single Measure for a DGM — compare_single_measure","text":"function provides pairwise comparison method Data-Generating Mechanisms (DGMs). compares method performance condition--condition basis using estimates. pair method, method estimate closer true value method B, gets score 1, gets 0, equal gets 0.5.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compare_single_measure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compare method with a Single Measure for a DGM — compare_single_measure","text":"","code":"compare_single_measure(   dgm_name,   measure_name,   method,   method_setting,   conditions,   estimate_col = \"estimate\",   true_effect_col = \"mean_effect\",   convergence_col = \"convergence\",   method_replacements = NULL,   n_repetitions = 1000,   overwrite = FALSE,   path = NULL,   ... )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compare_single_measure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compare method with a Single Measure for a DGM — compare_single_measure","text":"dgm_name Character string specifying name DGM dataset download. measure_name Name measure compute (e.g., \"bias\", \"mse\") method Character vector method names method_setting Character vector method settings, must length method conditions Data frame conditions dgm_conditions() estimate_col Character string specifying column name containing parameter estimates. Default \"estimate\" true_effect_col Character string specifying column name conditions data frame containing true effect sizes. Default \"mean_effect\" convergence_col Character string specifying column name containing convergence indicators. Default \"convergence\" method_replacements Named list replacement method specifications. element named \"method-method_setting\" combination (e.g., \"RMA-default\") contain named list : method: Character vector replacement method names method_setting: Character vector replacement method settings (length methods) power_test_type: Optional character vector power test types replacement method (length methods). specified, uses main power_test_type parameter multiple elements specified within vectors, replacements applied consecutively case previous replacements also failed converge. Defaults NULL, .e., omitting repetitions without converged results method--method basis. n_repetitions Number repetitions condition. Neccessary method replacement. Defaults 1000. overwrite Logical indicating whether overwrite existing files. Defaults FALSE, means missing files downloaded. path Character string specifying directory path datasets/results/measures saved. Defaults location specified via PublicationBiasBenchmark.get_option(\"simulation_directory\"). objects stored dgm_name/datasets, dgm_name/results, dgm_name/measures subfolders. ... Additional arguments passed measure functions","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compare_single_measure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compare method with a Single Measure for a DGM — compare_single_measure","text":"Data frame pairwise comparison scores long format (method_a, method_b, score)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_measures.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Multiple Performance measures for a DGM — compute_measures","title":"Compute Multiple Performance measures for a DGM — compute_measures","text":"high-level wrapper function computes multiple performance measures Data-Generating Mechanism (DGM) saves results CSV files. provides clean extensible interface computing standard simulation performance measures.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_measures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Multiple Performance measures for a DGM — compute_measures","text":"","code":"compute_measures(   dgm_name,   method,   method_setting,   measures = NULL,   verbose = TRUE,   power_test_type = \"p_value\",   power_threshold_p_value = 0.05,   power_threshold_bayes_factor = 10,   estimate_col = \"estimate\",   true_effect_col = \"mean_effect\",   ci_lower_col = \"ci_lower\",   ci_upper_col = \"ci_upper\",   p_value_col = \"p_value\",   bf_col = \"BF\",   convergence_col = \"convergence\",   method_replacements = NULL,   n_repetitions = 1000,   overwrite = FALSE,   conditions = NULL,   path = NULL )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_measures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Multiple Performance measures for a DGM — compute_measures","text":"dgm_name Character string specifying DGM name method Character vector method names method_setting Character vector method settings, must length method measures Character vector measures compute. NULL, computes standard measures. verbose Print detailed progress calculation. power_test_type Character vector specifying test type power computation: \"p_value\" (default) \"bayes_factor\" method. single value provided, repeated methods. power_threshold_p_value Numeric threshold power computation p-values. Default 0.05 (reject H0 p < 0.05). power_threshold_bayes_factor Numeric threshold power computation Bayes factors. Default 10 (reject H0 BF > 10) estimate_col Character string specifying column name containing parameter estimates. Default \"estimate\" true_effect_col Character string specifying column name conditions data frame containing true effect sizes. Default \"mean_effect\" ci_lower_col Character string specifying column name containing lower confidence interval bounds. Default \"ci_lower\" ci_upper_col Character string specifying column name containing upper confidence interval bounds. Default \"ci_upper\" p_value_col Character string specifying column name containing p-values. Default \"p_value\" bf_col Character string specifying column name containing Bayes factors. Default \"BF\" convergence_col Character string specifying column name containing convergence indicators. Default \"convergence\" method_replacements Named list replacement method specifications. element named \"method-method_setting\" combination (e.g., \"RMA-default\") contain named list : method: Character vector replacement method names method_setting: Character vector replacement method settings (length methods) power_test_type: Optional character vector power test types replacement method (length methods). specified, uses main power_test_type parameter multiple elements specified within vectors, replacements applied consecutively case previous replacements also failed converge. Defaults NULL, .e., omitting repetitions without converged results method--method basis. n_repetitions Number repetitions condition. Neccessary method replacement. Defaults 1000. overwrite Logical indicating whether overwrite existing results. FALSE (default), skip computation method-measure combinations already exist conditions Data frame conditions dgm_conditions() path Character string specifying directory path datasets/results/measures saved. Defaults location specified via PublicationBiasBenchmark.get_option(\"simulation_directory\"). objects stored dgm_name/datasets, dgm_name/results, dgm_name/measures subfolders.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_measures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Multiple Performance measures for a DGM — compute_measures","text":"Invisible list computed measures data frames","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_measures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Multiple Performance measures for a DGM — compute_measures","text":"","code":"if (FALSE) { # \\dontrun{ # Download DGM results dgm_name <- \"no_bias\" download_dgm_results(dgm_name)  # Basic usage compute_measures(   dgm_name        = dgm_name,   method          = c(\"mean\", \"RMA\", \"PET\"),   method_setting  = c(\"default\", \"default\", \"default\"),   measures        = c(\"bias\", \"mse\", \"coverage\") )  # With method replacements for non-converged results method_replacements <- list(   \"RMA-default\" = list(method = \"FMA\", method_setting = \"default\"),   \"PET-default\" = list(method = c(\"WLS\", \"FMA\"),                         method_setting = c(\"default\", \"default\")) )  compute_measures(   dgm_name            = dgm_name,   method              = c(\"RMA\", \"PET\"),   method_setting      = c(\"default\", \"default\"),   method_replacements = method_replacements,   measures            = c(\"bias\", \"mse\") ) } # }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_single_measure.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute Performance Measures — compute_single_measure","title":"Compute Performance Measures — compute_single_measure","text":"function provides modular extensible way compute performance measures (PM) Data-Generating Mechanisms (DGMs). handles different types measures automatically determines required arguments measure function.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_single_measure.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute Performance Measures — compute_single_measure","text":"","code":"compute_single_measure(   dgm_name,   measure_name,   method,   method_setting,   conditions,   measure_fun,   measure_mcse_fun,   power_test_type = \"p_value\",   estimate_col = \"estimate\",   true_effect_col = \"mean_effect\",   ci_lower_col = \"ci_lower\",   ci_upper_col = \"ci_upper\",   p_value_col = \"p_value\",   bf_col = \"BF\",   convergence_col = \"convergence\",   power_threshold_p_value = 0.05,   power_threshold_bayes_factor = 10,   method_replacements = NULL,   n_repetitions = 1000,   overwrite = FALSE,   path = NULL,   ... )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_single_measure.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute Performance Measures — compute_single_measure","text":"dgm_name Character string specifying DGM name measure_name Name measure compute (e.g., \"bias\", \"mse\") method Character vector method names method_setting Character vector method settings, must length method conditions Data frame conditions dgm_conditions() measure_fun Function compute measure measure_mcse_fun Function compute MCSE measure power_test_type Character vector specifying test type power computation: \"p_value\" (default) \"bayes_factor\" method. single value provided, repeated methods. estimate_col Character string specifying column name containing parameter estimates. Default \"estimate\" true_effect_col Character string specifying column name conditions data frame containing true effect sizes. Default \"mean_effect\" ci_lower_col Character string specifying column name containing lower confidence interval bounds. Default \"ci_lower\" ci_upper_col Character string specifying column name containing upper confidence interval bounds. Default \"ci_upper\" p_value_col Character string specifying column name containing p-values. Default \"p_value\" bf_col Character string specifying column name containing Bayes factors. Default \"BF\" convergence_col Character string specifying column name containing convergence indicators. Default \"convergence\" power_threshold_p_value Numeric threshold power computation p-values. Default 0.05 (reject H0 p < 0.05). power_threshold_bayes_factor Numeric threshold power computation Bayes factors. Default 10 (reject H0 BF > 10) method_replacements Named list replacement method specifications. element named \"method-method_setting\" combination (e.g., \"RMA-default\") contain named list : method: Character vector replacement method names method_setting: Character vector replacement method settings (length methods) power_test_type: Optional character vector power test types replacement method (length methods). specified, uses main power_test_type parameter multiple elements specified within vectors, replacements applied consecutively case previous replacements also failed converge. Defaults NULL, .e., omitting repetitions without converged results method--method basis. n_repetitions Number repetitions condition. Neccessary method replacement. Defaults 1000. overwrite Logical indicating whether overwrite existing results. FALSE (default), skip computation method-measure combinations already exist path Character string specifying directory path datasets/results/measures saved. Defaults location specified via PublicationBiasBenchmark.get_option(\"simulation_directory\"). objects stored dgm_name/datasets, dgm_name/results, dgm_name/measures subfolders. ... Additional arguments passed measure functions","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_single_measure.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute Performance Measures — compute_single_measure","text":"Data frame computed measures MCSEs","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/compute_single_measure.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute Performance Measures — compute_single_measure","text":"","code":"if (FALSE) { # \\dontrun{ # Get conditions for a DGM conditions <- dgm_conditions(\"no_bias\")   } # }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/create_empty_result.html","id":null,"dir":"Reference","previous_headings":"","what":"Create standardized empty method result for convergence failures — create_empty_result","title":"Create standardized empty method result for convergence failures — create_empty_result","text":"Create standardized empty method result convergence failures","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/create_empty_result.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create standardized empty method result for convergence failures — create_empty_result","text":"","code":"create_empty_result(method_name, note, extra_columns = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/create_empty_result.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create standardized empty method result for convergence failures — create_empty_result","text":"method_name Character string method name note Character string describing failure reason extra_columns Character vector additional empty columns add table","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/create_empty_result.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create standardized empty method result for convergence failures — create_empty_result","text":"Data frame standardized empty result structure","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Alinaghi2018.html","id":null,"dir":"Reference","previous_headings":"","what":"Alinaghi and Reed (2018) Data-Generating Mechansim — dgm.Alinaghi2018","title":"Alinaghi and Reed (2018) Data-Generating Mechansim — dgm.Alinaghi2018","text":"data-generating mechanism simulates univariate regression studies variable X affects continuous outcome Y. study estimates coefficient X, consists fixed component (α1) representing overall mean effect, random component varies across studies constant within study. \"Random Effects\" environment (\"RE\"), study produces one estimate, population effect differs across studies. description code based Hong Reed (2021) . data-generating mechanism introduced Alinaghi Reed (2018) .","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Alinaghi2018.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Alinaghi and Reed (2018) Data-Generating Mechansim — dgm.Alinaghi2018","text":"","code":"# S3 method for class 'Alinaghi2018' dgm(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Alinaghi2018.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Alinaghi and Reed (2018) Data-Generating Mechansim — dgm.Alinaghi2018","text":"dgm_name DGM name (automatically passed) settings List containing environment Type simulation environment. One \"FE\", \"RE\", \"PRE\". mean_effect Mean effect bias Type publication bias. One \"none\", \"positive\", \"significant\".","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Alinaghi2018.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Alinaghi and Reed (2018) Data-Generating Mechansim — dgm.Alinaghi2018","text":"Data frame yi effect size sei standard error ni sample size study_id study identifier es_type effect size type","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Alinaghi2018.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Alinaghi and Reed (2018) Data-Generating Mechansim — dgm.Alinaghi2018","text":"data-generating mechanism based Alinaghi & Reed (2018), study univariate regression models variable X affects continuous variable Y. parameter interest coefficient X. \"Random Effects\" environment, study produces one estimate, population effect differs across studies. coefficient X equals fixed component (α1) plus random component fixed within study varies across studies. overall mean effect X Y given α1. distinctive feature Alinaghi & Reed's experiments sample size estimated effects fixed publication selection, making meta-analyst's sample size endogenous affected effect size. Large population effects subject less publication selection, estimates satisfy selection criteria (statistical significance correct sign). Another feature separation statistical significance sign estimated effect criteria selection. Significant/correctly-signed estimates always \"published,\" insignificant/wrong-signed estimates 10% chance published. allows different sometimes conflicting consequences estimator performance. simulations designed representative meta-analyses economics business, typically several hundred estimates substantial effect heterogeneity. addition \"Random Effects\" environment, \"Panel Random Effects\" environment included, study 10 estimates, modeling common scenario multiple estimates per study. Effect estimates standard errors simulated similar within studies across studies, publication selection targets study rather individual estimates. inclusion meta-analyst's sample, study must least 7 10 estimates significant correctly signed.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Alinaghi2018.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Alinaghi and Reed (2018) Data-Generating Mechansim — dgm.Alinaghi2018","text":"Alinaghi N, Reed WR (2018). “Meta-analysis publication bias: well FAT-PET-PEESE procedure work?” Research Synthesis Methods, 9(2), 285-311. doi:10.1002/jrsm.1298 . Hong S, Reed WR (2021). “Using Monte Carlo experiments select meta-analytic estimators.” Research Synthesis Methods, 12(2), 192-215. doi:10.1002/jrsm.1467 .","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Bom2019.html","id":null,"dir":"Reference","previous_headings":"","what":"Bom and Rachinger (2019) Data-Generating Mechanism — dgm.Bom2019","title":"Bom and Rachinger (2019) Data-Generating Mechanism — dgm.Bom2019","text":"Simulates univariate regression environments estimate effect X1 Y (parameter α1). Effect heterogeneity introduced via omitted variable (X2) correlated X1, whose coefficient (α2) randomly distributed mean zero variance σ2_h. description code based Hong Reed (2021) . data-generating mechanism introduced Bom Rachinger (2019) .","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Bom2019.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bom and Rachinger (2019) Data-Generating Mechanism — dgm.Bom2019","text":"","code":"# S3 method for class 'Bom2019' dgm(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Bom2019.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bom and Rachinger (2019) Data-Generating Mechanism — dgm.Bom2019","text":"dgm_name DGM name (automatically passed) settings List containing mean_effect Mean effect effect_heterogeneity Mean effect heterogeneity bias Proportion studies affected publication bias n_studies Number effect size estimates sample_sizes Sample sizes effect size estimates. vector sample sizes needs supplied. sample sizes vector sequentially reused effect size estimates generated.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Bom2019.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bom and Rachinger (2019) Data-Generating Mechanism — dgm.Bom2019","text":"Data frame yi effect size sei standard error ni sample size es_type effect size type","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Bom2019.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bom and Rachinger (2019) Data-Generating Mechanism — dgm.Bom2019","text":"function simulates univariate regression environments, focusing estimating effect variable X1 dependent variable Y, represented parameter α1. simulation introduces variation standard errors estimated effects allowing sample sizes differ across primary studies. Effect heterogeneity modeled omitted variable (X2) correlated X1, coefficient omitted variable, α2, randomly distributed across studies mean zero variance σ2_h. Individual estimates α1 subject bias α2 nonzero due omitted variable. Across population studies, omitted variable bias averages . However, publication selection present—selection depends sign significance estimated effect α^1—bias induced meta-analyst's sample. Publication selection modeled two regimes: (1) selection, (2) 50% selection. 50% selection, estimate 50% chance evaluated inclusion. selected, positive statistically significant estimates published; otherwise, new estimates generated criterion met. process continues meta-analyst’s sample reaches predetermined size.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Bom2019.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bom and Rachinger (2019) Data-Generating Mechanism — dgm.Bom2019","text":"Bom PR, Rachinger H (2019). “kinked meta-regression model publication bias correction.” Research Synthesis Methods, 10(4), 497-514. doi:10.1002/jrsm.1352 . Hong S, Reed WR (2021). “Using Monte Carlo experiments select meta-analytic estimators.” Research Synthesis Methods, 12(2), 192-215. doi:10.1002/jrsm.1467 .","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Carter2019.html","id":null,"dir":"Reference","previous_headings":"","what":"Carter et al. (2019) Data-Generating Mechanism — dgm.Carter2019","title":"Carter et al. (2019) Data-Generating Mechanism — dgm.Carter2019","text":"data-generating mechanism simulates primary studies estimating treatment effects using Cohen's d. observed effect size modeled fixed mean plus random heterogeneity across studies, sample sizes varying generate differences standard errors. simulation introduces publication bias via selection algorithm probability publication depends nonlinearly sign p-value effect, regimes , medium, strong publication bias. also incorporates questionable research practices (QRPs) optional outlier removal, selection dependent variables, use moderators, optional stopping. description code based Hong Reed (2021) . data-generating mechanism introduced Carter et al. (2019) .","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Carter2019.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Carter et al. (2019) Data-Generating Mechanism — dgm.Carter2019","text":"","code":"# S3 method for class 'Carter2019' dgm(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Carter2019.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Carter et al. (2019) Data-Generating Mechanism — dgm.Carter2019","text":"dgm_name DGM name (automatically passed) settings List containing mean_effect Mean effect effect_heterogeneity Mean effect heterogeneity bias Degree publication bias one following levels: \"none\", \"medium\", \"high\". QRP Degree questionable research practices one following levels: \"none\", \"medium\", \"high\". n_studies Number effect size estimates","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Carter2019.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Carter et al. (2019) Data-Generating Mechanism — dgm.Carter2019","text":"Data frame yi effect size sei standard error ni sample size es_type effect size type","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Carter2019.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Carter et al. (2019) Data-Generating Mechanism — dgm.Carter2019","text":"simulation environment based framework described Carter, Schönbrodt, Gervais, Hilgard (2019). setup, primary studies estimate effect treatment using Cohen's d effect size metric. observed difference treatment control groups modeled sum fixed effect (α1) random component, introduces effect heterogeneity across studies. degree heterogeneity controlled parameter σ2_h. Variability standard errors d generated simulating primary studies different sample sizes. simulation incorporates two main types distortions research environment. First, publication selection algorithm used, probability study \"published\" depends nonlinearly sign estimated effect P-value. Three publication selection regimes modeled: \"Publication Bias,\" \"Medium Publication Bias,\" \"Strong Publication Bias,\" defined different parameters selection algorithm. Second, simulation includes four types questionable research practices (QRPs): () optional removal outliers, (b) optional selection two dependent variables, (c) optional use moderators, (d) optional stopping.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Carter2019.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Carter et al. (2019) Data-Generating Mechanism — dgm.Carter2019","text":"Carter EC, Schönbrodt FD, Gervais WM, Hilgard J (2019). “Correcting bias psychology: comparison meta-analytic methods.” Advances Methods Practices Psychological Science, 2(2), 115-144. doi:10.1177/2515245919847196 . Hong S, Reed WR (2021). “Using Monte Carlo experiments select meta-analytic estimators.” Research Synthesis Methods, 12(2), 192-215. doi:10.1002/jrsm.1467 .","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Stanley2017.html","id":null,"dir":"Reference","previous_headings":"","what":"Stanley, Doucouliagos, and Ioannidis (2017) Data-Generating Mechanism — dgm.Stanley2017","title":"Stanley, Doucouliagos, and Ioannidis (2017) Data-Generating Mechanism — dgm.Stanley2017","text":"Simulates two scenarios meta-analysis studies investigating effect treatment : (1) Log Odds Ratio scenario, outcome binary effect heterogeneity controlled random component, (2) Cohen's d scenario, outcome continuous effect heterogeneity introduced random component. scenarios allow varying sample sizes publication selection regimes, affecting inclusion study estimates based statistical significance sign. description code based Hong Reed (2021) . data-generating mechanism introduced Stanley et al. (2017) .","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Stanley2017.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Stanley, Doucouliagos, and Ioannidis (2017) Data-Generating Mechanism — dgm.Stanley2017","text":"","code":"# S3 method for class 'Stanley2017' dgm(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Stanley2017.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Stanley, Doucouliagos, and Ioannidis (2017) Data-Generating Mechanism — dgm.Stanley2017","text":"dgm_name DGM name (automatically passed) settings List containing environment Type simulation environment. One \"LogOR\" \"Cohens_d\". mean_effect Mean effect effect_heterogeneity Mean effect heterogeneity bias Proportion studies affected publication bias n_studies Number effect size estimates sample_sizes Sample sizes effect size estimates. vector sample sizes needs supplied. sample sizes vector sequentially reused effect size estimates generated.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Stanley2017.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Stanley, Doucouliagos, and Ioannidis (2017) Data-Generating Mechanism — dgm.Stanley2017","text":"Data frame yi effect size sei standard error ni sample size es_type effect size type","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Stanley2017.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Stanley, Doucouliagos, and Ioannidis (2017) Data-Generating Mechanism — dgm.Stanley2017","text":"function simulates two meta-analysis scenarios evaluate effect binary treatment variable (treat = {0, 1}) study outcomes, incorporating effect heterogeneity publication selection mechanisms. Log Odds Ratio (\"LogOR\") scenario, primary studies assess impact treatment binary success indicator (Y = 1). control group fixed 10% probability success, treatment group’s probability increased fixed effect mean-zero random component, whose variance (σ²_h) controls effect heterogeneity. study estimates logistic regression, coefficient treat (α₁) effect interest. Study sample sizes vary, resulting different standard errors estimated effects. Cohen's d (\"Cohens_d\") scenario, outcome variable continuous. treatment effect modeled fixed effect (α₁) plus random component (variance σ²_h). study computes Cohen's d, standardized mean difference treatment control groups. Study sample sizes vary, affecting standard errors d. Publication selection modeled two regimes: (1) selection, (2) 50% selection. 50% selection, estimate 50% chance evaluated inclusion. selected, positive statistically significant estimates published; otherwise, new estimates generated criterion met. process continues meta-analyst’s sample reaches predetermined size.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.Stanley2017.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Stanley, Doucouliagos, and Ioannidis (2017) Data-Generating Mechanism — dgm.Stanley2017","text":"Hong S, Reed WR (2021). “Using Monte Carlo experiments select meta-analytic estimators.” Research Synthesis Methods, 12(2), 192-215. doi:10.1002/jrsm.1467 . Stanley TD, Doucouliagos H, Ioannidis JP (2017). “Finding power reduce publication bias.” Statistics Medicine, 36(10), 1580-1598. doi:10.1002/sim.7228 .","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.default.html","id":null,"dir":"Reference","previous_headings":"","what":"Default DGM handler — dgm.default","title":"Default DGM handler — dgm.default","text":"Default DGM handler","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default DGM handler — dgm.default","text":"","code":"# Default S3 method dgm(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.default.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Default DGM handler — dgm.default","text":"dgm_name Character string specifying DGM type settings List containing required parameters DGM numeric condition_id","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.html","id":null,"dir":"Reference","previous_headings":"","what":"DGM Method — dgm","title":"DGM Method — dgm","text":"S3 Method defining data-generating mechanisms. See simulate_dgm() usage details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"DGM Method — dgm","text":"","code":"dgm(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"DGM Method — dgm","text":"dgm_name Character string specifying DGM type settings List containing required parameters DGM numeric condition_id","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.html","id":"output-structure","dir":"Reference","previous_headings":"","what":"Output Structure","title":"DGM Method — dgm","text":"returned data frame follows standardized schema downstream functions rely . Across currently implemented DGMs, following columns used: yi (numeric): effect size estimate. sei (numeric): Standard error yi. ni (integer): Total sample size estimate (e.g., sum groups applicable). es_type (character): Effect size type, used disambiguate scale yi. Currently used values \"SMD\" (standardized mean difference / Cohen's d), \"logOR\" (log odds ratio), \"none\" (unspecified generic continuous coefficient). study_id (integer/character, optional): Identifier primary study/cluster DGM yields multiple estimates per study (e.g., Alinaghi2018, PRE). absent, row treated independent study.","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"DGM Method — dgm","text":"","code":"simulate_dgm(\"Carter2019\", 1) #>              yi        sei  ni es_type #> 1  -0.266375111 0.35511784  32     SMD #> 2  -0.034256712 0.28869631  48     SMD #> 3   0.853607266 0.39480206  28     SMD #> 4   0.048247281 0.13547680 218     SMD #> 5   0.035897696 0.11010524 330     SMD #> 6  -0.022978317 0.42641550  22     SMD #> 7   0.088991270 0.09432756 450     SMD #> 8   0.266440019 0.44919345  20     SMD #> 9   0.214691131 0.15662302 164     SMD #> 10  0.001202358 0.44721364  20     SMD"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.no_bias.html","id":null,"dir":"Reference","previous_headings":"","what":"Normal Unbiased Data-Generating Mechanism — dgm.no_bias","title":"Normal Unbiased Data-Generating Mechanism — dgm.no_bias","text":"example data-generating mechanism simulate effect sizes without publication bias.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.no_bias.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Normal Unbiased Data-Generating Mechanism — dgm.no_bias","text":"","code":"# S3 method for class 'no_bias' dgm(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.no_bias.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Normal Unbiased Data-Generating Mechanism — dgm.no_bias","text":"dgm_name DGM name (automatically passed) settings List containing mean_effect Mean effect heterogeneity Effect heterogeneity n_studies Number effect size estimates","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.no_bias.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Normal Unbiased Data-Generating Mechanism — dgm.no_bias","text":"Data frame yi effect size sei standard error es_type effect size type","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.no_bias.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Normal Unbiased Data-Generating Mechanism — dgm.no_bias","text":"Sample sizes individual effect size estimates generated negative binomial distribution based empirical sample size distribution presented Appendix B Maier et al. (2023)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm.no_bias.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Normal Unbiased Data-Generating Mechanism — dgm.no_bias","text":"Maier M, Bartoš F, Wagenmakers E (2023). “Robust Bayesian meta-analysis: Addressing publication bias model-averaging.” Psychological Methods, 28(1), 107-122. doi:10.1037/met0000405 .","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm_conditions.html","id":null,"dir":"Reference","previous_headings":"","what":"Return Pre-specified DGM Settings — dgm_conditions","title":"Return Pre-specified DGM Settings — dgm_conditions","text":"function returns list pre-specified settings given Data Generating Mechanism (DGM).","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm_conditions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return Pre-specified DGM Settings — dgm_conditions","text":"","code":"dgm_conditions(dgm_name)  get_dgm_condition(dgm_name, condition_id)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm_conditions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return Pre-specified DGM Settings — dgm_conditions","text":"dgm_name Character string specifying DGM type condition_id conditions settings returned .","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm_conditions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return Pre-specified DGM Settings — dgm_conditions","text":"data frame containing pre-specified settings including condition_id column maps settings id corresponding settings.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/dgm_conditions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return Pre-specified DGM Settings — dgm_conditions","text":"","code":"dgm_conditions(\"Carter2019\") #>     n_studies mean_effect    QRP   bias effect_heterogeneity condition_id #> 1          10         0.0   none   none                  0.0            1 #> 2          30         0.0   none   none                  0.0            2 #> 3          60         0.0   none   none                  0.0            3 #> 4         100         0.0   none   none                  0.0            4 #> 5          10         0.2   none   none                  0.0            5 #> 6          30         0.2   none   none                  0.0            6 #> 7          60         0.2   none   none                  0.0            7 #> 8         100         0.2   none   none                  0.0            8 #> 9          10         0.5   none   none                  0.0            9 #> 10         30         0.5   none   none                  0.0           10 #> 11         60         0.5   none   none                  0.0           11 #> 12        100         0.5   none   none                  0.0           12 #> 13         10         0.8   none   none                  0.0           13 #> 14         30         0.8   none   none                  0.0           14 #> 15         60         0.8   none   none                  0.0           15 #> 16        100         0.8   none   none                  0.0           16 #> 17         10         0.0 medium   none                  0.0           17 #> 18         30         0.0 medium   none                  0.0           18 #> 19         60         0.0 medium   none                  0.0           19 #> 20        100         0.0 medium   none                  0.0           20 #> 21         10         0.2 medium   none                  0.0           21 #> 22         30         0.2 medium   none                  0.0           22 #> 23         60         0.2 medium   none                  0.0           23 #> 24        100         0.2 medium   none                  0.0           24 #> 25         10         0.5 medium   none                  0.0           25 #> 26         30         0.5 medium   none                  0.0           26 #> 27         60         0.5 medium   none                  0.0           27 #> 28        100         0.5 medium   none                  0.0           28 #> 29         10         0.8 medium   none                  0.0           29 #> 30         30         0.8 medium   none                  0.0           30 #> 31         60         0.8 medium   none                  0.0           31 #> 32        100         0.8 medium   none                  0.0           32 #> 33         10         0.0   high   none                  0.0           33 #> 34         30         0.0   high   none                  0.0           34 #> 35         60         0.0   high   none                  0.0           35 #> 36        100         0.0   high   none                  0.0           36 #> 37         10         0.2   high   none                  0.0           37 #> 38         30         0.2   high   none                  0.0           38 #> 39         60         0.2   high   none                  0.0           39 #> 40        100         0.2   high   none                  0.0           40 #> 41         10         0.5   high   none                  0.0           41 #> 42         30         0.5   high   none                  0.0           42 #> 43         60         0.5   high   none                  0.0           43 #> 44        100         0.5   high   none                  0.0           44 #> 45         10         0.8   high   none                  0.0           45 #> 46         30         0.8   high   none                  0.0           46 #> 47         60         0.8   high   none                  0.0           47 #> 48        100         0.8   high   none                  0.0           48 #> 49         10         0.0   none medium                  0.0           49 #> 50         30         0.0   none medium                  0.0           50 #> 51         60         0.0   none medium                  0.0           51 #> 52        100         0.0   none medium                  0.0           52 #> 53         10         0.2   none medium                  0.0           53 #> 54         30         0.2   none medium                  0.0           54 #> 55         60         0.2   none medium                  0.0           55 #> 56        100         0.2   none medium                  0.0           56 #> 57         10         0.5   none medium                  0.0           57 #> 58         30         0.5   none medium                  0.0           58 #> 59         60         0.5   none medium                  0.0           59 #> 60        100         0.5   none medium                  0.0           60 #> 61         10         0.8   none medium                  0.0           61 #> 62         30         0.8   none medium                  0.0           62 #> 63         60         0.8   none medium                  0.0           63 #> 64        100         0.8   none medium                  0.0           64 #> 65         10         0.0 medium medium                  0.0           65 #> 66         30         0.0 medium medium                  0.0           66 #> 67         60         0.0 medium medium                  0.0           67 #> 68        100         0.0 medium medium                  0.0           68 #> 69         10         0.2 medium medium                  0.0           69 #> 70         30         0.2 medium medium                  0.0           70 #> 71         60         0.2 medium medium                  0.0           71 #> 72        100         0.2 medium medium                  0.0           72 #> 73         10         0.5 medium medium                  0.0           73 #> 74         30         0.5 medium medium                  0.0           74 #> 75         60         0.5 medium medium                  0.0           75 #> 76        100         0.5 medium medium                  0.0           76 #> 77         10         0.8 medium medium                  0.0           77 #> 78         30         0.8 medium medium                  0.0           78 #> 79         60         0.8 medium medium                  0.0           79 #> 80        100         0.8 medium medium                  0.0           80 #> 81         10         0.0   high medium                  0.0           81 #> 82         30         0.0   high medium                  0.0           82 #> 83         60         0.0   high medium                  0.0           83 #> 84        100         0.0   high medium                  0.0           84 #> 85         10         0.2   high medium                  0.0           85 #> 86         30         0.2   high medium                  0.0           86 #> 87         60         0.2   high medium                  0.0           87 #> 88        100         0.2   high medium                  0.0           88 #> 89         10         0.5   high medium                  0.0           89 #> 90         30         0.5   high medium                  0.0           90 #> 91         60         0.5   high medium                  0.0           91 #> 92        100         0.5   high medium                  0.0           92 #> 93         10         0.8   high medium                  0.0           93 #> 94         30         0.8   high medium                  0.0           94 #> 95         60         0.8   high medium                  0.0           95 #> 96        100         0.8   high medium                  0.0           96 #> 97         10         0.0   none   high                  0.0           97 #> 98         30         0.0   none   high                  0.0           98 #> 99         60         0.0   none   high                  0.0           99 #> 100       100         0.0   none   high                  0.0          100 #> 101        10         0.2   none   high                  0.0          101 #> 102        30         0.2   none   high                  0.0          102 #> 103        60         0.2   none   high                  0.0          103 #> 104       100         0.2   none   high                  0.0          104 #> 105        10         0.5   none   high                  0.0          105 #> 106        30         0.5   none   high                  0.0          106 #> 107        60         0.5   none   high                  0.0          107 #> 108       100         0.5   none   high                  0.0          108 #> 109        10         0.8   none   high                  0.0          109 #> 110        30         0.8   none   high                  0.0          110 #> 111        60         0.8   none   high                  0.0          111 #> 112       100         0.8   none   high                  0.0          112 #> 113        10         0.0 medium   high                  0.0          113 #> 114        30         0.0 medium   high                  0.0          114 #> 115        60         0.0 medium   high                  0.0          115 #> 116       100         0.0 medium   high                  0.0          116 #> 117        10         0.2 medium   high                  0.0          117 #> 118        30         0.2 medium   high                  0.0          118 #> 119        60         0.2 medium   high                  0.0          119 #> 120       100         0.2 medium   high                  0.0          120 #> 121        10         0.5 medium   high                  0.0          121 #> 122        30         0.5 medium   high                  0.0          122 #> 123        60         0.5 medium   high                  0.0          123 #> 124       100         0.5 medium   high                  0.0          124 #> 125        10         0.8 medium   high                  0.0          125 #> 126        30         0.8 medium   high                  0.0          126 #> 127        60         0.8 medium   high                  0.0          127 #> 128       100         0.8 medium   high                  0.0          128 #> 129        10         0.0   high   high                  0.0          129 #> 130        30         0.0   high   high                  0.0          130 #> 131        60         0.0   high   high                  0.0          131 #> 132       100         0.0   high   high                  0.0          132 #> 133        10         0.2   high   high                  0.0          133 #> 134        30         0.2   high   high                  0.0          134 #> 135        60         0.2   high   high                  0.0          135 #> 136       100         0.2   high   high                  0.0          136 #> 137        10         0.5   high   high                  0.0          137 #> 138        30         0.5   high   high                  0.0          138 #> 139        60         0.5   high   high                  0.0          139 #> 140       100         0.5   high   high                  0.0          140 #> 141        10         0.8   high   high                  0.0          141 #> 142        30         0.8   high   high                  0.0          142 #> 143        60         0.8   high   high                  0.0          143 #> 144       100         0.8   high   high                  0.0          144 #> 145        10         0.0   none   none                  0.2          145 #> 146        30         0.0   none   none                  0.2          146 #> 147        60         0.0   none   none                  0.2          147 #> 148       100         0.0   none   none                  0.2          148 #> 149        10         0.2   none   none                  0.2          149 #> 150        30         0.2   none   none                  0.2          150 #> 151        60         0.2   none   none                  0.2          151 #> 152       100         0.2   none   none                  0.2          152 #> 153        10         0.5   none   none                  0.2          153 #> 154        30         0.5   none   none                  0.2          154 #> 155        60         0.5   none   none                  0.2          155 #> 156       100         0.5   none   none                  0.2          156 #> 157        10         0.8   none   none                  0.2          157 #> 158        30         0.8   none   none                  0.2          158 #> 159        60         0.8   none   none                  0.2          159 #> 160       100         0.8   none   none                  0.2          160 #> 161        10         0.0 medium   none                  0.2          161 #> 162        30         0.0 medium   none                  0.2          162 #> 163        60         0.0 medium   none                  0.2          163 #> 164       100         0.0 medium   none                  0.2          164 #> 165        10         0.2 medium   none                  0.2          165 #> 166        30         0.2 medium   none                  0.2          166 #> 167        60         0.2 medium   none                  0.2          167 #> 168       100         0.2 medium   none                  0.2          168 #> 169        10         0.5 medium   none                  0.2          169 #> 170        30         0.5 medium   none                  0.2          170 #> 171        60         0.5 medium   none                  0.2          171 #> 172       100         0.5 medium   none                  0.2          172 #> 173        10         0.8 medium   none                  0.2          173 #> 174        30         0.8 medium   none                  0.2          174 #> 175        60         0.8 medium   none                  0.2          175 #> 176       100         0.8 medium   none                  0.2          176 #> 177        10         0.0   high   none                  0.2          177 #> 178        30         0.0   high   none                  0.2          178 #> 179        60         0.0   high   none                  0.2          179 #> 180       100         0.0   high   none                  0.2          180 #> 181        10         0.2   high   none                  0.2          181 #> 182        30         0.2   high   none                  0.2          182 #> 183        60         0.2   high   none                  0.2          183 #> 184       100         0.2   high   none                  0.2          184 #> 185        10         0.5   high   none                  0.2          185 #> 186        30         0.5   high   none                  0.2          186 #> 187        60         0.5   high   none                  0.2          187 #> 188       100         0.5   high   none                  0.2          188 #> 189        10         0.8   high   none                  0.2          189 #> 190        30         0.8   high   none                  0.2          190 #> 191        60         0.8   high   none                  0.2          191 #> 192       100         0.8   high   none                  0.2          192 #> 193        10         0.0   none medium                  0.2          193 #> 194        30         0.0   none medium                  0.2          194 #> 195        60         0.0   none medium                  0.2          195 #> 196       100         0.0   none medium                  0.2          196 #> 197        10         0.2   none medium                  0.2          197 #> 198        30         0.2   none medium                  0.2          198 #> 199        60         0.2   none medium                  0.2          199 #> 200       100         0.2   none medium                  0.2          200 #> 201        10         0.5   none medium                  0.2          201 #> 202        30         0.5   none medium                  0.2          202 #> 203        60         0.5   none medium                  0.2          203 #> 204       100         0.5   none medium                  0.2          204 #> 205        10         0.8   none medium                  0.2          205 #> 206        30         0.8   none medium                  0.2          206 #> 207        60         0.8   none medium                  0.2          207 #> 208       100         0.8   none medium                  0.2          208 #> 209        10         0.0 medium medium                  0.2          209 #> 210        30         0.0 medium medium                  0.2          210 #> 211        60         0.0 medium medium                  0.2          211 #> 212       100         0.0 medium medium                  0.2          212 #> 213        10         0.2 medium medium                  0.2          213 #> 214        30         0.2 medium medium                  0.2          214 #> 215        60         0.2 medium medium                  0.2          215 #> 216       100         0.2 medium medium                  0.2          216 #> 217        10         0.5 medium medium                  0.2          217 #> 218        30         0.5 medium medium                  0.2          218 #> 219        60         0.5 medium medium                  0.2          219 #> 220       100         0.5 medium medium                  0.2          220 #> 221        10         0.8 medium medium                  0.2          221 #> 222        30         0.8 medium medium                  0.2          222 #> 223        60         0.8 medium medium                  0.2          223 #> 224       100         0.8 medium medium                  0.2          224 #> 225        10         0.0   high medium                  0.2          225 #> 226        30         0.0   high medium                  0.2          226 #> 227        60         0.0   high medium                  0.2          227 #> 228       100         0.0   high medium                  0.2          228 #> 229        10         0.2   high medium                  0.2          229 #> 230        30         0.2   high medium                  0.2          230 #> 231        60         0.2   high medium                  0.2          231 #> 232       100         0.2   high medium                  0.2          232 #> 233        10         0.5   high medium                  0.2          233 #> 234        30         0.5   high medium                  0.2          234 #> 235        60         0.5   high medium                  0.2          235 #> 236       100         0.5   high medium                  0.2          236 #> 237        10         0.8   high medium                  0.2          237 #> 238        30         0.8   high medium                  0.2          238 #> 239        60         0.8   high medium                  0.2          239 #> 240       100         0.8   high medium                  0.2          240 #> 241        10         0.0   none   high                  0.2          241 #> 242        30         0.0   none   high                  0.2          242 #> 243        60         0.0   none   high                  0.2          243 #> 244       100         0.0   none   high                  0.2          244 #> 245        10         0.2   none   high                  0.2          245 #> 246        30         0.2   none   high                  0.2          246 #> 247        60         0.2   none   high                  0.2          247 #> 248       100         0.2   none   high                  0.2          248 #> 249        10         0.5   none   high                  0.2          249 #> 250        30         0.5   none   high                  0.2          250 #> 251        60         0.5   none   high                  0.2          251 #> 252       100         0.5   none   high                  0.2          252 #> 253        10         0.8   none   high                  0.2          253 #> 254        30         0.8   none   high                  0.2          254 #> 255        60         0.8   none   high                  0.2          255 #> 256       100         0.8   none   high                  0.2          256 #> 257        10         0.0 medium   high                  0.2          257 #> 258        30         0.0 medium   high                  0.2          258 #> 259        60         0.0 medium   high                  0.2          259 #> 260       100         0.0 medium   high                  0.2          260 #> 261        10         0.2 medium   high                  0.2          261 #> 262        30         0.2 medium   high                  0.2          262 #> 263        60         0.2 medium   high                  0.2          263 #> 264       100         0.2 medium   high                  0.2          264 #> 265        10         0.5 medium   high                  0.2          265 #> 266        30         0.5 medium   high                  0.2          266 #> 267        60         0.5 medium   high                  0.2          267 #> 268       100         0.5 medium   high                  0.2          268 #> 269        10         0.8 medium   high                  0.2          269 #> 270        30         0.8 medium   high                  0.2          270 #> 271        60         0.8 medium   high                  0.2          271 #> 272       100         0.8 medium   high                  0.2          272 #> 273        10         0.0   high   high                  0.2          273 #> 274        30         0.0   high   high                  0.2          274 #> 275        60         0.0   high   high                  0.2          275 #> 276       100         0.0   high   high                  0.2          276 #> 277        10         0.2   high   high                  0.2          277 #> 278        30         0.2   high   high                  0.2          278 #> 279        60         0.2   high   high                  0.2          279 #> 280       100         0.2   high   high                  0.2          280 #> 281        10         0.5   high   high                  0.2          281 #> 282        30         0.5   high   high                  0.2          282 #> 283        60         0.5   high   high                  0.2          283 #> 284       100         0.5   high   high                  0.2          284 #> 285        10         0.8   high   high                  0.2          285 #> 286        30         0.8   high   high                  0.2          286 #> 287        60         0.8   high   high                  0.2          287 #> 288       100         0.8   high   high                  0.2          288 #> 289        10         0.0   none   none                  0.4          289 #> 290        30         0.0   none   none                  0.4          290 #> 291        60         0.0   none   none                  0.4          291 #> 292       100         0.0   none   none                  0.4          292 #> 293        10         0.2   none   none                  0.4          293 #> 294        30         0.2   none   none                  0.4          294 #> 295        60         0.2   none   none                  0.4          295 #> 296       100         0.2   none   none                  0.4          296 #> 297        10         0.5   none   none                  0.4          297 #> 298        30         0.5   none   none                  0.4          298 #> 299        60         0.5   none   none                  0.4          299 #> 300       100         0.5   none   none                  0.4          300 #> 301        10         0.8   none   none                  0.4          301 #> 302        30         0.8   none   none                  0.4          302 #> 303        60         0.8   none   none                  0.4          303 #> 304       100         0.8   none   none                  0.4          304 #> 305        10         0.0 medium   none                  0.4          305 #> 306        30         0.0 medium   none                  0.4          306 #> 307        60         0.0 medium   none                  0.4          307 #> 308       100         0.0 medium   none                  0.4          308 #> 309        10         0.2 medium   none                  0.4          309 #> 310        30         0.2 medium   none                  0.4          310 #> 311        60         0.2 medium   none                  0.4          311 #> 312       100         0.2 medium   none                  0.4          312 #> 313        10         0.5 medium   none                  0.4          313 #> 314        30         0.5 medium   none                  0.4          314 #> 315        60         0.5 medium   none                  0.4          315 #> 316       100         0.5 medium   none                  0.4          316 #> 317        10         0.8 medium   none                  0.4          317 #> 318        30         0.8 medium   none                  0.4          318 #> 319        60         0.8 medium   none                  0.4          319 #> 320       100         0.8 medium   none                  0.4          320 #> 321        10         0.0   high   none                  0.4          321 #> 322        30         0.0   high   none                  0.4          322 #> 323        60         0.0   high   none                  0.4          323 #> 324       100         0.0   high   none                  0.4          324 #> 325        10         0.2   high   none                  0.4          325 #> 326        30         0.2   high   none                  0.4          326 #> 327        60         0.2   high   none                  0.4          327 #> 328       100         0.2   high   none                  0.4          328 #> 329        10         0.5   high   none                  0.4          329 #> 330        30         0.5   high   none                  0.4          330 #> 331        60         0.5   high   none                  0.4          331 #> 332       100         0.5   high   none                  0.4          332 #> 333        10         0.8   high   none                  0.4          333 #> 334        30         0.8   high   none                  0.4          334 #> 335        60         0.8   high   none                  0.4          335 #> 336       100         0.8   high   none                  0.4          336 #> 337        10         0.0   none medium                  0.4          337 #> 338        30         0.0   none medium                  0.4          338 #> 339        60         0.0   none medium                  0.4          339 #> 340       100         0.0   none medium                  0.4          340 #> 341        10         0.2   none medium                  0.4          341 #> 342        30         0.2   none medium                  0.4          342 #> 343        60         0.2   none medium                  0.4          343 #> 344       100         0.2   none medium                  0.4          344 #> 345        10         0.5   none medium                  0.4          345 #> 346        30         0.5   none medium                  0.4          346 #> 347        60         0.5   none medium                  0.4          347 #> 348       100         0.5   none medium                  0.4          348 #> 349        10         0.8   none medium                  0.4          349 #> 350        30         0.8   none medium                  0.4          350 #> 351        60         0.8   none medium                  0.4          351 #> 352       100         0.8   none medium                  0.4          352 #> 353        10         0.0 medium medium                  0.4          353 #> 354        30         0.0 medium medium                  0.4          354 #> 355        60         0.0 medium medium                  0.4          355 #> 356       100         0.0 medium medium                  0.4          356 #> 357        10         0.2 medium medium                  0.4          357 #> 358        30         0.2 medium medium                  0.4          358 #> 359        60         0.2 medium medium                  0.4          359 #> 360       100         0.2 medium medium                  0.4          360 #> 361        10         0.5 medium medium                  0.4          361 #> 362        30         0.5 medium medium                  0.4          362 #> 363        60         0.5 medium medium                  0.4          363 #> 364       100         0.5 medium medium                  0.4          364 #> 365        10         0.8 medium medium                  0.4          365 #> 366        30         0.8 medium medium                  0.4          366 #> 367        60         0.8 medium medium                  0.4          367 #> 368       100         0.8 medium medium                  0.4          368 #> 369        10         0.0   high medium                  0.4          369 #> 370        30         0.0   high medium                  0.4          370 #> 371        60         0.0   high medium                  0.4          371 #> 372       100         0.0   high medium                  0.4          372 #> 373        10         0.2   high medium                  0.4          373 #> 374        30         0.2   high medium                  0.4          374 #> 375        60         0.2   high medium                  0.4          375 #> 376       100         0.2   high medium                  0.4          376 #> 377        10         0.5   high medium                  0.4          377 #> 378        30         0.5   high medium                  0.4          378 #> 379        60         0.5   high medium                  0.4          379 #> 380       100         0.5   high medium                  0.4          380 #> 381        10         0.8   high medium                  0.4          381 #> 382        30         0.8   high medium                  0.4          382 #> 383        60         0.8   high medium                  0.4          383 #> 384       100         0.8   high medium                  0.4          384 #> 385        10         0.0   none   high                  0.4          385 #> 386        30         0.0   none   high                  0.4          386 #> 387        60         0.0   none   high                  0.4          387 #> 388       100         0.0   none   high                  0.4          388 #> 389        10         0.2   none   high                  0.4          389 #> 390        30         0.2   none   high                  0.4          390 #> 391        60         0.2   none   high                  0.4          391 #> 392       100         0.2   none   high                  0.4          392 #> 393        10         0.5   none   high                  0.4          393 #> 394        30         0.5   none   high                  0.4          394 #> 395        60         0.5   none   high                  0.4          395 #> 396       100         0.5   none   high                  0.4          396 #> 397        10         0.8   none   high                  0.4          397 #> 398        30         0.8   none   high                  0.4          398 #> 399        60         0.8   none   high                  0.4          399 #> 400       100         0.8   none   high                  0.4          400 #> 401        10         0.0 medium   high                  0.4          401 #> 402        30         0.0 medium   high                  0.4          402 #> 403        60         0.0 medium   high                  0.4          403 #> 404       100         0.0 medium   high                  0.4          404 #> 405        10         0.2 medium   high                  0.4          405 #> 406        30         0.2 medium   high                  0.4          406 #> 407        60         0.2 medium   high                  0.4          407 #> 408       100         0.2 medium   high                  0.4          408 #> 409        10         0.5 medium   high                  0.4          409 #> 410        30         0.5 medium   high                  0.4          410 #> 411        60         0.5 medium   high                  0.4          411 #> 412       100         0.5 medium   high                  0.4          412 #> 413        10         0.8 medium   high                  0.4          413 #> 414        30         0.8 medium   high                  0.4          414 #> 415        60         0.8 medium   high                  0.4          415 #> 416       100         0.8 medium   high                  0.4          416 #> 417        10         0.0   high   high                  0.4          417 #> 418        30         0.0   high   high                  0.4          418 #> 419        60         0.0   high   high                  0.4          419 #> 420       100         0.0   high   high                  0.4          420 #> 421        10         0.2   high   high                  0.4          421 #> 422        30         0.2   high   high                  0.4          422 #> 423        60         0.2   high   high                  0.4          423 #> 424       100         0.2   high   high                  0.4          424 #> 425        10         0.5   high   high                  0.4          425 #> 426        30         0.5   high   high                  0.4          426 #> 427        60         0.5   high   high                  0.4          427 #> 428       100         0.5   high   high                  0.4          428 #> 429        10         0.8   high   high                  0.4          429 #> 430        30         0.8   high   high                  0.4          430 #> 431        60         0.8   high   high                  0.4          431 #> 432       100         0.8   high   high                  0.4          432 #> 433       200         0.0   none   none                  0.0          433 #> 434       400         0.0   none   none                  0.0          434 #> 435       800         0.0   none   none                  0.0          435 #> 436       200         0.2   none   none                  0.0          436 #> 437       400         0.2   none   none                  0.0          437 #> 438       800         0.2   none   none                  0.0          438 #> 439       200         0.5   none   none                  0.0          439 #> 440       400         0.5   none   none                  0.0          440 #> 441       800         0.5   none   none                  0.0          441 #> 442       200         0.8   none   none                  0.0          442 #> 443       400         0.8   none   none                  0.0          443 #> 444       800         0.8   none   none                  0.0          444 #> 445       200         0.0 medium   none                  0.0          445 #> 446       400         0.0 medium   none                  0.0          446 #> 447       800         0.0 medium   none                  0.0          447 #> 448       200         0.2 medium   none                  0.0          448 #> 449       400         0.2 medium   none                  0.0          449 #> 450       800         0.2 medium   none                  0.0          450 #> 451       200         0.5 medium   none                  0.0          451 #> 452       400         0.5 medium   none                  0.0          452 #> 453       800         0.5 medium   none                  0.0          453 #> 454       200         0.8 medium   none                  0.0          454 #> 455       400         0.8 medium   none                  0.0          455 #> 456       800         0.8 medium   none                  0.0          456 #> 457       200         0.0   high   none                  0.0          457 #> 458       400         0.0   high   none                  0.0          458 #> 459       800         0.0   high   none                  0.0          459 #> 460       200         0.2   high   none                  0.0          460 #> 461       400         0.2   high   none                  0.0          461 #> 462       800         0.2   high   none                  0.0          462 #> 463       200         0.5   high   none                  0.0          463 #> 464       400         0.5   high   none                  0.0          464 #> 465       800         0.5   high   none                  0.0          465 #> 466       200         0.8   high   none                  0.0          466 #> 467       400         0.8   high   none                  0.0          467 #> 468       800         0.8   high   none                  0.0          468 #> 469       200         0.0   none medium                  0.0          469 #> 470       400         0.0   none medium                  0.0          470 #> 471       800         0.0   none medium                  0.0          471 #> 472       200         0.2   none medium                  0.0          472 #> 473       400         0.2   none medium                  0.0          473 #> 474       800         0.2   none medium                  0.0          474 #> 475       200         0.5   none medium                  0.0          475 #> 476       400         0.5   none medium                  0.0          476 #> 477       800         0.5   none medium                  0.0          477 #> 478       200         0.8   none medium                  0.0          478 #> 479       400         0.8   none medium                  0.0          479 #> 480       800         0.8   none medium                  0.0          480 #> 481       200         0.0 medium medium                  0.0          481 #> 482       400         0.0 medium medium                  0.0          482 #> 483       800         0.0 medium medium                  0.0          483 #> 484       200         0.2 medium medium                  0.0          484 #> 485       400         0.2 medium medium                  0.0          485 #> 486       800         0.2 medium medium                  0.0          486 #> 487       200         0.5 medium medium                  0.0          487 #> 488       400         0.5 medium medium                  0.0          488 #> 489       800         0.5 medium medium                  0.0          489 #> 490       200         0.8 medium medium                  0.0          490 #> 491       400         0.8 medium medium                  0.0          491 #> 492       800         0.8 medium medium                  0.0          492 #> 493       200         0.0   high medium                  0.0          493 #> 494       400         0.0   high medium                  0.0          494 #> 495       800         0.0   high medium                  0.0          495 #> 496       200         0.2   high medium                  0.0          496 #> 497       400         0.2   high medium                  0.0          497 #> 498       800         0.2   high medium                  0.0          498 #> 499       200         0.5   high medium                  0.0          499 #> 500       400         0.5   high medium                  0.0          500 #> 501       800         0.5   high medium                  0.0          501 #> 502       200         0.8   high medium                  0.0          502 #> 503       400         0.8   high medium                  0.0          503 #> 504       800         0.8   high medium                  0.0          504 #> 505       200         0.0   none   high                  0.0          505 #> 506       400         0.0   none   high                  0.0          506 #> 507       800         0.0   none   high                  0.0          507 #> 508       200         0.2   none   high                  0.0          508 #> 509       400         0.2   none   high                  0.0          509 #> 510       800         0.2   none   high                  0.0          510 #> 511       200         0.5   none   high                  0.0          511 #> 512       400         0.5   none   high                  0.0          512 #> 513       800         0.5   none   high                  0.0          513 #> 514       200         0.8   none   high                  0.0          514 #> 515       400         0.8   none   high                  0.0          515 #> 516       800         0.8   none   high                  0.0          516 #> 517       200         0.0 medium   high                  0.0          517 #> 518       400         0.0 medium   high                  0.0          518 #> 519       800         0.0 medium   high                  0.0          519 #> 520       200         0.2 medium   high                  0.0          520 #> 521       400         0.2 medium   high                  0.0          521 #> 522       800         0.2 medium   high                  0.0          522 #> 523       200         0.5 medium   high                  0.0          523 #> 524       400         0.5 medium   high                  0.0          524 #> 525       800         0.5 medium   high                  0.0          525 #> 526       200         0.8 medium   high                  0.0          526 #> 527       400         0.8 medium   high                  0.0          527 #> 528       800         0.8 medium   high                  0.0          528 #> 529       200         0.0   high   high                  0.0          529 #> 530       400         0.0   high   high                  0.0          530 #> 531       800         0.0   high   high                  0.0          531 #> 532       200         0.2   high   high                  0.0          532 #> 533       400         0.2   high   high                  0.0          533 #> 534       800         0.2   high   high                  0.0          534 #> 535       200         0.5   high   high                  0.0          535 #> 536       400         0.5   high   high                  0.0          536 #> 537       800         0.5   high   high                  0.0          537 #> 538       200         0.8   high   high                  0.0          538 #> 539       400         0.8   high   high                  0.0          539 #> 540       800         0.8   high   high                  0.0          540 #> 541       200         0.0   none   none                  0.2          541 #> 542       400         0.0   none   none                  0.2          542 #> 543       800         0.0   none   none                  0.2          543 #> 544       200         0.2   none   none                  0.2          544 #> 545       400         0.2   none   none                  0.2          545 #> 546       800         0.2   none   none                  0.2          546 #> 547       200         0.5   none   none                  0.2          547 #> 548       400         0.5   none   none                  0.2          548 #> 549       800         0.5   none   none                  0.2          549 #> 550       200         0.8   none   none                  0.2          550 #> 551       400         0.8   none   none                  0.2          551 #> 552       800         0.8   none   none                  0.2          552 #> 553       200         0.0 medium   none                  0.2          553 #> 554       400         0.0 medium   none                  0.2          554 #> 555       800         0.0 medium   none                  0.2          555 #> 556       200         0.2 medium   none                  0.2          556 #> 557       400         0.2 medium   none                  0.2          557 #> 558       800         0.2 medium   none                  0.2          558 #> 559       200         0.5 medium   none                  0.2          559 #> 560       400         0.5 medium   none                  0.2          560 #> 561       800         0.5 medium   none                  0.2          561 #> 562       200         0.8 medium   none                  0.2          562 #> 563       400         0.8 medium   none                  0.2          563 #> 564       800         0.8 medium   none                  0.2          564 #> 565       200         0.0   high   none                  0.2          565 #> 566       400         0.0   high   none                  0.2          566 #> 567       800         0.0   high   none                  0.2          567 #> 568       200         0.2   high   none                  0.2          568 #> 569       400         0.2   high   none                  0.2          569 #> 570       800         0.2   high   none                  0.2          570 #> 571       200         0.5   high   none                  0.2          571 #> 572       400         0.5   high   none                  0.2          572 #> 573       800         0.5   high   none                  0.2          573 #> 574       200         0.8   high   none                  0.2          574 #> 575       400         0.8   high   none                  0.2          575 #> 576       800         0.8   high   none                  0.2          576 #> 577       200         0.0   none medium                  0.2          577 #> 578       400         0.0   none medium                  0.2          578 #> 579       800         0.0   none medium                  0.2          579 #> 580       200         0.2   none medium                  0.2          580 #> 581       400         0.2   none medium                  0.2          581 #> 582       800         0.2   none medium                  0.2          582 #> 583       200         0.5   none medium                  0.2          583 #> 584       400         0.5   none medium                  0.2          584 #> 585       800         0.5   none medium                  0.2          585 #> 586       200         0.8   none medium                  0.2          586 #> 587       400         0.8   none medium                  0.2          587 #> 588       800         0.8   none medium                  0.2          588 #> 589       200         0.0 medium medium                  0.2          589 #> 590       400         0.0 medium medium                  0.2          590 #> 591       800         0.0 medium medium                  0.2          591 #> 592       200         0.2 medium medium                  0.2          592 #> 593       400         0.2 medium medium                  0.2          593 #> 594       800         0.2 medium medium                  0.2          594 #> 595       200         0.5 medium medium                  0.2          595 #> 596       400         0.5 medium medium                  0.2          596 #> 597       800         0.5 medium medium                  0.2          597 #> 598       200         0.8 medium medium                  0.2          598 #> 599       400         0.8 medium medium                  0.2          599 #> 600       800         0.8 medium medium                  0.2          600 #> 601       200         0.0   high medium                  0.2          601 #> 602       400         0.0   high medium                  0.2          602 #> 603       800         0.0   high medium                  0.2          603 #> 604       200         0.2   high medium                  0.2          604 #> 605       400         0.2   high medium                  0.2          605 #> 606       800         0.2   high medium                  0.2          606 #> 607       200         0.5   high medium                  0.2          607 #> 608       400         0.5   high medium                  0.2          608 #> 609       800         0.5   high medium                  0.2          609 #> 610       200         0.8   high medium                  0.2          610 #> 611       400         0.8   high medium                  0.2          611 #> 612       800         0.8   high medium                  0.2          612 #> 613       200         0.0   none   high                  0.2          613 #> 614       400         0.0   none   high                  0.2          614 #> 615       800         0.0   none   high                  0.2          615 #> 616       200         0.2   none   high                  0.2          616 #> 617       400         0.2   none   high                  0.2          617 #> 618       800         0.2   none   high                  0.2          618 #> 619       200         0.5   none   high                  0.2          619 #> 620       400         0.5   none   high                  0.2          620 #> 621       800         0.5   none   high                  0.2          621 #> 622       200         0.8   none   high                  0.2          622 #> 623       400         0.8   none   high                  0.2          623 #> 624       800         0.8   none   high                  0.2          624 #> 625       200         0.0 medium   high                  0.2          625 #> 626       400         0.0 medium   high                  0.2          626 #> 627       800         0.0 medium   high                  0.2          627 #> 628       200         0.2 medium   high                  0.2          628 #> 629       400         0.2 medium   high                  0.2          629 #> 630       800         0.2 medium   high                  0.2          630 #> 631       200         0.5 medium   high                  0.2          631 #> 632       400         0.5 medium   high                  0.2          632 #> 633       800         0.5 medium   high                  0.2          633 #> 634       200         0.8 medium   high                  0.2          634 #> 635       400         0.8 medium   high                  0.2          635 #> 636       800         0.8 medium   high                  0.2          636 #> 637       200         0.0   high   high                  0.2          637 #> 638       400         0.0   high   high                  0.2          638 #> 639       800         0.0   high   high                  0.2          639 #> 640       200         0.2   high   high                  0.2          640 #> 641       400         0.2   high   high                  0.2          641 #> 642       800         0.2   high   high                  0.2          642 #> 643       200         0.5   high   high                  0.2          643 #> 644       400         0.5   high   high                  0.2          644 #> 645       800         0.5   high   high                  0.2          645 #> 646       200         0.8   high   high                  0.2          646 #> 647       400         0.8   high   high                  0.2          647 #> 648       800         0.8   high   high                  0.2          648 #> 649       200         0.0   none   none                  0.4          649 #> 650       400         0.0   none   none                  0.4          650 #> 651       800         0.0   none   none                  0.4          651 #> 652       200         0.2   none   none                  0.4          652 #> 653       400         0.2   none   none                  0.4          653 #> 654       800         0.2   none   none                  0.4          654 #> 655       200         0.5   none   none                  0.4          655 #> 656       400         0.5   none   none                  0.4          656 #> 657       800         0.5   none   none                  0.4          657 #> 658       200         0.8   none   none                  0.4          658 #> 659       400         0.8   none   none                  0.4          659 #> 660       800         0.8   none   none                  0.4          660 #> 661       200         0.0 medium   none                  0.4          661 #> 662       400         0.0 medium   none                  0.4          662 #> 663       800         0.0 medium   none                  0.4          663 #> 664       200         0.2 medium   none                  0.4          664 #> 665       400         0.2 medium   none                  0.4          665 #> 666       800         0.2 medium   none                  0.4          666 #> 667       200         0.5 medium   none                  0.4          667 #> 668       400         0.5 medium   none                  0.4          668 #> 669       800         0.5 medium   none                  0.4          669 #> 670       200         0.8 medium   none                  0.4          670 #> 671       400         0.8 medium   none                  0.4          671 #> 672       800         0.8 medium   none                  0.4          672 #> 673       200         0.0   high   none                  0.4          673 #> 674       400         0.0   high   none                  0.4          674 #> 675       800         0.0   high   none                  0.4          675 #> 676       200         0.2   high   none                  0.4          676 #> 677       400         0.2   high   none                  0.4          677 #> 678       800         0.2   high   none                  0.4          678 #> 679       200         0.5   high   none                  0.4          679 #> 680       400         0.5   high   none                  0.4          680 #> 681       800         0.5   high   none                  0.4          681 #> 682       200         0.8   high   none                  0.4          682 #> 683       400         0.8   high   none                  0.4          683 #> 684       800         0.8   high   none                  0.4          684 #> 685       200         0.0   none medium                  0.4          685 #> 686       400         0.0   none medium                  0.4          686 #> 687       800         0.0   none medium                  0.4          687 #> 688       200         0.2   none medium                  0.4          688 #> 689       400         0.2   none medium                  0.4          689 #> 690       800         0.2   none medium                  0.4          690 #> 691       200         0.5   none medium                  0.4          691 #> 692       400         0.5   none medium                  0.4          692 #> 693       800         0.5   none medium                  0.4          693 #> 694       200         0.8   none medium                  0.4          694 #> 695       400         0.8   none medium                  0.4          695 #> 696       800         0.8   none medium                  0.4          696 #> 697       200         0.0 medium medium                  0.4          697 #> 698       400         0.0 medium medium                  0.4          698 #> 699       800         0.0 medium medium                  0.4          699 #> 700       200         0.2 medium medium                  0.4          700 #> 701       400         0.2 medium medium                  0.4          701 #> 702       800         0.2 medium medium                  0.4          702 #> 703       200         0.5 medium medium                  0.4          703 #> 704       400         0.5 medium medium                  0.4          704 #> 705       800         0.5 medium medium                  0.4          705 #> 706       200         0.8 medium medium                  0.4          706 #> 707       400         0.8 medium medium                  0.4          707 #> 708       800         0.8 medium medium                  0.4          708 #> 709       200         0.0   high medium                  0.4          709 #> 710       400         0.0   high medium                  0.4          710 #> 711       800         0.0   high medium                  0.4          711 #> 712       200         0.2   high medium                  0.4          712 #> 713       400         0.2   high medium                  0.4          713 #> 714       800         0.2   high medium                  0.4          714 #> 715       200         0.5   high medium                  0.4          715 #> 716       400         0.5   high medium                  0.4          716 #> 717       800         0.5   high medium                  0.4          717 #> 718       200         0.8   high medium                  0.4          718 #> 719       400         0.8   high medium                  0.4          719 #> 720       800         0.8   high medium                  0.4          720 #> 721       200         0.0   none   high                  0.4          721 #> 722       400         0.0   none   high                  0.4          722 #> 723       800         0.0   none   high                  0.4          723 #> 724       200         0.2   none   high                  0.4          724 #> 725       400         0.2   none   high                  0.4          725 #> 726       800         0.2   none   high                  0.4          726 #> 727       200         0.5   none   high                  0.4          727 #> 728       400         0.5   none   high                  0.4          728 #> 729       800         0.5   none   high                  0.4          729 #> 730       200         0.8   none   high                  0.4          730 #> 731       400         0.8   none   high                  0.4          731 #> 732       800         0.8   none   high                  0.4          732 #> 733       200         0.0 medium   high                  0.4          733 #> 734       400         0.0 medium   high                  0.4          734 #> 735       800         0.0 medium   high                  0.4          735 #> 736       200         0.2 medium   high                  0.4          736 #> 737       400         0.2 medium   high                  0.4          737 #> 738       800         0.2 medium   high                  0.4          738 #> 739       200         0.5 medium   high                  0.4          739 #> 740       400         0.5 medium   high                  0.4          740 #> 741       800         0.5 medium   high                  0.4          741 #> 742       200         0.8 medium   high                  0.4          742 #> 743       400         0.8 medium   high                  0.4          743 #> 744       800         0.8 medium   high                  0.4          744 #> 745       200         0.0   high   high                  0.4          745 #> 746       400         0.0   high   high                  0.4          746 #> 747       800         0.0   high   high                  0.4          747 #> 748       200         0.2   high   high                  0.4          748 #> 749       400         0.2   high   high                  0.4          749 #> 750       800         0.2   high   high                  0.4          750 #> 751       200         0.5   high   high                  0.4          751 #> 752       400         0.5   high   high                  0.4          752 #> 753       800         0.5   high   high                  0.4          753 #> 754       200         0.8   high   high                  0.4          754 #> 755       400         0.8   high   high                  0.4          755 #> 756       800         0.8   high   high                  0.4          756 get_dgm_condition(\"Carter2019\", condition_id = 1) #>   n_studies mean_effect  QRP bias effect_heterogeneity condition_id #> 1        10           0 none none                    0            1  head(dgm_conditions(\"Alinaghi2018\")) #>   environment mean_effect bias condition_id #> 1          RE         0.0 none            1 #> 2         PRE         0.0 none            2 #> 3          FE         0.0 none            3 #> 4          RE         0.5 none            4 #> 5         PRE         0.5 none            5 #> 6          FE         0.5 none            6  head(dgm_conditions(\"Stanley2017\")) #>   mean_effect effect_heterogeneity bias n_studies environment #> 1         0.0               0.0000    0         5    Cohens_d #> 2         0.5               0.0000    0         5    Cohens_d #> 3         0.0               0.0625    0         5    Cohens_d #> 4         0.5               0.0625    0         5    Cohens_d #> 5         0.0               0.1250    0         5    Cohens_d #> 6         0.5               0.1250    0         5    Cohens_d #>            sample_sizes condition_id #> 1 32, 64, 125, 250, 500            1 #> 2 32, 64, 125, 250, 500            2 #> 3 32, 64, 125, 250, 500            3 #> 4 32, 64, 125, 250, 500            4 #> 5 32, 64, 125, 250, 500            5 #> 6 32, 64, 125, 250, 500            6"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/download_dgm.html","id":null,"dir":"Reference","previous_headings":"","what":"Download Datasets of a DGM — download_dgm","title":"Download Datasets of a DGM — download_dgm","text":"function downloads datasets specified Data-Generating Mechanism (DGM). data located https://osf.io/exf3m/.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/download_dgm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Download Datasets of a DGM — download_dgm","text":"","code":"download_dgm_datasets(   dgm_name,   path = NULL,   overwrite = FALSE,   progress = TRUE )  download_dgm_results(dgm_name, path = NULL, overwrite = FALSE, progress = TRUE)  download_dgm_measures(   dgm_name,   path = NULL,   overwrite = FALSE,   progress = TRUE )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/download_dgm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Download Datasets of a DGM — download_dgm","text":"dgm_name Character string specifying name DGM dataset download. path Character string specifying directory path datasets/results/measures saved. Defaults location specified via PublicationBiasBenchmark.get_option(\"simulation_directory\"). objects stored dgm_name/datasets, dgm_name/results, dgm_name/measures subfolders. overwrite Logical indicating whether overwrite existing files. Defaults FALSE, means missing files downloaded. progress Logical indicating whether show progress downloading files. Defaults TRUE.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/download_dgm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Download Datasets of a DGM — download_dgm","text":"TRUE download successful, otherwise error raised.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/download_dgm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Download Datasets of a DGM — download_dgm","text":"","code":"if (FALSE) { # \\dontrun{   download_dgm_datasets(\"no_bias\") } # }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/measures.html","id":null,"dir":"Reference","previous_headings":"","what":"Performance Measures and Monte Carlo Standard Errors — measures","title":"Performance Measures and Monte Carlo Standard Errors — measures","text":"comprehensive set functions computing performance measures Monte Carlo Standard Errors (MCSE) simulation studies. functions based definitions Table 3 Siepe et al. (2024) . Winkler interval score defined Winkler (1972) . Positive negative likelihood ratios defined Huang Trinquart (2023)  Deeks Altman (2004) . Also see Morris et al. (2019)  additional details. Bias relative bias modified account possibly different true values across repetitions.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/measures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Performance Measures and Monte Carlo Standard Errors — measures","text":"","code":"bias(theta_hat, theta)  bias_mcse(theta_hat)  relative_bias(theta_hat, theta)  relative_bias_mcse(theta_hat, theta)  mse(theta_hat, theta)  mse_mcse(theta_hat, theta)  rmse(theta_hat, theta)  rmse_mcse(theta_hat, theta)  empirical_variance(theta_hat)  empirical_variance_mcse(theta_hat)  empirical_se(theta_hat)  empirical_se_mcse(theta_hat)  coverage(ci_lower, ci_upper, theta)  coverage_mcse(ci_lower, ci_upper, theta)  power(test_rejects_h0)  power_mcse(test_rejects_h0)  mean_ci_width(ci_upper, ci_lower)  mean_ci_width_mcse(ci_upper, ci_lower)  mean_generic_statistic(G)  mean_generic_statistic_mcse(G)  positive_likelihood_ratio(tp, fp, fn, tn)  positive_likelihood_ratio_mcse(tp, fp, fn, tn)  negative_likelihood_ratio(tp, fp, fn, tn)  negative_likelihood_ratio_mcse(tp, fp, fn, tn)  interval_score(ci_lower, ci_upper, theta, alpha = 0.05)  interval_score_mcse(ci_lower, ci_upper, theta, alpha = 0.05)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/measures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Performance Measures and Monte Carlo Standard Errors — measures","text":"theta_hat Vector parameter estimates simulations theta True parameter value ci_lower Vector lower confidence interval bounds ci_upper Vector upper confidence interval bounds test_rejects_h0 Logical vector indicating whether statistical tests reject null hypothesis G Vector generic statistics simulations tp Numeric count true positive hypothesis tests fp Numeric count false positive hypothesis tests fn Numeric count false negative hypothesis tests tn Numeric count true negative hypothesis tests alpha Numeric indicating 1 - coverage level interval_score calculation","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/measures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Performance Measures and Monte Carlo Standard Errors — measures","text":"metric function returns numeric value representing performance measure. MCSE function returns numeric value representing Monte Carlo standard error.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/measures.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Performance Measures and Monte Carlo Standard Errors — measures","text":"package provides following performance measures corresponding MCSE functions: bias(theta_hat, theta): Bias estimate relative_bias(theta_hat, theta): Relative bias estimate mse(theta_hat, theta): Mean Square Error rmse(theta_hat, theta): Root Mean Square Error empirical_variance(theta_hat): Empirical variance empirical_se(theta_hat): Empirical standard error coverage(ci_lower, ci_upper, theta): Coverage probability mean_ci_width(ci_upper, ci_lower): Mean confidence interval width interval_score(ci_lower, ci_upper, theta, alpha): interval_score power(test_rejects_h0): Statistical power positive_likelihood_ratio(tp, fp, fn, tn): Log positive likelihood ratio negative_likelihood_ratio(tp, fp, fn, tn): Log negative likelihood ratio mean_generic_statistic(G): Mean generic statistic","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/measures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Performance Measures and Monte Carlo Standard Errors — measures","text":"","code":"# Generate some example data set.seed(123) theta_true <- 0.5 theta_estimates <- rnorm(1000, mean = theta_true, sd = 0.1)  # Compute bias and its MCSE bias_est <- bias(theta_estimates, theta_true) bias_se <- bias_mcse(theta_estimates)  # Compute MSE and its MCSE mse_est <- mse(theta_estimates, theta_true) mse_se <- mse_mcse(theta_estimates, theta_true)  # Example with coverage ci_lower <- theta_estimates - 1.96 * 0.1 ci_upper <- theta_estimates + 1.96 * 0.1 coverage_est <- coverage(ci_lower, ci_upper, theta_true) coverage_se <- coverage_mcse(ci_lower, ci_upper, theta_true)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.AK.html","id":null,"dir":"Reference","previous_headings":"","what":"AK Method — method.AK","title":"AK Method — method.AK","text":"Implements Andrews & Kasy (AK) method publication bias correction meta-analysis. AK method categorizes estimated effects groups different probabilities published. AK1 uses symmetric selection grouping estimates significant (|t| ≥ 1.96) insignificant (|t| < 1.96) estimates. AK2 uses asymmetric selection four groups based significance sign: highly significant positive/negative effects marginally significant positive/negative effects, different publication probabilities. See Andrews Kasy (2019)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.AK.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"AK Method — method.AK","text":"","code":"# S3 method for class 'AK' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.AK.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"AK Method — method.AK","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes), sei (standard errors), study_id (clustering whereever available) settings List method settings (see Details.)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.AK.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"AK Method — method.AK","text":"Data frame AK results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.AK.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"AK Method — method.AK","text":"following settings implemented \"default\" Uses AK1 estimator (symmetric selection) \"AK1\" Symmetric selection model grouping estimates significant (|t| ≥ 1.96) insignificant (|t| < 1.96) categories relative publication probabilities 1 p1 respectively. \"AK2\" Asymmetric selection model four groups based t-statistics: () t ≥ 1.96, (b) t < -1.96, (c) -1.96 ≤ t < 0, (d) 0 ≤ t < 1.96, relative publication probabilities 1, p1, p2, p3 respectively.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.AK.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"AK Method — method.AK","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply AK method result <- run_method(\"AK\", data, \"default\") #> Warning: NaNs produced print(result) #>   method  estimate standard_error   ci_lower  ci_upper    p_value BF #> 1     AK 0.1239951     0.07280359 -0.1892534 0.4372437 0.09217477 NA #>   convergence note tau_estimate      tau2_se   bias_coefficient #> 1        TRUE   NA            0 1.084652e-10 0.0685673164403796 #>   bias_coefficient_se version method_setting #> 1   0.110251775402335     AK1        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.EK.html","id":null,"dir":"Reference","previous_headings":"","what":"Endogenous Kink Method — method.EK","title":"Endogenous Kink Method — method.EK","text":"Implements endogenous kink (EK) method proposed Bom Rachinger publication bias correction meta-analysis. method modifies PET-PEESE approach incorporating non-linear relationship publication bias standard errors kinked regression specification. method recognizes true effect non-zero, minimal publication selection standard errors small (since estimates significant), selection increases standard errors grow. kink point endogenously determined using two-step procedure based confidence interval initial effect estimate. See Bom Rachinger (2019)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.EK.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Endogenous Kink Method — method.EK","text":"","code":"# S3 method for class 'EK' method(method_name, data, settings = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.EK.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Endogenous Kink Method — method.EK","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (settings version implemented)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.EK.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Endogenous Kink Method — method.EK","text":"Data frame EK results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.EK.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Endogenous Kink Method — method.EK","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply EK method result <- run_method(\"EK\", data) print(result) #>   method   estimate standard_error   ci_lower ci_upper   p_value BF convergence #> 1     EK -0.1360294      0.1863442 -0.7290598 0.457001 0.5182334 NA        TRUE #>   note method_setting #> 1   NA        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.FMA.html","id":null,"dir":"Reference","previous_headings":"","what":"Fixed Effects Meta-Analysis Method — method.FMA","title":"Fixed Effects Meta-Analysis Method — method.FMA","text":"Implements publication bias-unadjusted fixed effects meta-analysis.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.FMA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fixed Effects Meta-Analysis Method — method.FMA","text":"","code":"# S3 method for class 'FMA' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.FMA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fixed Effects Meta-Analysis Method — method.FMA","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (see Details.)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.FMA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fixed Effects Meta-Analysis Method — method.FMA","text":"Data frame FMA results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.FMA.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fixed Effects Meta-Analysis Method — method.FMA","text":"following settings implemented \"default\" T-distribution adjustment (test = \"t\") cluster robust standard errors small-sample adjustment (converged, otherwise small-sample adjustment cluster robust standard errors) fixed effects meta-analysis study_ids specified data","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.FMA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fixed Effects Meta-Analysis Method — method.FMA","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply FMA method result <- run_method(\"FMA\", data) print(result) #>   method  estimate standard_error   ci_lower  ci_upper     p_value BF #> 1    FMA 0.2179928     0.04501055 0.09302349 0.3429621 0.008380921 NA #>   convergence note tau_p_value method_setting #> 1        TRUE   NA   0.2941821        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PEESE.html","id":null,"dir":"Reference","previous_headings":"","what":"PEESE (Precision-Effect Estimate with Standard Errors) Method — method.PEESE","title":"PEESE (Precision-Effect Estimate with Standard Errors) Method — method.PEESE","text":"Implements Precision-Effect Estimate Standard Errors method publication bias correction. PEESE regresses effect sizes standard errors^2 correct publication bias. intercept represents bias-corrected effect size estimate. See Stanley Doucouliagos (2014)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PEESE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PEESE (Precision-Effect Estimate with Standard Errors) Method — method.PEESE","text":"","code":"# S3 method for class 'PEESE' method(method_name, data, settings = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PEESE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PEESE (Precision-Effect Estimate with Standard Errors) Method — method.PEESE","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (settings version implemented)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PEESE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"PEESE (Precision-Effect Estimate with Standard Errors) Method — method.PEESE","text":"Data frame PEESE results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PEESE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PEESE (Precision-Effect Estimate with Standard Errors) Method — method.PEESE","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply PEESE method result <- run_method(\"PEESE\", data) print(result) #>             method   estimate standard_error   ci_lower  ci_upper   p_value BF #> (Intercept)  PEESE 0.06720823     0.09919638 -0.1272167 0.2616331 0.5466436 NA #>             convergence note bias_coefficient bias_p_value method_setting #> (Intercept)        TRUE   NA         14.88532    0.1927957        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PET.html","id":null,"dir":"Reference","previous_headings":"","what":"PET (Precision-Effect Test) Method — method.PET","title":"PET (Precision-Effect Test) Method — method.PET","text":"Implements Precision-Effect Test publication bias correction. PET regresses effect sizes standard errors test correct publication bias. intercept represents bias-corrected effect size estimate. See Stanley Doucouliagos (2014)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PET.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PET (Precision-Effect Test) Method — method.PET","text":"","code":"# S3 method for class 'PET' method(method_name, data, settings = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PET.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PET (Precision-Effect Test) Method — method.PET","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (settings version implemented)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PET.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"PET (Precision-Effect Test) Method — method.PET","text":"Data frame PET results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PET.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PET (Precision-Effect Test) Method — method.PET","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply PET method result <- run_method(\"PET\", data) print(result) #>             method   estimate standard_error  ci_lower  ci_upper   p_value BF #> (Intercept)    PET -0.1360294      0.1863442 -0.501264 0.2292052 0.5182334 NA #>             convergence note bias_coefficient bias_p_value method_setting #> (Intercept)        TRUE   NA          3.59473     0.147487        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PETPEESE.html","id":null,"dir":"Reference","previous_headings":"","what":"PET-PEESE (Precision-Effect Test and Precision-Effect Estimate with Standard Errors) Method — method.PETPEESE","title":"PET-PEESE (Precision-Effect Test and Precision-Effect Estimate with Standard Errors) Method — method.PETPEESE","text":"Implements Precision-Effect Test Precision-Effect Estimate Standard Errors (PET-PEESE) regresses effect sizes standard errors^2 correct publication bias. intercept represents bias-corrected effect size estimate. See Stanley Doucouliagos (2014)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PETPEESE.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PET-PEESE (Precision-Effect Test and Precision-Effect Estimate with Standard Errors) Method — method.PETPEESE","text":"","code":"# S3 method for class 'PETPEESE' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PETPEESE.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"PET-PEESE (Precision-Effect Test and Precision-Effect Estimate with Standard Errors) Method — method.PETPEESE","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (see Details)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PETPEESE.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"PET-PEESE (Precision-Effect Test and Precision-Effect Estimate with Standard Errors) Method — method.PETPEESE","text":"Data frame PET-PEESE results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PETPEESE.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"PET-PEESE (Precision-Effect Test and Precision-Effect Estimate with Standard Errors) Method — method.PETPEESE","text":"following settings implemented \"default\" (conditional_alpha = 0.10) determines whether use PET (PET's effect significant alpha = 0.10 PEESE estimate (PET's effect significant alpha = 0.10)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.PETPEESE.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"PET-PEESE (Precision-Effect Test and Precision-Effect Estimate with Standard Errors) Method — method.PETPEESE","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply PETPEESE method result <- run_method(\"PETPEESE\", data) print(result) #>               method   estimate standard_error  ci_lower  ci_upper   p_value BF #> (Intercept) PETPEESE -0.1360294      0.1863442 -0.501264 0.2292052 0.5182334 NA #>             convergence note bias_coefficient bias_p_value selected_method #> (Intercept)        TRUE   NA          3.59473     0.147487             PET #>             method_setting #> (Intercept)        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RMA.html","id":null,"dir":"Reference","previous_headings":"","what":"Random Effects Meta-Analysis Method — method.RMA","title":"Random Effects Meta-Analysis Method — method.RMA","text":"Implements publication bias-unadjusted random effects meta-analysis.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RMA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Random Effects Meta-Analysis Method — method.RMA","text":"","code":"# S3 method for class 'RMA' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RMA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Random Effects Meta-Analysis Method — method.RMA","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (see Details.)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RMA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Random Effects Meta-Analysis Method — method.RMA","text":"Data frame RMA results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RMA.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Random Effects Meta-Analysis Method — method.RMA","text":"following settings implemented \"default\" Restricted Maximum Likelihood estimator (method = \"REML\") Knapp-Hartung adjustment (test = \"knha\") simple random effects meta-analysis Restricted Maximum Likelihood estimator (method = \"REML\") t-distribution adjustment (test = \"t\") cluster robust standard errors small-sample adjustment (converged, otherwise small-sample adjustment cluster robust standard errors) multilevel random effects meta-analysis study_ids specified data","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RMA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Random Effects Meta-Analysis Method — method.RMA","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply RMA method result <- run_method(\"RMA\", data) print(result) #>   method  estimate standard_error   ci_lower  ci_upper    p_value BF #> 1    RMA 0.2255651     0.05033069 0.08582468 0.3653055 0.01097584 NA #>   convergence note tau_estimate tau_ci_lower tau_ci_upper tau_p_value #> 1        TRUE   NA   0.05721499            0    0.3038259   0.2941821 #>   method_setting #> 1        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RoBMA.html","id":null,"dir":"Reference","previous_headings":"","what":"Robust Bayesian Meta-Analysis (RoBMA) Method — method.RoBMA","title":"Robust Bayesian Meta-Analysis (RoBMA) Method — method.RoBMA","text":"Implements robust Bayesian meta-analysis (RoBMA) method uses Bayesian model-averaging combine results across several complementary publication bias adjustment methods. See Maier et al. (2023)  Bartoš et al. (2023)  details. Note prior settings dispatched based \"es_type\" column attached dataset. resulting estimates summarized scale dataset input (\"r\", heterogeneity summarized Fisher's z).","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RoBMA.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Robust Bayesian Meta-Analysis (RoBMA) Method — method.RoBMA","text":"","code":"# S3 method for class 'RoBMA' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RoBMA.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Robust Bayesian Meta-Analysis (RoBMA) Method — method.RoBMA","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes), sei (standard errors), es_type (either \"SMD\" Cohen's d / Hedge's g, \"logOR\" log odds ratio, \"z\" Fisher's z, \"r\" correlations. Defaults \"none\" re-scales default priors unit-information width based total sample size supplied \"ni\".) settings List method settings (see Details.)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RoBMA.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Robust Bayesian Meta-Analysis (RoBMA) Method — method.RoBMA","text":"Data frame RoBMA results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RoBMA.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Robust Bayesian Meta-Analysis (RoBMA) Method — method.RoBMA","text":"following settings implemented \"default\" RoBMA-PSMA publication bias adjustment described Bartoš et al. (2023) . (MCMC settings reduced speed-simulations) three-level specification whenever \"study_ids\" supplied data \"PSMA\" RoBMA-PSMA publication bias adjustment described Bartoš et al. (2023) . (MCMC settings reduced speed-simulations) three-level specification whenever \"study_ids\" supplied data","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.RoBMA.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Robust Bayesian Meta-Analysis (RoBMA) Method — method.RoBMA","text":"","code":"if (FALSE) { # \\dontrun{ # Generate some example data data <- data.frame(   yi      = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei     = c(0.1, 0.15, 0.08, 0.12, 0.09),   es_type = \"SMD\" )  # Apply RoBMA method result <- run_method(\"RoBMA\", data) print(result) } # }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.SM.html","id":null,"dir":"Reference","previous_headings":"","what":"SM (Selection Models) Method — method.SM","title":"SM (Selection Models) Method — method.SM","text":"Implements Selection Models publication bias correction meta-analysis. method first fits random effects meta-analysis model, applies selection modeling adjust publication bias using metafor package. Selection models account probability studies published based p-values effect sizes. See Vevea Hedges (1995)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.SM.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"SM (Selection Models) Method — method.SM","text":"","code":"# S3 method for class 'SM' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.SM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"SM (Selection Models) Method — method.SM","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (see Details.)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.SM.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"SM (Selection Models) Method — method.SM","text":"Data frame SM results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.SM.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"SM (Selection Models) Method — method.SM","text":"following settings implemented \"default\" \"3PSM\" 3-parameter step function selection model Maximum Likelihood estimator (method = \"ML\") one step one-sided p = 0.025 (.e., selection significance)) \"4PSM\" 4-parameter step function selection model Maximum Likelihood estimator (method = \"ML\") two steps one-sided p = 0.025 p = 0.50 (.e., selection significance direction effect)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.SM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"SM (Selection Models) Method — method.SM","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply SM method result <- run_method(\"SM\", data, \"3PSM\") #> Error : Optimizer (optim) did not achieve convergence (convergence = 1). print(result) #>   method estimate standard_error ci_lower ci_upper p_value BF convergence #> 1     SM       NA             NA       NA       NA      NA NA       FALSE #>                                                                         note #> 1 Error : Optimizer (optim) did not achieve convergence (convergence = 1).\\n #>   tau_estimate tau_ci_lower tau_ci_upper tau_p_value bias_coefficient #> 1           NA           NA           NA          NA               NA #>   bias_coefficient_se bias_p_value method_setting #> 1                  NA           NA           3PSM"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WAAPWLS.html","id":null,"dir":"Reference","previous_headings":"","what":"WAAPWLS (Weighted Average of Adequately Powered Studies) Method — method.WAAPWLS","title":"WAAPWLS (Weighted Average of Adequately Powered Studies) Method — method.WAAPWLS","text":"Implements WAAPWLS method meta-analysis, combines WLS WAAP approaches. First fits WLS model studies, identifies high-powered studies based criterion WLS estimate divided 2.8 greater equal standard error. least 2 high-powered studies found, uses WAAP (weighted average adequate power studies ), otherwise uses original WLS estimate. See Stanley et al. (2017)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WAAPWLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"WAAPWLS (Weighted Average of Adequately Powered Studies) Method — method.WAAPWLS","text":"","code":"# S3 method for class 'WAAPWLS' method(method_name, data, settings = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WAAPWLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"WAAPWLS (Weighted Average of Adequately Powered Studies) Method — method.WAAPWLS","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (settings version implemented)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WAAPWLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"WAAPWLS (Weighted Average of Adequately Powered Studies) Method — method.WAAPWLS","text":"Data frame WAAPWLS results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WAAPWLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"WAAPWLS (Weighted Average of Adequately Powered Studies) Method — method.WAAPWLS","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply WAAPWLS method result <- run_method(\"WAAPWLS\", data) print(result) #>              method  estimate standard_error  ci_lower  ci_upper    p_value BF #> (Intercept) WAAPWLS 0.2179928     0.04998789 0.1200165 0.3159691 0.01205352 NA #>             convergence note selected_method n_high_powered method_setting #> (Intercept)        TRUE   NA             WLS              0        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WILS.html","id":null,"dir":"Reference","previous_headings":"","what":"Weighted and Iterated Least Squares (WILS) Method — method.WILS","title":"Weighted and Iterated Least Squares (WILS) Method — method.WILS","text":"Implements weighted iterated least squares (WILS) method publication bias correction meta-analysis. method based idea using excess statistical significance (ESS) identify many underpowered studies removed reduce publication selection bias. See Stanley Doucouliagos (2024)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WILS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weighted and Iterated Least Squares (WILS) Method — method.WILS","text":"","code":"# S3 method for class 'WILS' method(method_name, data, settings = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WILS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weighted and Iterated Least Squares (WILS) Method — method.WILS","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (settings version implemented)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WILS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Weighted and Iterated Least Squares (WILS) Method — method.WILS","text":"Data frame WILS results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WILS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weighted and Iterated Least Squares (WILS) Method — method.WILS","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply WILS method result <- run_method(\"WILS\", data) print(result) #>             method  estimate standard_error   ci_lower  ci_upper   p_value BF #> (Intercept)   WILS 0.1390244     0.04878049 0.04341463 0.2346341 0.2148312 NA #>             convergence note n_removed method_setting #> (Intercept)        TRUE   NA         3        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WLS.html","id":null,"dir":"Reference","previous_headings":"","what":"WLS (Weighted Least Squares) Method — method.WLS","title":"WLS (Weighted Least Squares) Method — method.WLS","text":"Implements Weighted Least Squares method meta-analysis. WLS fits weighted regression model effect sizes outcome weights based inverse squared standard errors. intercept represents weighted average effect size estimate.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WLS.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"WLS (Weighted Least Squares) Method — method.WLS","text":"","code":"# S3 method for class 'WLS' method(method_name, data, settings = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WLS.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"WLS (Weighted Least Squares) Method — method.WLS","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (settings version implemented)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WLS.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"WLS (Weighted Least Squares) Method — method.WLS","text":"Data frame WLS results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.WLS.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"WLS (Weighted Least Squares) Method — method.WLS","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply WLS method result <- run_method(\"WLS\", data) print(result) #>             method  estimate standard_error  ci_lower  ci_upper    p_value BF #> (Intercept)    WLS 0.2179928     0.04998789 0.1200165 0.3159691 0.01205352 NA #>             convergence note method_setting #> (Intercept)        TRUE   NA        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.default.html","id":null,"dir":"Reference","previous_headings":"","what":"Default method handler — method.default","title":"Default method handler — method.default","text":"Default method handler","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.default.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Default method handler — method.default","text":"","code":"# Default S3 method method(method_name, data, settings = list())"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.default.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Default method handler — method.default","text":"method_name Character string specifying method type data Data frame containing yi (effect sizes) sei (standard errors) settings Either character identifying method version list containing method-specific settings. emty input result running default (first implemented) version method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.html","id":null,"dir":"Reference","previous_headings":"","what":"Method Method — method","title":"Method Method — method","text":"S3 Method defining methods. See run_method() usage details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Method Method — method","text":"","code":"method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Method Method — method","text":"method_name Character string specifying method type data Data frame containing yi (effect sizes) sei (standard errors) settings Either character identifying method version list containing method-specific settings. emty input result running default (first implemented) version method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.html","id":"output-structure","dir":"Reference","previous_headings":"","what":"Output Structure","title":"Method Method — method","text":"returned data frame follows standardized schema downstream functions rely . Across currently implemented DGMs, following columns used: yi (numeric): effect size estimate. sei (numeric): Standard error yi. ni (integer): Total sample size estimate (e.g., sum groups applicable). es_type (character): Effect size type, used disambiguate scale yi. Currently used values \"SMD\" (standardized mean difference / Cohen's d), \"logOR\" (log odds ratio), \"none\" (unspecified generic continuous coefficient). study_id (integer/character, optional): Identifier primary study/cluster DGM yields multiple estimates per study (e.g., Alinaghi2018, PRE). absent, row treated independent study.","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Method Method — method","text":"","code":"data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4),   sei = c(0.1, 0.15, 0.08, 0.12) ) result <- run_method(\"RMA\", data, \"default\")"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.mean.html","id":null,"dir":"Reference","previous_headings":"","what":"Mean Method — method.mean","title":"Mean Method — method.mean","text":"Implements unweighted mean method. .e., mean observed effect sizes.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.mean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Mean Method — method.mean","text":"","code":"# S3 method for class 'mean' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.mean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Mean Method — method.mean","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) settings List method settings (see Details.)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.mean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Mean Method — method.mean","text":"Data frame mean results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.mean.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Mean Method — method.mean","text":"following settings implemented \"default\" settings","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.mean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Mean Method — method.mean","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply mean method result <- run_method(\"mean\", data) print(result) #>   method estimate standard_error ci_lower ci_upper      p_value BF convergence #> 1   mean     0.25     0.04955805 0.152868 0.347132 4.544962e-07 NA        TRUE #>   note method_setting #> 1   NA        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.pcurve.html","id":null,"dir":"Reference","previous_headings":"","what":"pcurve (P-Curve) Method — method.pcurve","title":"pcurve (P-Curve) Method — method.pcurve","text":"Implements P-Curve method. P-Curve analyzes distribution p-values significant studies assess whether significant findings reflect true effects QRP/publication bias. method also provides tests evidential value, lack evidential value, p-hacking. See Simonsohn et al. (2014)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.pcurve.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"pcurve (P-Curve) Method — method.pcurve","text":"","code":"# S3 method for class 'pcurve' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.pcurve.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"pcurve (P-Curve) Method — method.pcurve","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes), sei (standard errors), ni (sample sizes wherever available, otherwise set Inf) settings List method settings (see Details)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.pcurve.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"pcurve (P-Curve) Method — method.pcurve","text":"Data frame P-Curve results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.pcurve.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"pcurve (P-Curve) Method — method.pcurve","text":"following settings implemented \"default\" options","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.pcurve.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"pcurve (P-Curve) Method — method.pcurve","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply pcurve method result <- run_method(\"pcurve\", data) print(result) #>   method estimate standard_error ci_lower ci_upper p_value BF convergence note #> 1 pcurve  3.99994             NA       NA       NA      NA NA        TRUE   NA #>   p_value_evidence p_value_lack p_value_hack method_setting #> 1        0.3699365    0.6300635    0.1962818        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.puniform.html","id":null,"dir":"Reference","previous_headings":"","what":"puniform (P-Uniform) Method — method.puniform","title":"puniform (P-Uniform) Method — method.puniform","text":"Implements P-Uniform method publication bias detection correction. P-Uniform uses distribution p-values significant studies test publication bias estimate effect size corrected publication bias. method assumes p-values follow uniform distribution null hypothesis effect, uses detect correct bias. See Van Assen et al. (2015)  van Aert van Assen (2025)  details.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.puniform.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"puniform (P-Uniform) Method — method.puniform","text":"","code":"# S3 method for class 'puniform' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.puniform.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"puniform (P-Uniform) Method — method.puniform","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (see Details)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.puniform.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"puniform (P-Uniform) Method — method.puniform","text":"Data frame P-Uniform results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.puniform.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"puniform (P-Uniform) Method — method.puniform","text":"following settings implemented \"default\" Default p-uniform analysis settings.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.puniform.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"puniform (P-Uniform) Method — method.puniform","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply puniform method result <- run_method(\"puniform\", data) print(result) #>     method   estimate standard_error  ci_lower  ci_upper   p_value BF #> 1 puniform 0.03038879             NA -2.250357 0.3192963 0.4631793 NA #>   convergence note  version tau_estimate tau_ci_lower tau_ci_upper tau_p_value #> 1        TRUE   NA original           NA           NA           NA          NA #>   bias_p_value method_setting #> 1    0.1467798        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.trimfill.html","id":null,"dir":"Reference","previous_headings":"","what":"Trim-and-Fill Meta-Analysis Method — method.trimfill","title":"Trim-and-Fill Meta-Analysis Method — method.trimfill","text":"Implements trim--fill method adjusting publication bias meta-analysis using metafor package.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.trimfill.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trim-and-Fill Meta-Analysis Method — method.trimfill","text":"","code":"# S3 method for class 'trimfill' method(method_name, data, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.trimfill.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trim-and-Fill Meta-Analysis Method — method.trimfill","text":"method_name Method name (automatically passed) data Data frame yi (effect sizes) sei (standard errors) settings List method settings (see Details.)","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.trimfill.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trim-and-Fill Meta-Analysis Method — method.trimfill","text":"Data frame trim--fill results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.trimfill.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Trim-and-Fill Meta-Analysis Method — method.trimfill","text":"following settings implemented \"default\" Random effects model fitted Restricted Maximum Likelihood estimator (method = \"REML\") Knapp-Hartung adjustment (test = \"knha\"), followed trim--fill using left-side trimming (side = \"left\") L0 estimator (estimator = \"L0\").","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method.trimfill.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Trim-and-Fill Meta-Analysis Method — method.trimfill","text":"","code":"# Generate some example data data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4, 0.25),   sei = c(0.1, 0.15, 0.08, 0.12, 0.09) )  # Apply trimfill method result <- run_method(\"trimfill\", data) print(result) #>     method  estimate standard_error   ci_lower  ci_upper      p_value BF #> 1 trimfill 0.1774672     0.05366124 0.07229309 0.2826413 0.0009424154 NA #>   convergence note tau_estimate tau_ci_lower tau_ci_upper tau_p_value k_missing #> 1        TRUE   NA    0.0877971            0    0.3178726   0.1226888         2 #>   k_missing_se method_setting #> 1     1.602467        default"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_extra_columns.html","id":null,"dir":"Reference","previous_headings":"","what":"Method Extra Columns — method_extra_columns","title":"Method Extra Columns — method_extra_columns","text":"Retrieves character vector custom columns given method. method-specific columns beyond standard columns (method, estimate, standard_error, ci_lower, ci_upper, p_value, BF, convergence, note) method returns.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_extra_columns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Method Extra Columns — method_extra_columns","text":"","code":"get_method_extra_columns(method_name)  method_extra_columns(method_name)  # Default S3 method method_extra_columns(method_name)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_extra_columns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Method Extra Columns — method_extra_columns","text":"method_name Character string method name","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_extra_columns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Method Extra Columns — method_extra_columns","text":"Character vector extra column names, empty character vector extra columns defined method","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_extra_columns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Method Extra Columns — method_extra_columns","text":"","code":"# Get extra columns for PET method get_method_extra_columns(\"PET\") #> [1] \"bias_coefficient\" \"bias_p_value\"      # Get extra columns for RMA method get_method_extra_columns(\"RMA\") #> [1] \"tau_estimate\" \"tau_ci_lower\" \"tau_ci_upper\" \"tau_p_value\""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_settings.html","id":null,"dir":"Reference","previous_headings":"","what":"Return Pre-specified Method Settings — method_settings","title":"Return Pre-specified Method Settings — method_settings","text":"function returns list pre-specified settings given Method","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_settings.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Return Pre-specified Method Settings — method_settings","text":"","code":"method_settings(method_name)  get_method_setting(method_name, version_id)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_settings.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Return Pre-specified Method Settings — method_settings","text":"method_name Character string specifying method type version_id method version used.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_settings.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Return Pre-specified Method Settings — method_settings","text":"list containing pre-specified settings. methods, list contains extension function call, however, elaborate list settings dispatched within method call possible.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/method_settings.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Return Pre-specified Method Settings — method_settings","text":"","code":"method_settings(\"RMA\") #> $default #> $default$method #> [1] \"REML\" #>  #> $default$test.uni #> [1] \"knha\" #>  #> $default$test.mv #> [1] \"t\" #>  #> $default$control #> $default$control$stepadj #> [1] 0.5 #>  #> $default$control$maxiter #> [1] 500 #>  #>  #>  get_method_setting(\"RMA\", version_id = \"default\") #> $method #> [1] \"REML\" #>  #> $test.uni #> [1] \"knha\" #>  #> $test.mv #> [1] \"t\" #>  #> $control #> $control$stepadj #> [1] 0.5 #>  #> $control$maxiter #> [1] 500 #>  #>"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_dataset.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a Pre-Simulated Condition and Repetition From a DGM — retrieve_dgm_dataset","title":"Retrieve a Pre-Simulated Condition and Repetition From a DGM — retrieve_dgm_dataset","text":"function returns pre-simulated dataset given repetition condition dgm. pre-simulated datasets must already stored locally. See download_dgm_datasets() function guidance.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_dataset.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a Pre-Simulated Condition and Repetition From a DGM — retrieve_dgm_dataset","text":"","code":"retrieve_dgm_dataset(dgm_name, condition_id, repetition_id = NULL, path = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_dataset.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a Pre-Simulated Condition and Repetition From a DGM — retrieve_dgm_dataset","text":"dgm_name Character string specifying DGM type condition_id conditions settings returned . repetition_id repetition returned. complete condition can returned setting either NULL. path Character string specifying directory path datasets/results/measures saved. Defaults location specified via PublicationBiasBenchmark.get_option(\"simulation_directory\"). objects stored dgm_name/datasets, dgm_name/results, dgm_name/measures subfolders.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_dataset.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a Pre-Simulated Condition and Repetition From a DGM — retrieve_dgm_dataset","text":"data.frame","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_dataset.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve a Pre-Simulated Condition and Repetition From a DGM — retrieve_dgm_dataset","text":"","code":"if (FALSE) { # \\dontrun{   # get condition 1, repetition 1   retrieve_dgm_dataset(\"no_bias\", condition_id = 1, repetition_id = 1)    # get condition 1, all repetitions   retrieve_dgm_dataset(\"no_bias\", condition_id = 1) } # }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_measures.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve Pre-Computed Performance measures for a DGM — retrieve_dgm_measures","title":"Retrieve Pre-Computed Performance measures for a DGM — retrieve_dgm_measures","text":"function returns pre-computed performance measures specified Data-Generating Mechanism (DGM). pre-computed measures must already stored locally. See download_dgm_measures() function guidance.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_measures.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve Pre-Computed Performance measures for a DGM — retrieve_dgm_measures","text":"","code":"retrieve_dgm_measures(   dgm_name,   measure = NULL,   method = NULL,   condition_id = NULL,   path = NULL,   replacement = FALSE )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_measures.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve Pre-Computed Performance measures for a DGM — retrieve_dgm_measures","text":"dgm_name Character string specifying DGM type measure performance measure returned (e.g., \"bias\", \"mse\", \"coverage\"). measures can returned setting NULL. method method returned. methods can returned setting NULL. condition_id conditions settings returned . path Character string specifying directory path datasets/results/measures saved. Defaults location specified via PublicationBiasBenchmark.get_option(\"simulation_directory\"). objects stored dgm_name/datasets, dgm_name/results, dgm_name/measures subfolders. replacement Whether performance measures computed using replacement returned. Defaults FALSE.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_measures.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve Pre-Computed Performance measures for a DGM — retrieve_dgm_measures","text":"data.frame","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_measures.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve Pre-Computed Performance measures for a DGM — retrieve_dgm_measures","text":"","code":"if (FALSE) { # \\dontrun{   # get bias measures for all methods and conditions   retrieve_dgm_measures(\"no_bias\", measure = \"bias\")    # get all measures for RMA method   retrieve_dgm_measures(\"no_bias\", method = \"RMA\")    # get MSE measures for PET method in condition 1   retrieve_dgm_measures(\"no_bias\", measure = \"mse\", method = \"PET\", condition_id = 1) } # }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_results.html","id":null,"dir":"Reference","previous_headings":"","what":"Retrieve a Pre-Computed Results of a Method Applied to DGM — retrieve_dgm_results","title":"Retrieve a Pre-Computed Results of a Method Applied to DGM — retrieve_dgm_results","text":"function returns pre-computed results given method specific repetition condition dgm. pre-computed results must already stored locally. See download_dgm_results() function guidance.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_results.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Retrieve a Pre-Computed Results of a Method Applied to DGM — retrieve_dgm_results","text":"","code":"retrieve_dgm_results(   dgm_name,   method = NULL,   method_setting = \"default\",   condition_id = NULL,   repetition_id = NULL,   path = NULL )"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_results.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Retrieve a Pre-Computed Results of a Method Applied to DGM — retrieve_dgm_results","text":"dgm_name Character string specifying DGM type method method returned. complete results can returned setting NULL. method_setting method setting returned. Defaults \"default\". condition_id conditions settings returned . repetition_id repetition returned. complete condition can returned setting either NULL. path Character string specifying directory path datasets/results/measures saved. Defaults location specified via PublicationBiasBenchmark.get_option(\"simulation_directory\"). objects stored dgm_name/datasets, dgm_name/results, dgm_name/measures subfolders.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_results.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Retrieve a Pre-Computed Results of a Method Applied to DGM — retrieve_dgm_results","text":"data.frame","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/retrieve_dgm_results.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Retrieve a Pre-Computed Results of a Method Applied to DGM — retrieve_dgm_results","text":"","code":"if (FALSE) { # \\dontrun{   # get condition 1, repetition 1 for default method setting   retrieve_dgm_results(\"no_bias\", condition_id = 1, repetition_id = 1)    # get condition 1, all repetitions for default method setting   retrieve_dgm_results(\"no_bias\", condition_id = 1) } # }"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/run_method.html","id":null,"dir":"Reference","previous_headings":"","what":"Generic method function for publication bias correction — run_method","title":"Generic method function for publication bias correction — run_method","text":"function provides unified interface various publication bias correction methods. specific method determined first argument. See vignette(\"Adding_New_Methods\", package = \"PublicationBiasBenchmark\") details extending package new methods","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/run_method.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generic method function for publication bias correction — run_method","text":"","code":"run_method(method_name, data, settings = NULL)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/run_method.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generic method function for publication bias correction — run_method","text":"method_name Character string specifying method type data Data frame containing yi (effect sizes) sei (standard errors) settings Either character identifying method version list containing method-specific settings. emty input result running default (first implemented) version method.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/run_method.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generic method function for publication bias correction — run_method","text":"data frame standardized method results","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/run_method.html","id":"output-structure","dir":"Reference","previous_headings":"","what":"Output Structure","title":"Generic method function for publication bias correction — run_method","text":"returned data frame follows standardized schema downstream functions rely . methods return following columns: method (character): name method used. estimate (numeric): meta-analytic effect size estimate. standard_error (numeric): Standard error estimate. ci_lower (numeric): Lower bound 95% confidence interval. ci_upper (numeric): Upper bound 95% confidence interval. p_value (numeric): P-value estimate. BF (numeric): Bayes Factor estimate. convergence (logical): Whether method converged successfully. note (character): Additional notes describing convergence issues. methods may include additional method-specific columns beyond standard columns. Use get_method_extra_columns() query additional columns particular method returns.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/run_method.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generic method function for publication bias correction — run_method","text":"","code":"# Example usage with PET method data <- data.frame(   yi = c(0.2, 0.3, 0.1, 0.4),   sei = c(0.1, 0.15, 0.08, 0.12) ) result <- run_method(\"RMA\", data, \"default\")  # Example usage with PETPEESE method # result <- method(\"PETPEESE\", data)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/simulate_dgm.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate From Data-Generating Mechanism — simulate_dgm","title":"Simulate From Data-Generating Mechanism — simulate_dgm","text":"function provides unified interface various data-generating mechanisms simulation studies. specific DGM determined first argument. See vignette(\"Adding_New_DGMs\", package = \"PublicationBiasBenchmark\") details extending package new DGMs.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/simulate_dgm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate From Data-Generating Mechanism — simulate_dgm","text":"","code":"simulate_dgm(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/simulate_dgm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate From Data-Generating Mechanism — simulate_dgm","text":"dgm_name Character string specifying DGM type settings List containing required parameters DGM numeric condition_id","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/simulate_dgm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate From Data-Generating Mechanism — simulate_dgm","text":"data frame containing generated data standardized structure","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/simulate_dgm.html","id":"output-structure","dir":"Reference","previous_headings":"","what":"Output Structure","title":"Simulate From Data-Generating Mechanism — simulate_dgm","text":"returned data frame follows standardized schema downstream functions rely . Across currently implemented DGMs, following columns used: yi (numeric): effect size estimate. sei (numeric): Standard error yi. ni (integer): Total sample size estimate (e.g., sum groups applicable). es_type (character): Effect size type, used disambiguate scale yi. Currently used values \"SMD\" (standardized mean difference / Cohen's d), \"logOR\" (log odds ratio), \"none\" (unspecified generic continuous coefficient). study_id (integer/character, optional): Identifier primary study/cluster DGM yields multiple estimates per study (e.g., Alinaghi2018, PRE). absent, row treated independent study.","code":""},{"path":[]},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/simulate_dgm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate From Data-Generating Mechanism — simulate_dgm","text":"","code":"simulate_dgm(\"Carter2019\", 1) #>              yi        sei  ni es_type #> 1  -0.027406349 0.28285599  50     SMD #> 2  -0.096607006 0.07837172 652     SMD #> 3   0.016303565 0.25400447  62     SMD #> 4   0.002914326 0.14990642 178     SMD #> 5  -0.199668025 0.50124431  16     SMD #> 6   0.169618220 0.20888902  92     SMD #> 7  -0.403814247 0.45174844  20     SMD #> 8   0.250896476 0.27843914  52     SMD #> 9  -0.152818816 0.47209208  18     SMD #> 10  0.361928439 0.22543004  80     SMD  simulate_dgm(\"Carter2019\", list(mean_effect = 0, effect_heterogeneity = 0,                        bias = \"high\", QRP = \"high\", n_studies = 10)) #>           yi       sei  ni es_type #> 1  0.7838699 0.3281473  40     SMD #> 2  0.6203106 0.2961379  48     SMD #> 3  0.4809701 0.1971873 106     SMD #> 4  0.1201830 0.1645514 148     SMD #> 5  0.7183940 0.3273348  40     SMD #> 6  0.6595228 0.2853888  52     SMD #> 7  0.2868821 0.1319919 232     SMD #> 8  0.2173063 0.1052860 363     SMD #> 9  1.0274545 0.2855963  56     SMD #> 10 0.2882129 0.1368006 216     SMD  simulate_dgm(\"Stanley2017\", list(environment = \"Cohens_d\", mean_effect = 0,                         effect_heterogeneity = 0, bias = 0, n_studies = 5,                         sample_sizes = c(32,64,125,250,500))) #>             yi        sei   ni es_type #> 1 -0.046999782 0.25003451   64     SMD #> 2 -0.115987229 0.17692527  128     SMD #> 3  0.036326602 0.12650154  250     SMD #> 4  0.067512997 0.08946820  500     SMD #> 5  0.006335243 0.06324571 1000     SMD"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/validate_dgm_setting.html","id":null,"dir":"Reference","previous_headings":"","what":"Validate DGM Settings — validate_dgm_setting","title":"Validate DGM Settings — validate_dgm_setting","text":"function validates settings provided given Data Generating Mechanism (DGM).","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/validate_dgm_setting.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Validate DGM Settings — validate_dgm_setting","text":"","code":"validate_dgm_setting(dgm_name, settings)"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/validate_dgm_setting.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Validate DGM Settings — validate_dgm_setting","text":"dgm_name Character string specifying DGM type settings List containing required parameters DGM numeric condition_id","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/validate_dgm_setting.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Validate DGM Settings — validate_dgm_setting","text":"Error TRUE depending whether settings valid specified DGM.","code":""},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/reference/validate_dgm_setting.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Validate DGM Settings — validate_dgm_setting","text":"","code":"validate_dgm_setting(\"Carter2019\", list(mean_effect = 0,                         effect_heterogeneity = 0, bias = \"high\",                         QRP = \"high\", n_studies = 10))  validate_dgm_setting(\"Alinaghi2018\", list(environment = \"FE\",                         mean_effect = 0, bias = \"positive\"))  validate_dgm_setting(\"Stanley2017\", list(environment = \"Cohens_d\",                         mean_effect = 0,                         effect_heterogeneity = 0, bias = 0, n_studies = 5,                         sample_sizes = c(32,64,125,250,500)))"},{"path":"https://fbartos.github.io/PublicationBiasBenchmark/news/index.html","id":"publicationbiasbenchmark-010","dir":"Changelog","previous_headings":"","what":"PublicationBiasBenchmark 0.1.0","title":"PublicationBiasBenchmark 0.1.0","text":"Initial CRAN submission.","code":""}]
