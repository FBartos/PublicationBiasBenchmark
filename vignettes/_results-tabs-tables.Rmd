---
title: "results-tabs"
output: html_document
---

```{r, include = FALSE}
# This is a child document meant to be included in other Rmd files
# Exit if being knit directly (not as a child)
if (!exists("dgm_names")) {
  message("This is a helper file that should be included as a child in other Rmd documents.")
  message("It requires 'dgm_names' to be defined in the parent document.")
  knitr::knit_exit()
}
```

Method performance measures are aggregated across all simulated conditions to provide an overall impression of method performance. However, keep in mind that a method with a high overall ranking is not necessarily the "best" method for a particular application. To select a suitable method for your application, consider also non-aggregated performance measures in conditions most relevant to your application.

#### RMSE

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "RMSE", common_scale)
```

RMSE (Root Mean Square Error) is an overall summary measure of estimation performance that combines bias and empirical SE. RMSE is the square root of the average squared difference between the meta-analytic estimate and the true effect across simulation runs. A lower RMSE indicates a better method.
`r if (!common_scale) "Methods are compared using condition-wise ranks. Direct comparison using the average RMSE is not possible because the data-generating mechanisms differ in the outcome scale. See the DGM-specific results (or subresults) to see the distribution of RMSE values on the corresponding outcome scale."`

#### Bias

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "Bias", common_scale)
```

Bias is the average difference between the meta-analytic estimate and the true effect across simulation runs. Ideally, this value should be close to 0.
`r if (!common_scale) "Methods are compared using condition-wise ranks. Direct comparison using the average bias is not possible because the data-generating mechanisms differ in the outcome scale. See the DGM-specific results (or subresults) to see the distribution of bias values on the corresponding outcome scale."`

#### Empirical SE

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "Emp_se", common_scale)
```

The empirical SE is the standard deviation of the meta-analytic estimate across simulation runs. A lower empirical SE indicates less variability and better method performance.
`r if (!common_scale) "Methods are compared using condition-wise ranks. Direct comparison using the average empirical standard error is not possible because the data-generating mechanisms differ in the outcome scale. See the DGM-specific results (or subresults) to see the distribution of empirical standard error values on the corresponding outcome scale."`

#### Interval Score

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "interval_score", common_scale)
```

The interval score measures the accuracy of a confidence interval by combining its width and coverage. It penalizes intervals that are too wide or that fail to include the true value. A lower interval score indicates a better method.
`r if (!common_scale) "Methods are compared using condition-wise ranks. Direct comparison using the average Interval Score is not possible because the data-generating mechanisms differ in the outcome scale. See the DGM-specific results (or subresults) to see the distribution of empirical standard error values on the corresponding outcome scale."`

#### 95% CI Coverage

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "Coverage", common_scale)
```

95% CI coverage is the proportion of simulation runs in which the 95% confidence interval contained the true effect. Ideally, this value should be close to the nominal level of 95%.

#### 95% CI Width

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "CI_width", common_scale)
```

95% CI width is the average length of the 95% confidence interval for the true effect. A lower average 95% CI length indicates a better method.
`r if (!common_scale) "Methods are compared using condition-wise ranks. Direct comparison using the average CI width is not possible because the data-generating mechanisms differ in the outcome scale. See the DGM-specific results (or subresults) to see the distribution of CI width values on the corresponding outcome scale."`

#### Log Positive Likelihood Ratio

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "pos_LR", common_scale)
```

The positive likelihood ratio is an overall summary measure of hypothesis testing performance that combines power and type I error rate. It indicates how much a significant test result changes the odds of the alternative hypothesis versus the null hypothesis. A useful method has a positive likelihood ratio greater than 1 (or a log positive likelihood ratio greater than 0). A higher (log) positive likelihood ratio indicates a better method.

#### Log Negative Likelihood Ratio

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "neg_LR", common_scale)
```

The negative likelihood ratio is an overall summary measure of hypothesis testing performance that combines power and type I error rate. It indicates how much a non-significant test result changes the odds of the alternative hypothesis versus the null hypothesis. A useful method has a negative likelihood ratio less than 1 (or a log negative likelihood ratio less than 0). A lower (log) negative likelihood ratio indicates a better method.

#### Type I Error Rate

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "Error", common_scale)
```

The type I error rate is the proportion of simulation runs in which the null hypothesis of no effect was incorrectly rejected when it was true. Ideally, this value should be close to the nominal level of 5%.

#### Power

```{r echo=FALSE} 
create_ranking_table(this_rankings_conditional, this_rankings_replacement,
                     this_tables_conditional, this_tables_replacement,
                     "Power", common_scale)
```

The power is the proportion of simulation runs in which the null hypothesis of no effect was correctly rejected when the alternative hypothesis was true. A higher power indicates a better method.

