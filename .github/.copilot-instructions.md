# PublicationBiasBenchmark R Package

## Project Overview
This R package provides a comprehensive benchmarking framework for publication bias correction methods through simulation studies. The package implements a modular, extensible architecture designed for cumulative methodological research in meta-analysis. It serves as a "living benchmark" that facilitates:

- **Reproducible evaluation** of publication bias correction methods across standardized conditions
- **Easy integration** of new data-generating mechanisms and methods
- **Performance comparison** using precomputed datasets, results, and measures
- **Community contribution** to continuously expand the benchmark

## Core Architecture

### Main Components
1. **Data-Generating Mechanisms (DGMs)**: Generate simulated meta-analytic datasets under specified conditions
2. **Methods**: Apply statistical methods to datasets and return standardized estimates
3. **Precomputed Resources**: Datasets, results, and performance measures stored on OSF
4. **Performance Measures**: Functions to evaluate method performance (bias, RMSE, coverage, power, etc.)

### Design Patterns
- Both DGM and method functions use S3 generic dispatch for extensibility
- Each function type has a generic interface with specific implementations
- New DGMs and methods can be added without modifying core code
- All results follow standardized formats for reproducibility and interoperability
- Precomputed resources enable immediate benchmarking without re-running simulations

## File Structure and Naming Conventions

### Directory Structure
```
R/
├── dgm-*.R              # DGM implementations (e.g., dgm-Stanley2017.R)
├── method-*.R           # Method implementations (e.g., method-PET.R)
├── measures*.R          # Performance measure functions
├── download.R           # Functions for downloading precomputed resources
└── utilities.R          # Helper functions

vignettes/
├── Adding_New_DGMs.Rmd           # Guide for adding DGMs
├── Adding_New_Methods.Rmd        # Guide for adding methods
├── Computing_Method_Results.Rmd  # Computing results workflow
├── Computing_Method_Measures.Rmd # Computing measures workflow
├── Using_Presimulated_Datasets.Rmd
├── Using_Precomputed_Results.Rmd
├── Using_Precomputed_Measures.Rmd
└── Results*.Rmd                  # Benchmark result summaries

resources/
└── {DGM_NAME}/
    ├── data/            # Presimulated datasets by condition
    ├── results/         # Method results files
    ├── measures/        # Performance measures
    └── metadata/        # Condition mappings and session info
```

### Naming Conventions
- **DGM files**: `R/dgm-{DGM_NAME}.R` (e.g., `dgm-Carter2019.R`)
- **Method files**: `R/method-{METHOD_NAME}.R` (e.g., `method-PETPEESE.R`)
- **Function names**: Use snake_case (e.g., `simulate_dgm`, `run_method`)
- **S3 methods**: Use dot notation (e.g., `dgm.Stanley2017`, `method.PET`)

## Data-Generating Mechanisms (DGMs)

### Required Components
Each DGM must implement three S3 methods in a single file `R/dgm-{DGM_NAME}.R`:

1. **`dgm.{DGM_NAME}(dgm_name, settings)`**: Main data generation function
2. **`validate_dgm_setting.{DGM_NAME}(dgm_name, settings)`**: Settings validation
3. **`dgm_conditions.{DGM_NAME}(dgm_name)`**: Prespecified conditions

### Main DGM Function: `dgm.{DGM_NAME}()`

**Input Parameters:**
- `dgm_name`: DGM identifier (automatically passed by framework)
- `settings`: Named list of parameters OR numeric `condition_id`

**Required Output Columns:**
```r
data.frame(
    yi      = effect_sizes,      # Effect size estimates (numeric)
    sei     = standard_errors,   # Standard errors (numeric)
    ni      = sample_sizes,      # Sample sizes (integer)
    es_type = effect_size_type   # Type: "SMD", "logOR", or "none"
)
```

**Optional Output Columns:**
- `study_id`: Unique study identifier (for multilevel/clustered data)
- Additional variables relevant to the DGM

**Key Requirements:**
- Include roxygen2 documentation with `@title`, `@description`, `@param`, `@return`, `@references`, `@export`
- Describe all settings parameters in documentation

### Validation Function: `validate_dgm_setting.{DGM_NAME}()`

**Purpose:** Validate that settings are complete and valid before data generation

**Requirements:**
- Check for missing required parameters
- Validate parameter types (numeric, integer, character, etc.)
- Check parameter ranges and constraints
- Provide clear, informative error messages
- Return `invisible(TRUE)` on success
- Use `stop()` with descriptive messages for failures

**Example checks:**
```r
# Check required parameters exist
missing_params <- setdiff(c("n_studies", "mean_effect"), names(settings))
if (length(missing_params) > 0) 
  stop("Missing required settings: ", paste(missing_params, collapse = ", "))

# Validate types and ranges
if (!is.numeric(n_studies) || n_studies < 1)
  stop("'n_studies' must be a positive integer")
```

### Conditions Function: `dgm_conditions.{DGM_NAME}()`

**Purpose:** Define prespecified simulation conditions for benchmarking

**Requirements:**
- Return data.frame with one row per condition
- Include all DGM-specific setting parameters as columns
- **Must include** `condition_id` column with unique integer identifiers
- Conditions are immutable once published (for reproducibility)
- Use `expand.grid()` to create factorial designs when appropriate

**Example:**
```r
dgm_conditions.no_bias <- function(dgm_name) {
  settings <- expand.grid(
    mean_effect   = c(0, 0.3),
    heterogeneity = c(0, 0.15),
    n_studies     = c(10, 100)
  )
  settings$condition_id <- 1:nrow(settings)
  return(settings)
}
```

### Available DGMs
Current implementations (use `methods("dgm")` to see full list):
- `"no_bias"`: Unbiased data (test DGM)
- `"Stanley2017"`: Stanley et al. (2017)
- `"Alinaghi2018"`: Alinaghi & Reed (2018)
- `"Bom2019"`: Bom & Rachinger (2019)
- `"Carter2019"`: Carter et al. (2019)

### User Interface
Users interact with DGMs via:
```r
# Generate with custom settings
data <- simulate_dgm("Carter2019", list(mean_effect = 0.2, n_studies = 50, ...))

# Generate from predefined condition
data <- simulate_dgm("Carter2019", condition_id = 1)

# View available conditions
conditions <- dgm_conditions("Carter2019")
```

## Statistical Methods

### Required Components
Each method must implement three functions in a single file `R/method-{METHOD_NAME}.R`:

1. **`method.{METHOD_NAME}(method_name, data, settings)`**: Main method implementation
2. **`method_settings.{METHOD_NAME}(method_name)`**: Available method configurations
3. **`method_extra_columns.{METHOD_NAME}(method_name)`**: Additional output columns

### Main Method Function: `method.{METHOD_NAME}()`

**Input Parameters:**
- `method_name`: Method identifier (automatically passed)
- `data`: Data.frame with `yi` (effect sizes), `sei` (standard errors), `ni` (sample sizes)
- `settings`: Method-specific settings (optional, from `method_settings`)

**Required Output Columns:**
```r
data.frame(
    method         = method_name,           # Method identifier
    estimate       = estimate_value,        # Main effect estimate
    standard_error = se_value,              # SE (or NA if not applicable)
    ci_lower       = lower_ci,              # 95% CI lower (or NA)
    ci_upper       = upper_ci,              # 95% CI upper (or NA)
    p_value        = p_val,                 # P-value (or NA)
    BF             = bayes_factor,          # Bayes factor (or NA)
    convergence    = TRUE/FALSE,            # Convergence status
    note           = NA_character_          # Error messages/notes
)
```

**Additional Requirements:**
- Can include method-specific columns (declare in `method_extra_columns`)
- Must handle errors gracefully (framework catches errors automatically)
- Use `stop(message, call. = FALSE)` for user-friendly error messages
- Include comprehensive roxygen2 documentation
- Validate minimum sample requirements (e.g., at least 3 studies)
- Check for sufficient variance in predictors if applicable

**Error Handling:**
The framework automatically catches errors and returns an empty result row with:
- `convergence = FALSE`
- Error message in `note` column
- `NA` for all numeric columns

### Settings Function: `method_settings.{METHOD_NAME}()`

**Purpose:** Define available method configurations

**Requirements:**
- Return named list of settings configurations
- Each configuration is itself a named list of parameters
- Settings are immutable once published
- Include a `"default"` configuration

**Example:**
```r
method_settings.RMA <- function(method_name) {
  list(
    "default" = list(
      method   = "REML",
      test.uni = "knha",
      control  = list(stepadj = 0.5, maxiter = 500)
    )
  )
}

# For methods without configurable settings
method_settings.PET <- function(method_name) {
  list("default" = list())
}
```

### Extra Columns Function: `method_extra_columns.{METHOD_NAME}()`

**Purpose:** Declare method-specific output columns beyond required columns

**Requirements:**
- Return character vector of column names
- Must match exactly the additional columns returned by main function
- Ensures proper merging when methods fail
- Use `character(0)` or `c()` if no extra columns

**Example:**
```r
method_extra_columns.PET <- function(method_name) {
  c("bias_coefficient", "bias_p_value")
}
```

### Available Methods
Current implementations (use `methods("method")` to see full list):
- `"mean"`: Mean effect size
- `"FMA"`: Fixed effects meta-analysis
- `"RMA"`: Random effects meta-analysis
- `"PET"`, `"PEESE"`, `"PETPEESE"`: PET-PEESE family
- `"trimfill"`: Trim-and-Fill
- `"pcurve"`: P-curve
- `"puniform"`: P-uniform and P-uniform*
- `"SM"`: Selection models (3PSM, 4PSM)
- `"RoBMA"`: Robust Bayesian meta-analysis
- And more...

### User Interface
Users interact with methods via:
```r
# Apply method with default settings
result <- run_method("PETPEESE", data)

# Apply method with specific settings
result <- run_method("RMA", data, settings = "default")

# View available settings
settings <- method_settings("RMA")
```

## Performance Measures

The package computes standardized performance measures to evaluate method accuracy across conditions. See `?measures` for complete documentation.

### Available Measures
- **Estimation measures**: `mse`, `rmse`, `empirical_variance`, `empirical_se`, `bias`, `relative_bias`, `bias_mcse`
- **Confidence interval measures**: `coverage`, `mean_ci_width`, `interval_score`
- **Hypothesis testing measures**:   `power`, `positive_likelihood_ratio`, `negative_likelihood_ratio`
- **Convergence**: `convergence` rate

### Method Replacement Strategy
For methods with convergence issues, performance can be computed with fallback replacement:
- Compute measures only from successful cases
- Substitute fallback method results for failed cases
- Ensures fair comparison across conditions
- Common replacement: RMA → FMA (random-effects → fixed-effects)

## Working with Precomputed Resources

The package provides three types of precomputed resources hosted on OSF:

### 1. Presimulated Datasets
Download and retrieve raw meta-analytic datasets:
```r
# Download datasets for a DGM
download_dgm_datasets("Stanley2017")

# Retrieve specific dataset
data <- retrieve_dgm_dataset("Stanley2017", condition_id = 1, repetition_id = 1)

# Retrieve all repetitions for a condition
all_data <- retrieve_dgm_dataset("Stanley2017", condition_id = 1)
```

### 2. Precomputed Results
Download and retrieve method results:
```r
# Download results for a DGM
download_dgm_results("Stanley2017")

# Retrieve results for specific method
results <- retrieve_dgm_results("Stanley2017", method = "RMA", condition_id = 1)

# Retrieve all results
all_results <- retrieve_dgm_results("Stanley2017")
```

### 3. Precomputed Measures
Download and retrieve performance measures:
```r
# Download measures for a DGM
download_dgm_measures("Stanley2017")

# Retrieve bias measure for method
bias <- retrieve_dgm_measures("Stanley2017", measure = "bias", method = "RMA")

# Retrieve all measures
all_measures <- retrieve_dgm_measures("Stanley2017")
```

### Storage Location
Default storage: `PublicationBiasBenchmark.get_option("simulation_directory")`

Change with: `PublicationBiasBenchmark.options(simulation_directory = "/path/")`

## Package Options

Configure package behavior with:
```r
# Set custom directory for resources
PublicationBiasBenchmark.options(simulation_directory = "/path/to/resources")

# Retrieve current setting
PublicationBiasBenchmark.get_option("simulation_directory")
```

## Documentation Standards

### Required Documentation Elements
All exported functions must include roxygen2 documentation with:

- `@title`: Brief function title
- `@description`: Detailed description of purpose and behavior
- `@param`: Document each parameter with type and meaning
- `@return`: Describe return value structure and contents
- `@references`: Cite relevant literature using `\insertAllCited{}`
- `@seealso`: Link to related functions
- `@examples`: Provide working examples (can use `\dontrun{}` for long-running code)
- `@export`: Export user-facing functions

### Example Documentation Template
```r
#' @title Method Name
#'
#' @description
#' Detailed description of what the method does, including any relevant
#' background or methodological details.
#'
#' @param method_name Method identifier (automatically passed)
#' @param data Data frame containing yi (effect sizes) and sei (standard errors)
#' @param settings Optional list of method-specific settings
#'
#' @return Data frame with standardized method output. See [run_method()] for
#'   details on the required output structure.
#'
#' @references
#' \insertAllCited{}
#'
#' @seealso [run_method()], [method_settings()]
#'
#' @examples
#' \dontrun{
#' data <- simulate_dgm("no_bias", list(n_studies = 50, mean_effect = 0.3))
#' result <- run_method("MyMethod", data)
#' }
#'
#' @export
method.MyMethod <- function(method_name, data, settings = NULL) {
  # Implementation
}
```

## Contributing New Components

### Adding a New DGM
1. Create `R/dgm-{NAME}.R` with three required functions
2. Implement comprehensive documentation
3. Test with `validate_dgm_setting()` and `simulate_dgm()`
4. Submit with documentation of simulation design
5. Package maintainers will generate presimulated datasets

### Adding a New Method
1. Create `R/method-{NAME}.R` with three required functions
2. Implement comprehensive documentation and error handling
3. Test method on existing datasets
4. Compute results for all DGMs (see Computing Method Results vignette)
5. Document session info for reproducibility
6. Submit results files with code
7. Package maintainers will compute performance measures

## Testing and Validation

### Always Test
- New DGMs: Generate data with various settings, check output structure
- New methods: Apply to example datasets, verify convergence handling
- Run `devtools::check()` before submitting
- Verify documentation renders correctly with `devtools::document()`

### Common Issues to Avoid
- Missing required output columns
- Inconsistent naming (ensure proper S3 dispatch)
- Missing `condition_id` in dgm_conditions
- Incomplete error handling in methods
- Undocumented parameters or return values
- Examples that don't run or take too long

## Reproducibility Requirements

### Session Information
When computing results or measures, always document:
```r
# Save session info
sessionInfo()
sessioninfo::session_info()  # Preferred - more detailed
```

## Coding Style
- Use snake_case for function names and variables
- Use descriptive parameter names
- Include comments for complex logic
- Keep functions focused and modular
- Prefer explicit over implicit behavior
